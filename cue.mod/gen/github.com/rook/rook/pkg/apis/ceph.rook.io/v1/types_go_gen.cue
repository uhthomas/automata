// Code generated by cue get go. DO NOT EDIT.

//cue:generate cue get go github.com/rook/rook/pkg/apis/ceph.rook.io/v1

package v1

import (
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"time"
	"k8s.io/apimachinery/pkg/api/resource"
	"k8s.io/api/core/v1"
)

// CephCluster is a Ceph storage cluster
// +kubebuilder:printcolumn:name="DataDirHostPath",type=string,JSONPath=`.spec.dataDirHostPath`,description="Directory used on the K8s nodes"
// +kubebuilder:printcolumn:name="MonCount",type=string,JSONPath=`.spec.mon.count`,description="Number of MONs"
// +kubebuilder:printcolumn:name="Age",type=date,JSONPath=`.metadata.creationTimestamp`
// +kubebuilder:printcolumn:name="Phase",type=string,JSONPath=`.status.phase`
// +kubebuilder:printcolumn:name="Message",type=string,JSONPath=`.status.message`,description="Message"
// +kubebuilder:printcolumn:name="Health",type=string,JSONPath=`.status.ceph.health`,description="Ceph Health"
// +kubebuilder:printcolumn:name="External",type=boolean,JSONPath=`.spec.external.enable`
// +kubebuilder:printcolumn:name="FSID",type=string,JSONPath=`.status.ceph.fsid`,description="Ceph FSID"
// +kubebuilder:subresource:status
#CephCluster: {
	metav1.#TypeMeta
	metadata: metav1.#ObjectMeta @go(ObjectMeta)
	spec:     #ClusterSpec       @go(Spec)

	// +kubebuilder:pruning:PreserveUnknownFields
	// +optional
	// +nullable
	status?: #ClusterStatus @go(Status)
}

// CephClusterHealthCheckSpec represent the healthcheck for Ceph daemons
#CephClusterHealthCheckSpec: {
	// DaemonHealth is the health check for a given daemon
	// +optional
	// +nullable
	daemonHealth?: #DaemonHealthSpec @go(DaemonHealth)

	// LivenessProbe allows changing the livenessProbe configuration for a given daemon
	// +optional
	livenessProbe?: {[string]: null | #ProbeSpec} @go(LivenessProbe,map[KeyType]*ProbeSpec)

	// StartupProbe allows changing the startupProbe configuration for a given daemon
	// +optional
	startupProbe?: {[string]: null | #ProbeSpec} @go(StartupProbe,map[KeyType]*ProbeSpec)
}

// DaemonHealthSpec is a daemon health check
#DaemonHealthSpec: {
	// Status represents the health check settings for the Ceph health
	// +optional
	// +nullable
	status?: #HealthCheckSpec @go(Status)

	// Monitor represents the health check settings for the Ceph monitor
	// +optional
	// +nullable
	mon?: #HealthCheckSpec @go(Monitor)

	// ObjectStorageDaemon represents the health check settings for the Ceph OSDs
	// +optional
	// +nullable
	osd?: #HealthCheckSpec @go(ObjectStorageDaemon)
}

// CephClusterList is a list of CephCluster
#CephClusterList: {
	metav1.#TypeMeta
	metadata: metav1.#ListMeta @go(ListMeta)
	items: [...#CephCluster] @go(Items,[]CephCluster)
}

// ClusterSpec represents the specification of Ceph Cluster
#ClusterSpec: {
	// The version information that instructs Rook to orchestrate a particular version of Ceph.
	// +optional
	// +nullable
	cephVersion?: #CephVersionSpec @go(CephVersion)

	// A spec for available storage in the cluster and how it should be used
	// +optional
	// +nullable
	storage?: #StorageScopeSpec @go(Storage)

	// The annotations-related configuration to add/set on each Pod related object.
	// +nullable
	// +optional
	annotations?: #AnnotationsSpec @go(Annotations)

	// The labels-related configuration to add/set on each Pod related object.
	// +kubebuilder:pruning:PreserveUnknownFields
	// +nullable
	// +optional
	labels?: #LabelsSpec @go(Labels)

	// The placement-related configuration to pass to kubernetes (affinity, node selector, tolerations).
	// +kubebuilder:pruning:PreserveUnknownFields
	// +nullable
	// +optional
	placement?: #PlacementSpec @go(Placement)

	// Network related configuration
	// +kubebuilder:pruning:PreserveUnknownFields
	// +nullable
	// +optional
	network?: #NetworkSpec @go(Network)

	// Resources set resource requests and limits
	// +kubebuilder:pruning:PreserveUnknownFields
	// +nullable
	// +optional
	resources?: #ResourceSpec @go(Resources)

	// PriorityClassNames sets priority classes on components
	// +kubebuilder:pruning:PreserveUnknownFields
	// +nullable
	// +optional
	priorityClassNames?: #PriorityClassNamesSpec @go(PriorityClassNames)

	// The path on the host where config and data can be persisted
	// +kubebuilder:validation:Pattern=`^/(\S+)`
	// +optional
	dataDirHostPath?: string @go(DataDirHostPath)

	// SkipUpgradeChecks defines if an upgrade should be forced even if one of the check fails
	// +optional
	skipUpgradeChecks?: bool @go(SkipUpgradeChecks)

	// ContinueUpgradeAfterChecksEvenIfNotHealthy defines if an upgrade should continue even if PGs are not clean
	// +optional
	continueUpgradeAfterChecksEvenIfNotHealthy?: bool @go(ContinueUpgradeAfterChecksEvenIfNotHealthy)

	// WaitTimeoutForHealthyOSDInMinutes defines the time the operator would wait before an OSD can be stopped for upgrade or restart.
	// If the timeout exceeds and OSD is not ok to stop, then the operator would skip upgrade for the current OSD and proceed with the next one
	// if `continueUpgradeAfterChecksEvenIfNotHealthy` is `false`. If `continueUpgradeAfterChecksEvenIfNotHealthy` is `true`, then operator would
	// continue with the upgrade of an OSD even if its not ok to stop after the timeout. This timeout won't be applied if `skipUpgradeChecks` is `true`.
	// The default wait timeout is 10 minutes.
	// +optional
	waitTimeoutForHealthyOSDInMinutes?: time.#Duration @go(WaitTimeoutForHealthyOSDInMinutes)

	// A spec for configuring disruption management.
	// +nullable
	// +optional
	disruptionManagement?: #DisruptionManagementSpec @go(DisruptionManagement)

	// A spec for mon related options
	// +optional
	// +nullable
	mon?: #MonSpec @go(Mon)

	// A spec for the crash controller
	// +optional
	// +nullable
	crashCollector?: #CrashCollectorSpec @go(CrashCollector)

	// Dashboard settings
	// +optional
	// +nullable
	dashboard?: #DashboardSpec @go(Dashboard)

	// Prometheus based Monitoring settings
	// +optional
	// +nullable
	monitoring?: #MonitoringSpec @go(Monitoring)

	// Whether the Ceph Cluster is running external to this Kubernetes cluster
	// mon, mgr, osd, mds, and discover daemons will not be created for external clusters.
	// +optional
	// +nullable
	external?: #ExternalSpec @go(External)

	// A spec for mgr related options
	// +optional
	// +nullable
	mgr?: #MgrSpec @go(Mgr)

	// Remove the OSD that is out and safe to remove only if this option is true
	// +optional
	removeOSDsIfOutAndSafeToRemove?: bool @go(RemoveOSDsIfOutAndSafeToRemove)

	// Indicates user intent when deleting a cluster; blocks orchestration and should not be set if cluster
	// deletion is not imminent.
	// +optional
	// +nullable
	cleanupPolicy?: #CleanupPolicySpec @go(CleanupPolicy)

	// Internal daemon healthchecks and liveness probe
	// +optional
	// +nullable
	healthCheck?: #CephClusterHealthCheckSpec @go(HealthCheck)

	// Security represents security settings
	// +optional
	// +nullable
	security?: #SecuritySpec @go(Security)

	// Logging represents loggings settings
	// +optional
	// +nullable
	logCollector?: #LogCollectorSpec @go(LogCollector)
}

// LogCollectorSpec is the logging spec
#LogCollectorSpec: {
	// Enabled represents whether the log collector is enabled
	// +optional
	enabled?: bool @go(Enabled)

	// Periodicity is the periodicity of the log rotation.
	// +kubebuilder:validation:Pattern=`^$|^(hourly|daily|weekly|monthly|1h|24h|1d)$`
	// +optional
	periodicity?: string @go(Periodicity)

	// MaxLogSize is the maximum size of the log per ceph daemons. Must be at least 1M.
	// +optional
	maxLogSize?: null | resource.#Quantity @go(MaxLogSize,*resource.Quantity)
}

// SecuritySpec is security spec to include various security items such as kms
#SecuritySpec: {
	// KeyManagementService is the main Key Management option
	// +optional
	// +nullable
	kms?: #KeyManagementServiceSpec @go(KeyManagementService)

	// KeyRotation defines options for Key Rotation.
	// +optional
	// +nullable
	keyRotation?: #KeyRotationSpec @go(KeyRotation)
}

// ObjectStoreSecuritySpec is spec to define security features like encryption
#ObjectStoreSecuritySpec: {
	// +optional
	// +nullable
	SecuritySpec: #SecuritySpec

	// The settings for supporting AWS-SSE:S3 with RGW
	// +optional
	// +nullable
	s3?: #KeyManagementServiceSpec @go(ServerSideEncryptionS3)
}

// KeyManagementServiceSpec represent various details of the KMS server
#KeyManagementServiceSpec: {
	// ConnectionDetails contains the KMS connection details (address, port etc)
	// +optional
	// +nullable
	// +kubebuilder:pruning:PreserveUnknownFields
	connectionDetails?: {[string]: string} @go(ConnectionDetails,map[string]string)

	// TokenSecretName is the kubernetes secret containing the KMS token
	// +optional
	tokenSecretName?: string @go(TokenSecretName)
}

// KeyRotationSpec represents the settings for Key Rotation.
#KeyRotationSpec: {
	// Enabled represents whether the key rotation is enabled.
	// +optional
	// +kubebuilder:default=false
	enabled?: bool @go(Enabled)

	// Schedule represents the cron schedule for key rotation.
	// +optional
	schedule?: string @go(Schedule)
}

// CephVersionSpec represents the settings for the Ceph version that Rook is orchestrating.
#CephVersionSpec: {
	// Image is the container image used to launch the ceph daemons, such as quay.io/ceph/ceph:<tag>
	// The full list of images can be found at https://quay.io/repository/ceph/ceph?tab=tags
	// +optional
	image?: string @go(Image)

	// Whether to allow unsupported versions (do not set to true in production)
	// +optional
	allowUnsupported?: bool @go(AllowUnsupported)

	// ImagePullPolicy describes a policy for if/when to pull a container image
	// One of Always, Never, IfNotPresent.
	// +kubebuilder:validation:Enum=IfNotPresent;Always;Never;""
	// +optional
	imagePullPolicy?: v1.#PullPolicy @go(ImagePullPolicy)
}

// DashboardSpec represents the settings for the Ceph dashboard
#DashboardSpec: {
	// Enabled determines whether to enable the dashboard
	// +optional
	enabled?: bool @go(Enabled)

	// URLPrefix is a prefix for all URLs to use the dashboard with a reverse proxy
	// +optional
	urlPrefix?: string @go(URLPrefix)

	// Port is the dashboard webserver port
	// +kubebuilder:validation:Minimum=0
	// +kubebuilder:validation:Maximum=65535
	// +optional
	port?: int @go(Port)

	// SSL determines whether SSL should be used
	// +optional
	ssl?: bool @go(SSL)
}

// MonitoringSpec represents the settings for Prometheus based Ceph monitoring
#MonitoringSpec: {
	// Enabled determines whether to create the prometheus rules for the ceph cluster. If true, the prometheus
	// types must exist or the creation will fail. Default is false.
	// +optional
	enabled?: bool @go(Enabled)

	// Whether to disable the metrics reported by Ceph. If false, the prometheus mgr module and Ceph exporter are enabled.
	// If true, the prometheus mgr module and Ceph exporter are both disabled. Default is false.
	// +optional
	metricsDisabled?: bool @go(MetricsDisabled)

	// ExternalMgrEndpoints points to an existing Ceph prometheus exporter endpoint
	// +optional
	// +nullable
	externalMgrEndpoints?: [...v1.#EndpointAddress] @go(ExternalMgrEndpoints,[]v1.EndpointAddress)

	// ExternalMgrPrometheusPort Prometheus exporter port
	// +kubebuilder:validation:Minimum=0
	// +kubebuilder:validation:Maximum=65535
	// +optional
	externalMgrPrometheusPort?: uint16 @go(ExternalMgrPrometheusPort)

	// Port is the prometheus server port
	// +kubebuilder:validation:Minimum=0
	// +kubebuilder:validation:Maximum=65535
	// +optional
	port?: int @go(Port)

	// Interval determines prometheus scrape interval
	// +optional
	interval?: null | metav1.#Duration @go(Interval,*metav1.Duration)
}

// ClusterStatus represents the status of a Ceph cluster
#ClusterStatus: {
	state?:   #ClusterState  @go(State)
	phase?:   #ConditionType @go(Phase)
	message?: string         @go(Message)
	conditions?: [...#Condition] @go(Conditions,[]Condition)
	ceph?:    null | #CephStatus     @go(CephStatus,*CephStatus)
	storage?: null | #CephStorage    @go(CephStorage,*CephStorage)
	version?: null | #ClusterVersion @go(CephVersion,*ClusterVersion)

	// ObservedGeneration is the latest generation observed by the controller.
	// +optional
	observedGeneration?: int64 @go(ObservedGeneration)
}

// CephDaemonsVersions show the current ceph version for different ceph daemons
#CephDaemonsVersions: {
	// Mon shows Mon Ceph version
	// +optional
	mon?: {[string]: int} @go(Mon,map[string]int)

	// Mgr shows Mgr Ceph version
	// +optional
	mgr?: {[string]: int} @go(Mgr,map[string]int)

	// Osd shows Osd Ceph version
	// +optional
	osd?: {[string]: int} @go(Osd,map[string]int)

	// Rgw shows Rgw Ceph version
	// +optional
	rgw?: {[string]: int} @go(Rgw,map[string]int)

	// Mds shows Mds Ceph version
	// +optional
	mds?: {[string]: int} @go(Mds,map[string]int)

	// RbdMirror shows RbdMirror Ceph version
	// +optional
	"rbd-mirror"?: {[string]: int} @go(RbdMirror,map[string]int)

	// CephFSMirror shows CephFSMirror Ceph version
	// +optional
	"cephfs-mirror"?: {[string]: int} @go(CephFSMirror,map[string]int)

	// Overall shows overall Ceph version
	// +optional
	overall?: {[string]: int} @go(Overall,map[string]int)
}

// CephStatus is the details health of a Ceph Cluster
#CephStatus: {
	health?: string @go(Health)
	details?: {[string]: #CephHealthMessage} @go(Details,map[string]CephHealthMessage)
	lastChecked?:    string    @go(LastChecked)
	lastChanged?:    string    @go(LastChanged)
	previousHealth?: string    @go(PreviousHealth)
	capacity?:       #Capacity @go(Capacity)

	// +optional
	versions?: null | #CephDaemonsVersions @go(Versions,*CephDaemonsVersions)
	fsid?:     string                      @go(FSID)
}

// Capacity is the capacity information of a Ceph Cluster
#Capacity: {
	bytesTotal?:     uint64 @go(TotalBytes)
	bytesUsed?:      uint64 @go(UsedBytes)
	bytesAvailable?: uint64 @go(AvailableBytes)
	lastUpdated?:    string @go(LastUpdated)
}

// CephStorage represents flavors of Ceph Cluster Storage
#CephStorage: {
	deviceClasses?: [...#DeviceClasses] @go(DeviceClasses,[]DeviceClasses)
}

// DeviceClasses represents device classes of a Ceph Cluster
#DeviceClasses: {
	name?: string @go(Name)
}

// ClusterVersion represents the version of a Ceph Cluster
#ClusterVersion: {
	image?:   string @go(Image)
	version?: string @go(Version)
}

// CephHealthMessage represents the health message of a Ceph Cluster
#CephHealthMessage: {
	severity: string @go(Severity)
	message:  string @go(Message)
}

// Condition represents a status condition on any Rook-Ceph Custom Resource.
#Condition: {
	type?:               #ConditionType      @go(Type)
	status?:             v1.#ConditionStatus @go(Status)
	reason?:             #ConditionReason    @go(Reason)
	message?:            string              @go(Message)
	lastHeartbeatTime?:  metav1.#Time        @go(LastHeartbeatTime)
	lastTransitionTime?: metav1.#Time        @go(LastTransitionTime)
}

// ConditionReason is a reason for a condition
#ConditionReason: string // #enumConditionReason

#enumConditionReason:
	#ClusterCreatedReason |
	#ClusterConnectedReason |
	#ClusterProgressingReason |
	#ClusterDeletingReason |
	#ClusterConnectingReason |
	#ReconcileSucceeded |
	#ReconcileFailed |
	#ReconcileStarted |
	#DeletingReason |
	#ObjectHasDependentsReason |
	#ObjectHasNoDependentsReason

// ClusterCreatedReason is cluster created reason
#ClusterCreatedReason: #ConditionReason & "ClusterCreated"

// ClusterConnectedReason is cluster connected reason
#ClusterConnectedReason: #ConditionReason & "ClusterConnected"

// ClusterProgressingReason is cluster progressing reason
#ClusterProgressingReason: #ConditionReason & "ClusterProgressing"

// ClusterDeletingReason is cluster deleting reason
#ClusterDeletingReason: #ConditionReason & "ClusterDeleting"

// ClusterConnectingReason is cluster connecting reason
#ClusterConnectingReason: #ConditionReason & "ClusterConnecting"

// ReconcileSucceeded represents when a resource reconciliation was successful.
#ReconcileSucceeded: #ConditionReason & "ReconcileSucceeded"

// ReconcileFailed represents when a resource reconciliation failed.
#ReconcileFailed: #ConditionReason & "ReconcileFailed"

// ReconcileStarted represents when a resource reconciliation started.
#ReconcileStarted: #ConditionReason & "ReconcileStarted"

// DeletingReason represents when Rook has detected a resource object should be deleted.
#DeletingReason: #ConditionReason & "Deleting"

// ObjectHasDependentsReason represents when a resource object has dependents that are blocking
// deletion.
#ObjectHasDependentsReason: #ConditionReason & "ObjectHasDependents"

// ObjectHasNoDependentsReason represents when a resource object has no dependents that are
// blocking deletion.
#ObjectHasNoDependentsReason: #ConditionReason & "ObjectHasNoDependents"

// ConditionType represent a resource's status
#ConditionType: string // #enumConditionType

#enumConditionType:
	#ConditionConnecting |
	#ConditionConnected |
	#ConditionProgressing |
	#ConditionReady |
	#ConditionFailure |
	#ConditionDeleting |
	#ConditionDeletionIsBlocked

// ConditionConnecting represents Connecting state of an object
#ConditionConnecting: #ConditionType & "Connecting"

// ConditionConnected represents Connected state of an object
#ConditionConnected: #ConditionType & "Connected"

// ConditionProgressing represents Progressing state of an object
#ConditionProgressing: #ConditionType & "Progressing"

// ConditionReady represents Ready state of an object
#ConditionReady: #ConditionType & "Ready"

// ConditionFailure represents Failure state of an object
#ConditionFailure: #ConditionType & "Failure"

// ConditionDeleting represents Deleting state of an object
#ConditionDeleting: #ConditionType & "Deleting"

// ConditionDeletionIsBlocked represents when deletion of the object is blocked.
#ConditionDeletionIsBlocked: #ConditionType & "DeletionIsBlocked"

// ClusterState represents the state of a Ceph Cluster
#ClusterState: string // #enumClusterState

#enumClusterState:
	#ClusterStateCreating |
	#ClusterStateCreated |
	#ClusterStateUpdating |
	#ClusterStateConnecting |
	#ClusterStateConnected |
	#ClusterStateError

// ClusterStateCreating represents the Creating state of a Ceph Cluster
#ClusterStateCreating: #ClusterState & "Creating"

// ClusterStateCreated represents the Created state of a Ceph Cluster
#ClusterStateCreated: #ClusterState & "Created"

// ClusterStateUpdating represents the Updating state of a Ceph Cluster
#ClusterStateUpdating: #ClusterState & "Updating"

// ClusterStateConnecting represents the Connecting state of a Ceph Cluster
#ClusterStateConnecting: #ClusterState & "Connecting"

// ClusterStateConnected represents the Connected state of a Ceph Cluster
#ClusterStateConnected: #ClusterState & "Connected"

// ClusterStateError represents the Error state of a Ceph Cluster
#ClusterStateError: #ClusterState & "Error"

// MonSpec represents the specification of the monitor
#MonSpec: {
	// Count is the number of Ceph monitors
	// +kubebuilder:validation:Minimum=0
	// +kubebuilder:validation:Maximum=9
	// +optional
	count?: int @go(Count)

	// AllowMultiplePerNode determines if we can run multiple monitors on the same node (not recommended)
	// +optional
	allowMultiplePerNode?: bool @go(AllowMultiplePerNode)

	// StretchCluster is the stretch cluster specification
	// +optional
	stretchCluster?: null | #StretchClusterSpec @go(StretchCluster,*StretchClusterSpec)

	// VolumeClaimTemplate is the PVC definition
	// +kubebuilder:pruning:PreserveUnknownFields
	// +optional
	volumeClaimTemplate?: null | v1.#PersistentVolumeClaim @go(VolumeClaimTemplate,*v1.PersistentVolumeClaim)
}

// StretchClusterSpec represents the specification of a stretched Ceph Cluster
#StretchClusterSpec: {
	// FailureDomainLabel the failure domain name (e,g: zone)
	// +optional
	failureDomainLabel?: string @go(FailureDomainLabel)

	// SubFailureDomain is the failure domain within a zone
	// +optional
	subFailureDomain?: string @go(SubFailureDomain)

	// Zones is the list of zones
	// +optional
	// +nullable
	zones?: [...#StretchClusterZoneSpec] @go(Zones,[]StretchClusterZoneSpec)
}

// StretchClusterZoneSpec represents the specification of a stretched zone in a Ceph Cluster
#StretchClusterZoneSpec: {
	// Name is the name of the zone
	// +optional
	name?: string @go(Name)

	// Arbiter determines if the zone contains the arbiter
	// +optional
	arbiter?: bool @go(Arbiter)

	// VolumeClaimTemplate is the PVC template
	// +kubebuilder:pruning:PreserveUnknownFields
	// +optional
	volumeClaimTemplate?: null | v1.#PersistentVolumeClaim @go(VolumeClaimTemplate,*v1.PersistentVolumeClaim)
}

// MgrSpec represents options to configure a ceph mgr
#MgrSpec: {
	// Count is the number of manager to run
	// +kubebuilder:validation:Minimum=0
	// +kubebuilder:validation:Maximum=2
	// +optional
	count?: int @go(Count)

	// AllowMultiplePerNode allows to run multiple managers on the same node (not recommended)
	// +optional
	allowMultiplePerNode?: bool @go(AllowMultiplePerNode)

	// Modules is the list of ceph manager modules to enable/disable
	// +optional
	// +nullable
	modules?: [...#Module] @go(Modules,[]Module)
}

// Module represents mgr modules that the user wants to enable or disable
#Module: {
	// Name is the name of the ceph manager module
	// +optional
	name?: string @go(Name)

	// Enabled determines whether a module should be enabled or not
	// +optional
	enabled?: bool @go(Enabled)
}

// ExternalSpec represents the options supported by an external cluster
// +kubebuilder:pruning:PreserveUnknownFields
// +nullable
#ExternalSpec: {
	// Enable determines whether external mode is enabled or not
	// +optional
	enable?: bool @go(Enable)
}

// CrashCollectorSpec represents options to configure the crash controller
#CrashCollectorSpec: {
	// Disable determines whether we should enable the crash collector
	// +optional
	disable?: bool @go(Disable)

	// DaysToRetain represents the number of days to retain crash until they get pruned
	// +optional
	daysToRetain?: uint @go(DaysToRetain)
}

// CephBlockPool represents a Ceph Storage Pool
// +kubebuilder:printcolumn:name="Phase",type=string,JSONPath=`.status.phase`
// +kubebuilder:subresource:status
#CephBlockPool: {
	metav1.#TypeMeta
	metadata: metav1.#ObjectMeta  @go(ObjectMeta)
	spec:     #NamedBlockPoolSpec @go(Spec)

	// +kubebuilder:pruning:PreserveUnknownFields
	status?: null | #CephBlockPoolStatus @go(Status,*CephBlockPoolStatus)
}

// CephBlockPoolList is a list of Ceph Storage Pools
// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object
#CephBlockPoolList: {
	metav1.#TypeMeta
	metadata: metav1.#ListMeta @go(ListMeta)
	items: [...#CephBlockPool] @go(Items,[]CephBlockPool)
}

// DefaultFailureDomain for PoolSpec
#DefaultFailureDomain: "host"

// DefaultCRUSHRoot is the default name of the CRUSH root bucket
#DefaultCRUSHRoot: "default"

// PoolSpec represents the spec of ceph pool
#PoolSpec: {
	// The failure domain: osd/host/(region or zone if available) - technically also any type in the crush map
	// +optional
	failureDomain?: string @go(FailureDomain)

	// The root of the crush hierarchy utilized by the pool
	// +optional
	// +nullable
	crushRoot?: string @go(CrushRoot)

	// The device class the OSD should set to for use in the pool
	// +optional
	// +nullable
	deviceClass?: string @go(DeviceClass)

	// DEPRECATED: use Parameters instead, e.g., Parameters["compression_mode"] = "force"
	// The inline compression mode in Bluestore OSD to set to (options are: none, passive, aggressive, force)
	// +kubebuilder:validation:Enum=none;passive;aggressive;force;""
	// Do NOT set a default value for kubebuilder as this will override the Parameters
	// +optional
	// +nullable
	compressionMode?: string @go(CompressionMode)

	// The replication settings
	// +optional
	replicated?: #ReplicatedSpec @go(Replicated)

	// The erasure code settings
	// +optional
	erasureCoded?: #ErasureCodedSpec @go(ErasureCoded)

	// Parameters is a list of properties to enable on a given pool
	// +kubebuilder:pruning:PreserveUnknownFields
	// +optional
	// +nullable
	parameters?: {[string]: string} @go(Parameters,map[string]string)

	// EnableRBDStats is used to enable gathering of statistics for all RBD images in the pool
	enableRBDStats?: bool @go(EnableRBDStats)

	// The mirroring settings
	mirroring?: #MirroringSpec @go(Mirroring)

	// The mirroring statusCheck
	// +kubebuilder:pruning:PreserveUnknownFields
	statusCheck?: #MirrorHealthCheckSpec @go(StatusCheck)

	// The quota settings
	// +optional
	// +nullable
	quotas?: #QuotaSpec @go(Quotas)
}

// NamedBlockPoolSpec allows a block pool to be created with a non-default name.
// This is more specific than the NamedPoolSpec so we get schema validation on the
// allowed pool names that can be specified.
#NamedBlockPoolSpec: {
	// The desired name of the pool if different from the CephBlockPool CR name.
	// +kubebuilder:validation:Enum=device_health_metrics;.nfs;.mgr
	// +optional
	name?: string @go(Name)

	#PoolSpec
}

// NamedPoolSpec represents the named ceph pool spec
#NamedPoolSpec: {
	// Name of the pool
	name?: string @go(Name)

	#PoolSpec
}

// MirrorHealthCheckSpec represents the health specification of a Ceph Storage Pool mirror
#MirrorHealthCheckSpec: {
	// +optional
	// +nullable
	mirror?: #HealthCheckSpec @go(Mirror)
}

// CephBlockPoolStatus represents the mirroring status of Ceph Storage Pool
#CephBlockPoolStatus: {
	// +optional
	phase?: #ConditionType @go(Phase)

	// +optional
	mirroringStatus?: null | #MirroringStatusSpec @go(MirroringStatus,*MirroringStatusSpec)

	// +optional
	mirroringInfo?: null | #MirroringInfoSpec @go(MirroringInfo,*MirroringInfoSpec)

	// +optional
	snapshotScheduleStatus?: null | #SnapshotScheduleStatusSpec @go(SnapshotScheduleStatus,*SnapshotScheduleStatusSpec)

	// +optional
	// +nullable
	info?: {[string]: string} @go(Info,map[string]string)

	// ObservedGeneration is the latest generation observed by the controller.
	// +optional
	observedGeneration?: int64 @go(ObservedGeneration)
	conditions?: [...#Condition] @go(Conditions,[]Condition)
}

// MirroringStatusSpec is the status of the pool mirroring
#MirroringStatusSpec: {
	#PoolMirroringStatus

	// LastChecked is the last time time the status was checked
	// +optional
	lastChecked?: string @go(LastChecked)

	// LastChanged is the last time time the status last changed
	// +optional
	lastChanged?: string @go(LastChanged)

	// Details contains potential status errors
	// +optional
	details?: string @go(Details)
}

// PoolMirroringStatus is the pool mirror status
#PoolMirroringStatus: {
	// Summary is the mirroring status summary
	// +optional
	summary?: null | #PoolMirroringStatusSummarySpec @go(Summary,*PoolMirroringStatusSummarySpec)
}

// PoolMirroringStatusSummarySpec is the summary output of the command
#PoolMirroringStatusSummarySpec: {
	// Health is the mirroring health
	// +optional
	health?: string @go(Health)

	// DaemonHealth is the health of the mirroring daemon
	// +optional
	daemon_health?: string @go(DaemonHealth)

	// ImageHealth is the health of the mirrored image
	// +optional
	image_health?: string @go(ImageHealth)

	// States is the various state for all mirrored images
	// +optional
	// +nullable
	states?: #StatesSpec @go(States)
}

// StatesSpec are rbd images mirroring state
#StatesSpec: {
	// StartingReplay is when the replay of the mirroring journal starts
	// +optional
	starting_replay?: int @go(StartingReplay)

	// Replaying is when the replay of the mirroring journal is on-going
	// +optional
	replaying?: int @go(Replaying)

	// Syncing is when the image is syncing
	// +optional
	syncing?: int @go(Syncing)

	// StopReplaying is when the replay of the mirroring journal stops
	// +optional
	stopping_replay?: int @go(StopReplaying)

	// Stopped is when the mirroring state is stopped
	// +optional
	stopped?: int @go(Stopped)

	// Unknown is when the mirroring state is unknown
	// +optional
	unknown?: int @go(Unknown)

	// Error is when the mirroring state is errored
	// +optional
	error?: int @go(Error)
}

// MirroringInfoSpec is the status of the pool mirroring
#MirroringInfoSpec: {
	#PoolMirroringInfo

	// +optional
	lastChecked?: string @go(LastChecked)

	// +optional
	lastChanged?: string @go(LastChanged)

	// +optional
	details?: string @go(Details)
}

// PoolMirroringInfo is the mirroring info of a given pool
#PoolMirroringInfo: {
	// Mode is the mirroring mode
	// +optional
	mode?: string @go(Mode)

	// SiteName is the current site name
	// +optional
	site_name?: string @go(SiteName)

	// Peers are the list of peer sites connected to that cluster
	// +optional
	peers?: [...#PeersSpec] @go(Peers,[]PeersSpec)
}

// PeersSpec contains peer details
#PeersSpec: {
	// UUID is the peer UUID
	// +optional
	uuid?: string @go(UUID)

	// Direction is the peer mirroring direction
	// +optional
	direction?: string @go(Direction)

	// SiteName is the current site name
	// +optional
	site_name?: string @go(SiteName)

	// MirrorUUID is the mirror UUID
	// +optional
	mirror_uuid?: string @go(MirrorUUID)

	// ClientName is the CephX user used to connect to the peer
	// +optional
	client_name?: string @go(ClientName)
}

// SnapshotScheduleStatusSpec is the status of the snapshot schedule
#SnapshotScheduleStatusSpec: {
	// SnapshotSchedules is the list of snapshots scheduled
	// +nullable
	// +optional
	snapshotSchedules?: [...#SnapshotSchedulesSpec] @go(SnapshotSchedules,[]SnapshotSchedulesSpec)

	// LastChecked is the last time time the status was checked
	// +optional
	lastChecked?: string @go(LastChecked)

	// LastChanged is the last time time the status last changed
	// +optional
	lastChanged?: string @go(LastChanged)

	// Details contains potential status errors
	// +optional
	details?: string @go(Details)
}

// SnapshotSchedulesSpec is the list of snapshot scheduled for images in a pool
#SnapshotSchedulesSpec: {
	// Pool is the pool name
	// +optional
	pool?: string @go(Pool)

	// Namespace is the RADOS namespace the image is part of
	// +optional
	namespace?: string @go(Namespace)

	// Image is the mirrored image
	// +optional
	image?: string @go(Image)

	// Items is the list schedules times for a given snapshot
	// +optional
	items?: [...#SnapshotSchedule] @go(Items,[]SnapshotSchedule)
}

// SnapshotSchedule is a schedule
#SnapshotSchedule: {
	// Interval is the interval in which snapshots will be taken
	// +optional
	interval?: string @go(Interval)

	// StartTime is the snapshot starting time
	// +optional
	start_time?: string @go(StartTime)
}

// Status represents the status of an object
#Status: {
	// +optional
	phase?: string @go(Phase)

	// ObservedGeneration is the latest generation observed by the controller.
	// +optional
	observedGeneration?: int64 @go(ObservedGeneration)
	conditions?: [...#Condition] @go(Conditions,[]Condition)
}

// ReplicatedSpec represents the spec for replication in a pool
#ReplicatedSpec: {
	// Size - Number of copies per object in a replicated storage pool, including the object itself (required for replicated pool type)
	// +kubebuilder:validation:Minimum=0
	size: uint @go(Size)

	// TargetSizeRatio gives a hint (%) to Ceph in terms of expected consumption of the total cluster capacity
	// +optional
	targetSizeRatio?: float64 @go(TargetSizeRatio)

	// RequireSafeReplicaSize if false allows you to set replica 1
	// +optional
	requireSafeReplicaSize?: bool @go(RequireSafeReplicaSize)

	// ReplicasPerFailureDomain the number of replica in the specified failure domain
	// +kubebuilder:validation:Minimum=1
	// +optional
	replicasPerFailureDomain?: uint @go(ReplicasPerFailureDomain)

	// SubFailureDomain the name of the sub-failure domain
	// +optional
	subFailureDomain?: string @go(SubFailureDomain)

	// HybridStorage represents hybrid storage tier settings
	// +optional
	// +nullable
	hybridStorage?: null | #HybridStorageSpec @go(HybridStorage,*HybridStorageSpec)
}

// HybridStorageSpec represents the settings for hybrid storage pool
#HybridStorageSpec: {
	// PrimaryDeviceClass represents high performance tier (for example SSD or NVME) for Primary OSD
	// +kubebuilder:validation:MinLength=1
	// +kubebuilder:validation:Required
	// +required
	primaryDeviceClass: string @go(PrimaryDeviceClass)

	// SecondaryDeviceClass represents low performance tier (for example HDDs) for remaining OSDs
	// +kubebuilder:validation:MinLength=1
	// +kubebuilder:validation:Required
	// +required
	secondaryDeviceClass: string @go(SecondaryDeviceClass)
}

// MirroringSpec represents the setting for a mirrored pool
#MirroringSpec: {
	// Enabled whether this pool is mirrored or not
	// +optional
	enabled?: bool @go(Enabled)

	// Mode is the mirroring mode: either pool or image
	// +optional
	mode?: string @go(Mode)

	// SnapshotSchedules is the scheduling of snapshot for mirrored images/pools
	// +optional
	snapshotSchedules?: [...#SnapshotScheduleSpec] @go(SnapshotSchedules,[]SnapshotScheduleSpec)

	// Peers represents the peers spec
	// +nullable
	// +optional
	peers?: null | #MirroringPeerSpec @go(Peers,*MirroringPeerSpec)
}

// SnapshotScheduleSpec represents the snapshot scheduling settings of a mirrored pool
#SnapshotScheduleSpec: {
	// Path is the path to snapshot, only valid for CephFS
	// +optional
	path?: string @go(Path)

	// Interval represent the periodicity of the snapshot.
	// +optional
	interval?: string @go(Interval)

	// StartTime indicates when to start the snapshot
	// +optional
	startTime?: string @go(StartTime)
}

// QuotaSpec represents the spec for quotas in a pool
#QuotaSpec: {
	// MaxBytes represents the quota in bytes
	// Deprecated in favor of MaxSize
	// +optional
	maxBytes?: null | uint64 @go(MaxBytes,*uint64)

	// MaxSize represents the quota in bytes as a string
	// +kubebuilder:validation:Pattern=`^[0-9]+[\.]?[0-9]*([KMGTPE]i|[kMGTPE])?$`
	// +optional
	maxSize?: null | string @go(MaxSize,*string)

	// MaxObjects represents the quota in objects
	// +optional
	maxObjects?: null | uint64 @go(MaxObjects,*uint64)
}

// ErasureCodedSpec represents the spec for erasure code in a pool
#ErasureCodedSpec: {
	// Number of coding chunks per object in an erasure coded storage pool (required for erasure-coded pool type).
	// This is the number of OSDs that can be lost simultaneously before data cannot be recovered.
	// +kubebuilder:validation:Minimum=0
	codingChunks: uint @go(CodingChunks)

	// Number of data chunks per object in an erasure coded storage pool (required for erasure-coded pool type).
	// The number of chunks required to recover an object when any single OSD is lost is the same
	// as dataChunks so be aware that the larger the number of data chunks, the higher the cost of recovery.
	// +kubebuilder:validation:Minimum=0
	dataChunks: uint @go(DataChunks)

	// The algorithm for erasure coding
	// +optional
	algorithm?: string @go(Algorithm)
}

// CephFilesystem represents a Ceph Filesystem
// +kubebuilder:printcolumn:name="ActiveMDS",type=string,JSONPath=`.spec.metadataServer.activeCount`,description="Number of desired active MDS daemons"
// +kubebuilder:printcolumn:name="Age",type=date,JSONPath=`.metadata.creationTimestamp`
// +kubebuilder:printcolumn:name="Phase",type=string,JSONPath=`.status.phase`
// +kubebuilder:subresource:status
#CephFilesystem: {
	metav1.#TypeMeta
	metadata: metav1.#ObjectMeta @go(ObjectMeta)
	spec:     #FilesystemSpec    @go(Spec)

	// +kubebuilder:pruning:PreserveUnknownFields
	status?: null | #CephFilesystemStatus @go(Status,*CephFilesystemStatus)
}

// CephFilesystemList represents a list of Ceph Filesystems
#CephFilesystemList: {
	metav1.#TypeMeta
	metadata: metav1.#ListMeta @go(ListMeta)
	items: [...#CephFilesystem] @go(Items,[]CephFilesystem)
}

// FilesystemSpec represents the spec of a file system
#FilesystemSpec: {
	// The metadata pool settings
	// +nullable
	metadataPool: #PoolSpec @go(MetadataPool)

	// The data pool settings, with optional predefined pool name.
	// +nullable
	dataPools: [...#NamedPoolSpec] @go(DataPools,[]NamedPoolSpec)

	// Preserve pools on filesystem deletion
	// +optional
	preservePoolsOnDelete?: bool @go(PreservePoolsOnDelete)

	// Preserve the fs in the cluster on CephFilesystem CR deletion. Setting this to true automatically implies PreservePoolsOnDelete is true.
	// +optional
	preserveFilesystemOnDelete?: bool @go(PreserveFilesystemOnDelete)

	// The mds pod info
	metadataServer: #MetadataServerSpec @go(MetadataServer)

	// The mirroring settings
	// +nullable
	// +optional
	mirroring?: null | #FSMirroringSpec @go(Mirroring,*FSMirroringSpec)

	// The mirroring statusCheck
	// +kubebuilder:pruning:PreserveUnknownFields
	statusCheck?: #MirrorHealthCheckSpec @go(StatusCheck)
}

// MetadataServerSpec represents the specification of a Ceph Metadata Server
#MetadataServerSpec: {
	// The number of metadata servers that are active. The remaining servers in the cluster will be in standby mode.
	// +kubebuilder:validation:Minimum=1
	// +kubebuilder:validation:Maximum=10
	activeCount: int32 @go(ActiveCount)

	// Whether each active MDS instance will have an active standby with a warm metadata cache for faster failover.
	// If false, standbys will still be available, but will not have a warm metadata cache.
	// +optional
	activeStandby?: bool @go(ActiveStandby)

	// The affinity to place the mds pods (default is to place on all available node) with a daemonset
	// +kubebuilder:pruning:PreserveUnknownFields
	// +nullable
	// +optional
	placement?: #Placement @go(Placement)

	// The annotations-related configuration to add/set on each Pod related object.
	// +kubebuilder:pruning:PreserveUnknownFields
	// +nullable
	// +optional
	annotations?: #Annotations @go(Annotations)

	// The labels-related configuration to add/set on each Pod related object.
	// +kubebuilder:pruning:PreserveUnknownFields
	// +nullable
	// +optional
	labels?: #Labels @go(Labels)

	// The resource requirements for the rgw pods
	// +kubebuilder:pruning:PreserveUnknownFields
	// +nullable
	// +optional
	resources?: v1.#ResourceRequirements @go(Resources)

	// PriorityClassName sets priority classes on components
	// +optional
	priorityClassName?: string @go(PriorityClassName)

	// +optional
	livenessProbe?: null | #ProbeSpec @go(LivenessProbe,*ProbeSpec)

	// +optional
	startupProbe?: null | #ProbeSpec @go(StartupProbe,*ProbeSpec)
}

// FSMirroringSpec represents the setting for a mirrored filesystem
#FSMirroringSpec: {
	// Enabled whether this filesystem is mirrored or not
	// +optional
	enabled?: bool @go(Enabled)

	// Peers represents the peers spec
	// +nullable
	// +optional
	peers?: null | #MirroringPeerSpec @go(Peers,*MirroringPeerSpec)

	// SnapshotSchedules is the scheduling of snapshot for mirrored filesystems
	// +optional
	snapshotSchedules?: [...#SnapshotScheduleSpec] @go(SnapshotSchedules,[]SnapshotScheduleSpec)

	// Retention is the retention policy for a snapshot schedule
	// One path has exactly one retention policy.
	// A policy can however contain multiple count-time period pairs in order to specify complex retention policies
	// +optional
	snapshotRetention?: [...#SnapshotScheduleRetentionSpec] @go(SnapshotRetention,[]SnapshotScheduleRetentionSpec)
}

// SnapshotScheduleRetentionSpec is a retention policy
#SnapshotScheduleRetentionSpec: {
	// Path is the path to snapshot
	// +optional
	path?: string @go(Path)

	// Duration represents the retention duration for a snapshot
	// +optional
	duration?: string @go(Duration)
}

// CephFilesystemStatus represents the status of a Ceph Filesystem
#CephFilesystemStatus: {
	// +optional
	phase?: #ConditionType @go(Phase)

	// +optional
	snapshotScheduleStatus?: null | #FilesystemSnapshotScheduleStatusSpec @go(SnapshotScheduleStatus,*FilesystemSnapshotScheduleStatusSpec)

	// Use only info and put mirroringStatus in it?
	// +optional
	// +nullable
	info?: {[string]: string} @go(Info,map[string]string)

	// MirroringStatus is the filesystem mirroring status
	// +optional
	mirroringStatus?: null | #FilesystemMirroringInfoSpec @go(MirroringStatus,*FilesystemMirroringInfoSpec)
	conditions?: [...#Condition] @go(Conditions,[]Condition)

	// ObservedGeneration is the latest generation observed by the controller.
	// +optional
	observedGeneration?: int64 @go(ObservedGeneration)
}

// FilesystemMirroringInfo is the status of the pool mirroring
#FilesystemMirroringInfoSpec: {
	// PoolMirroringStatus is the mirroring status of a filesystem
	// +nullable
	// +optional
	daemonsStatus?: [...#FilesystemMirroringInfo] @go(FilesystemMirroringAllInfo,[]FilesystemMirroringInfo)

	// LastChecked is the last time time the status was checked
	// +optional
	lastChecked?: string @go(LastChecked)

	// LastChanged is the last time time the status last changed
	// +optional
	lastChanged?: string @go(LastChanged)

	// Details contains potential status errors
	// +optional
	details?: string @go(Details)
}

// FilesystemSnapshotScheduleStatusSpec is the status of the snapshot schedule
#FilesystemSnapshotScheduleStatusSpec: {
	// SnapshotSchedules is the list of snapshots scheduled
	// +nullable
	// +optional
	snapshotSchedules?: [...#FilesystemSnapshotSchedulesSpec] @go(SnapshotSchedules,[]FilesystemSnapshotSchedulesSpec)

	// LastChecked is the last time time the status was checked
	// +optional
	lastChecked?: string @go(LastChecked)

	// LastChanged is the last time time the status last changed
	// +optional
	lastChanged?: string @go(LastChanged)

	// Details contains potential status errors
	// +optional
	details?: string @go(Details)
}

// FilesystemSnapshotSchedulesSpec is the list of snapshot scheduled for images in a pool
#FilesystemSnapshotSchedulesSpec: {
	// Fs is the name of the Ceph Filesystem
	// +optional
	fs?: string @go(Fs)

	// Subvol is the name of the sub volume
	// +optional
	subvol?: string @go(Subvol)

	// Path is the path on the filesystem
	// +optional
	path?: string @go(Path)

	// +optional
	rel_path?: string @go(RelPath)

	// +optional
	schedule?: string @go(Schedule)

	// +optional
	retention?: #FilesystemSnapshotScheduleStatusRetention @go(Retention)
}

// FilesystemSnapshotScheduleStatusRetention is the retention specification for a filesystem snapshot schedule
#FilesystemSnapshotScheduleStatusRetention: {
	// Start is when the snapshot schedule starts
	// +optional
	start?: string @go(Start)

	// Created is when the snapshot schedule was created
	// +optional
	created?: string @go(Created)

	// First is when the first snapshot schedule was taken
	// +optional
	first?: string @go(First)

	// Last is when the last snapshot schedule was taken
	// +optional
	last?: string @go(Last)

	// LastPruned is when the last snapshot schedule was pruned
	// +optional
	last_pruned?: string @go(LastPruned)

	// CreatedCount is total amount of snapshots
	// +optional
	created_count?: int @go(CreatedCount)

	// PrunedCount is total amount of pruned snapshots
	// +optional
	pruned_count?: int @go(PrunedCount)

	// Active is whether the scheduled is active or not
	// +optional
	active?: bool @go(Active)
}

// FilesystemMirrorInfoSpec is the filesystem mirror status of a given filesystem
#FilesystemMirroringInfo: {
	// DaemonID is the cephfs-mirror name
	// +optional
	daemon_id?: int @go(DaemonID)

	// Filesystems is the list of filesystems managed by a given cephfs-mirror daemon
	// +optional
	filesystems?: [...#FilesystemsSpec] @go(Filesystems,[]FilesystemsSpec)
}

// FilesystemsSpec is spec for the mirrored filesystem
#FilesystemsSpec: {
	// FilesystemID is the filesystem identifier
	// +optional
	filesystem_id?: int @go(FilesystemID)

	// Name is name of the filesystem
	// +optional
	name?: string @go(Name)

	// DirectoryCount is the number of directories in the filesystem
	// +optional
	directory_count?: int @go(DirectoryCount)

	// Peers represents the mirroring peers
	// +optional
	peers?: [...#FilesystemMirrorInfoPeerSpec] @go(Peers,[]FilesystemMirrorInfoPeerSpec)
}

// FilesystemMirrorInfoPeerSpec is the specification of a filesystem peer mirror
#FilesystemMirrorInfoPeerSpec: {
	// UUID is the peer unique identifier
	// +optional
	uuid?: string @go(UUID)

	// Remote are the remote cluster information
	// +optional
	remote?: null | #PeerRemoteSpec @go(Remote,*PeerRemoteSpec)

	// Stats are the stat a peer mirror
	// +optional
	stats?: null | #PeerStatSpec @go(Stats,*PeerStatSpec)
}

#PeerRemoteSpec: {
	// ClientName is cephx name
	// +optional
	client_name?: string @go(ClientName)

	// ClusterName is the name of the cluster
	// +optional
	cluster_name?: string @go(ClusterName)

	// FsName is the filesystem name
	// +optional
	fs_name?: string @go(FsName)
}

// PeerStatSpec are the mirror stat with a given peer
#PeerStatSpec: {
	// FailureCount is the number of mirroring failure
	// +optional
	failure_count?: int @go(FailureCount)

	// RecoveryCount is the number of recovery attempted after failures
	// +optional
	recovery_count?: int @go(RecoveryCount)
}

// CephObjectStore represents a Ceph Object Store Gateway
// +kubebuilder:printcolumn:name="Phase",type=string,JSONPath=`.status.phase`
// +kubebuilder:subresource:status
#CephObjectStore: {
	metav1.#TypeMeta
	metadata: metav1.#ObjectMeta @go(ObjectMeta)
	spec:     #ObjectStoreSpec   @go(Spec)

	// +kubebuilder:pruning:PreserveUnknownFields
	status?: null | #ObjectStoreStatus @go(Status,*ObjectStoreStatus)
}

// CephObjectStoreList represents a Ceph Object Store Gateways
#CephObjectStoreList: {
	metav1.#TypeMeta
	metadata: metav1.#ListMeta @go(ListMeta)
	items: [...#CephObjectStore] @go(Items,[]CephObjectStore)
}

// ObjectStoreSpec represent the spec of a pool
#ObjectStoreSpec: {
	// The metadata pool settings
	// +optional
	// +nullable
	metadataPool?: #PoolSpec @go(MetadataPool)

	// The data pool settings
	// +optional
	// +nullable
	dataPool?: #PoolSpec @go(DataPool)

	// Preserve pools on object store deletion
	// +optional
	preservePoolsOnDelete?: bool @go(PreservePoolsOnDelete)

	// The rgw pod info
	// +optional
	// +nullable
	gateway: #GatewaySpec @go(Gateway)

	// The multisite info
	// +optional
	// +nullable
	zone?: #ZoneSpec @go(Zone)

	// The RGW health probes
	// +optional
	// +nullable
	healthCheck?: #ObjectHealthCheckSpec @go(HealthCheck)

	// Security represents security settings
	// +optional
	// +nullable
	security?: null | #ObjectStoreSecuritySpec @go(Security,*ObjectStoreSecuritySpec)
}

// ObjectHealthCheckSpec represents the health check of an object store
#ObjectHealthCheckSpec: {
	// +optional
	readinessProbe?: null | #ProbeSpec @go(ReadinessProbe,*ProbeSpec)

	// +optional
	startupProbe?: null | #ProbeSpec @go(StartupProbe,*ProbeSpec)
}

// HealthCheckSpec represents the health check of an object store bucket
#HealthCheckSpec: {
	// +optional
	disabled?: bool @go(Disabled)

	// Interval is the internal in second or minute for the health check to run like 60s for 60 seconds
	// +optional
	interval?: null | metav1.#Duration @go(Interval,*metav1.Duration)

	// +optional
	timeout?: string @go(Timeout)
}

// GatewaySpec represents the specification of Ceph Object Store Gateway
#GatewaySpec: {
	// The port the rgw service will be listening on (http)
	// +optional
	port?: int32 @go(Port)

	// The port the rgw service will be listening on (https)
	// +kubebuilder:validation:Minimum=0
	// +kubebuilder:validation:Maximum=65535
	// +nullable
	// +optional
	securePort?: int32 @go(SecurePort)

	// The number of pods in the rgw replicaset.
	// +nullable
	// +optional
	instances?: int32 @go(Instances)

	// The name of the secret that stores the ssl certificate for secure rgw connections
	// +nullable
	// +optional
	sslCertificateRef?: string @go(SSLCertificateRef)

	// The name of the secret that stores custom ca-bundle with root and intermediate certificates.
	// +nullable
	// +optional
	caBundleRef?: string @go(CaBundleRef)

	// The affinity to place the rgw pods (default is to place on any available node)
	// +kubebuilder:pruning:PreserveUnknownFields
	// +nullable
	// +optional
	placement?: #Placement @go(Placement)

	// The annotations-related configuration to add/set on each Pod related object.
	// +kubebuilder:pruning:PreserveUnknownFields
	// +nullable
	// +optional
	annotations?: #Annotations @go(Annotations)

	// The labels-related configuration to add/set on each Pod related object.
	// +kubebuilder:pruning:PreserveUnknownFields
	// +nullable
	// +optional
	labels?: #Labels @go(Labels)

	// The resource requirements for the rgw pods
	// +kubebuilder:pruning:PreserveUnknownFields
	// +nullable
	// +optional
	resources?: v1.#ResourceRequirements @go(Resources)

	// PriorityClassName sets priority classes on the rgw pods
	// +optional
	priorityClassName?: string @go(PriorityClassName)

	// ExternalRgwEndpoints points to external RGW endpoint(s). Multiple endpoints can be given, but
	// for stability of ObjectBucketClaims, we highly recommend that users give only a single
	// external RGW endpoint that is a load balancer that sends requests to the multiple RGWs.
	// +nullable
	// +optional
	externalRgwEndpoints?: [...#EndpointAddress] @go(ExternalRgwEndpoints,[]EndpointAddress)

	// The configuration related to add/set on each rgw service.
	// +optional
	// +nullable
	service?: null | #RGWServiceSpec @go(Service,*RGWServiceSpec)

	// Whether host networking is enabled for the rgw daemon. If not set, the network settings from the cluster CR will be applied.
	// +kubebuilder:pruning:PreserveUnknownFields
	// +nullable
	// +optional
	hostNetwork?: null | bool @go(HostNetwork,*bool)

	// Whether rgw dashboard is enabled for the rgw daemon. If not set, the rgw dashboard will be enabled.
	// +kubebuilder:pruning:PreserveUnknownFields
	// +nullable
	// +optional
	dashboardEnabled?: null | bool @go(DashboardEnabled,*bool)
}

// EndpointAddress is a tuple that describes a single IP address or host name. This is a subset of
// Kubernetes's v1.EndpointAddress.
// +structType=atomic
#EndpointAddress: {
	// The IP of this endpoint.
	// +optional
	ip: string @go(IP) @protobuf(1,bytes,opt)

	// The Hostname of this endpoint
	// +optional
	hostname?: string @go(Hostname) @protobuf(3,bytes,opt)
}

// ZoneSpec represents a Ceph Object Store Gateway Zone specification
#ZoneSpec: {
	// RGW Zone the Object Store is in
	name: string @go(Name)
}

// ObjectStoreStatus represents the status of a Ceph Object Store resource
#ObjectStoreStatus: {
	// +optional
	phase?: #ConditionType @go(Phase)

	// +optional
	message?: string @go(Message)

	// +optional
	endpoints: #ObjectEndpoints @go(Endpoints)

	// +optional
	// +nullable
	info?: {[string]: string} @go(Info,map[string]string)
	conditions?: [...#Condition] @go(Conditions,[]Condition)

	// ObservedGeneration is the latest generation observed by the controller.
	// +optional
	observedGeneration?: int64 @go(ObservedGeneration)
}

#ObjectEndpoints: {
	// +optional
	// +nullable
	insecure: [...string] @go(Insecure,[]string)

	// +optional
	// +nullable
	secure: [...string] @go(Secure,[]string)
}

// CephObjectStoreUser represents a Ceph Object Store Gateway User
// +kubebuilder:resource:shortName=rcou;objectuser
// +kubebuilder:printcolumn:name="Phase",type=string,JSONPath=`.status.phase`
// +kubebuilder:subresource:status
#CephObjectStoreUser: {
	metav1.#TypeMeta
	metadata: metav1.#ObjectMeta   @go(ObjectMeta)
	spec:     #ObjectStoreUserSpec @go(Spec)

	// +kubebuilder:pruning:PreserveUnknownFields
	// +optional
	status?: null | #ObjectStoreUserStatus @go(Status,*ObjectStoreUserStatus)
}

// ObjectStoreUserStatus represents the status Ceph Object Store Gateway User
#ObjectStoreUserStatus: {
	// +optional
	phase?: string @go(Phase)

	// +optional
	// +nullable
	info?: {[string]: string} @go(Info,map[string]string)

	// ObservedGeneration is the latest generation observed by the controller.
	// +optional
	observedGeneration?: int64 @go(ObservedGeneration)
}

// CephObjectStoreUserList represents a list Ceph Object Store Gateway Users
#CephObjectStoreUserList: {
	metav1.#TypeMeta
	metadata: metav1.#ListMeta @go(ListMeta)
	items: [...#CephObjectStoreUser] @go(Items,[]CephObjectStoreUser)
}

// ObjectStoreUserSpec represent the spec of an Objectstoreuser
#ObjectStoreUserSpec: {
	// The store the user will be created in
	// +optional
	store?: string @go(Store)

	// The display name for the ceph users
	// +optional
	displayName?: string @go(DisplayName)

	// +optional
	// +nullable
	capabilities?: null | #ObjectUserCapSpec @go(Capabilities,*ObjectUserCapSpec)

	// +optional
	// +nullable
	quotas?: null | #ObjectUserQuotaSpec @go(Quotas,*ObjectUserQuotaSpec)
}

// Additional admin-level capabilities for the Ceph object store user
#ObjectUserCapSpec: {
	// +optional
	// +kubebuilder:validation:Enum={"*","read","write","read, write"}
	// Admin capabilities to read/write Ceph object store users. Documented in https://docs.ceph.com/en/latest/radosgw/admin/?#add-remove-admin-capabilities
	user?: string @go(User)

	// +optional
	// +kubebuilder:validation:Enum={"*","read","write","read, write"}
	// Admin capabilities to read/write Ceph object store buckets. Documented in https://docs.ceph.com/en/latest/radosgw/admin/?#add-remove-admin-capabilities
	bucket?: string @go(Bucket)

	// +optional
	// +kubebuilder:validation:Enum={"*","read","write","read, write"}
	// Admin capabilities to read/write Ceph object store metadata. Documented in https://docs.ceph.com/en/latest/radosgw/admin/?#add-remove-admin-capabilities
	metadata?: string @go(MetaData)

	// +optional
	// +kubebuilder:validation:Enum={"*","read","write","read, write"}
	// Admin capabilities to read/write Ceph object store usage. Documented in https://docs.ceph.com/en/latest/radosgw/admin/?#add-remove-admin-capabilities
	usage?: string @go(Usage)

	// +optional
	// +kubebuilder:validation:Enum={"*","read","write","read, write"}
	// Admin capabilities to read/write Ceph object store zones. Documented in https://docs.ceph.com/en/latest/radosgw/admin/?#add-remove-admin-capabilities
	zone?: string @go(Zone)
}

// ObjectUserQuotaSpec can be used to set quotas for the object store user to limit their usage. See the [Ceph docs](https://docs.ceph.com/en/latest/radosgw/admin/?#quota-management) for more
#ObjectUserQuotaSpec: {
	// Maximum bucket limit for the ceph user
	// +optional
	// +nullable
	maxBuckets?: null | int @go(MaxBuckets,*int)

	// Maximum size limit of all objects across all the user's buckets
	// See https://pkg.go.dev/k8s.io/apimachinery/pkg/api/resource#Quantity for more info.
	// +optional
	// +nullable
	maxSize?: null | resource.#Quantity @go(MaxSize,*resource.Quantity)

	// Maximum number of objects across all the user's buckets
	// +optional
	// +nullable
	maxObjects?: null | int64 @go(MaxObjects,*int64)
}

// CephObjectRealm represents a Ceph Object Store Gateway Realm
// +kubebuilder:subresource:status
#CephObjectRealm: {
	metav1.#TypeMeta
	metadata: metav1.#ObjectMeta @go(ObjectMeta)

	// +nullable
	// +optional
	spec?: #ObjectRealmSpec @go(Spec)

	// +kubebuilder:pruning:PreserveUnknownFields
	// +optional
	status?: null | #Status @go(Status,*Status)
}

// CephObjectRealmList represents a list Ceph Object Store Gateway Realms
// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object
#CephObjectRealmList: {
	metav1.#TypeMeta
	metadata: metav1.#ListMeta @go(ListMeta)
	items: [...#CephObjectRealm] @go(Items,[]CephObjectRealm)
}

// ObjectRealmSpec represent the spec of an ObjectRealm
#ObjectRealmSpec: {
	pull?: #PullSpec @go(Pull)
}

// PullSpec represents the pulling specification of a Ceph Object Storage Gateway Realm
#PullSpec: {
	// +kubebuilder:validation:Pattern=`^https*://`
	endpoint?: string @go(Endpoint)
}

// CephObjectZoneGroup represents a Ceph Object Store Gateway Zone Group
// +kubebuilder:printcolumn:name="Phase",type=string,JSONPath=`.status.phase`
// +kubebuilder:subresource:status
#CephObjectZoneGroup: {
	metav1.#TypeMeta
	metadata: metav1.#ObjectMeta   @go(ObjectMeta)
	spec:     #ObjectZoneGroupSpec @go(Spec)

	// +kubebuilder:pruning:PreserveUnknownFields
	// +optional
	status?: null | #Status @go(Status,*Status)
}

// CephObjectZoneGroupList represents a list Ceph Object Store Gateway Zone Groups
// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object
#CephObjectZoneGroupList: {
	metav1.#TypeMeta
	metadata: metav1.#ListMeta @go(ListMeta)
	items: [...#CephObjectZoneGroup] @go(Items,[]CephObjectZoneGroup)
}

// ObjectZoneGroupSpec represent the spec of an ObjectZoneGroup
#ObjectZoneGroupSpec: {
	//The display name for the ceph users
	realm: string @go(Realm)
}

// CephObjectZone represents a Ceph Object Store Gateway Zone
// +kubebuilder:printcolumn:name="Phase",type=string,JSONPath=`.status.phase`
// +kubebuilder:subresource:status
#CephObjectZone: {
	metav1.#TypeMeta
	metadata: metav1.#ObjectMeta @go(ObjectMeta)
	spec:     #ObjectZoneSpec    @go(Spec)

	// +kubebuilder:pruning:PreserveUnknownFields
	// +optional
	status?: null | #Status @go(Status,*Status)
}

// CephObjectZoneList represents a list Ceph Object Store Gateway Zones
// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object
#CephObjectZoneList: {
	metav1.#TypeMeta
	metadata: metav1.#ListMeta @go(ListMeta)
	items: [...#CephObjectZone] @go(Items,[]CephObjectZone)
}

// ObjectZoneSpec represent the spec of an ObjectZone
#ObjectZoneSpec: {
	//The display name for the ceph users
	zoneGroup: string @go(ZoneGroup)

	// The metadata pool settings
	// +nullable
	metadataPool: #PoolSpec @go(MetadataPool)

	// The data pool settings
	// +nullable
	dataPool: #PoolSpec @go(DataPool)

	// If this zone cannot be accessed from other peer Ceph clusters via the ClusterIP Service
	// endpoint created by Rook, you must set this to the externally reachable endpoint(s). You may
	// include the port in the definition. For example: "https://my-object-store.my-domain.net:443".
	// In many cases, you should set this to the endpoint of the ingress resource that makes the
	// CephObjectStore associated with this CephObjectStoreZone reachable to peer clusters.
	// The list can have one or more endpoints pointing to different RGW servers in the zone.
	// +nullable
	// +optional
	customEndpoints?: [...string] @go(CustomEndpoints,[]string)

	// Preserve pools on object zone deletion
	// +optional
	// +kubebuilder:default=true
	preservePoolsOnDelete: bool @go(PreservePoolsOnDelete)
}

// CephBucketTopic represents a Ceph Object Topic for Bucket Notifications
// +kubebuilder:printcolumn:name="Phase",type=string,JSONPath=`.status.phase`
// +kubebuilder:subresource:status
#CephBucketTopic: {
	metav1.#TypeMeta
	metadata: metav1.#ObjectMeta @go(ObjectMeta)
	spec:     #BucketTopicSpec   @go(Spec)

	// +kubebuilder:pruning:PreserveUnknownFields
	// +optional
	status?: null | #BucketTopicStatus @go(Status,*BucketTopicStatus)
}

// BucketTopicStatus represents the Status of a CephBucketTopic
#BucketTopicStatus: {
	// +optional
	phase?: string @go(Phase)

	// The ARN of the topic generated by the RGW
	// +optional
	// +nullable
	ARN?: null | string @go(,*string)

	// ObservedGeneration is the latest generation observed by the controller.
	// +optional
	observedGeneration?: int64 @go(ObservedGeneration)
}

// CephBucketTopicList represents a list Ceph Object Store Bucket Notification Topics
// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object
#CephBucketTopicList: {
	metav1.#TypeMeta
	metadata: metav1.#ListMeta @go(ListMeta)
	items: [...#CephBucketTopic] @go(Items,[]CephBucketTopic)
}

// BucketTopicSpec represent the spec of a Bucket Topic
#BucketTopicSpec: {
	// The name of the object store on which to define the topic
	// +kubebuilder:validation:MinLength=1
	objectStoreName: string @go(ObjectStoreName)

	// The namespace of the object store on which to define the topic
	// +kubebuilder:validation:MinLength=1
	objectStoreNamespace: string @go(ObjectStoreNamespace)

	// Data which is sent in each event
	// +optional
	opaqueData?: string @go(OpaqueData)

	// Indication whether notifications to this endpoint are persistent or not
	// +optional
	persistent?: bool @go(Persistent)

	// Contains the endpoint spec of the topic
	endpoint: #TopicEndpointSpec @go(Endpoint)
}

// TopicEndpointSpec contains exactly one of the endpoint specs of a Bucket Topic
#TopicEndpointSpec: {
	// Spec of HTTP endpoint
	// +optional
	http?: null | #HTTPEndpointSpec @go(HTTP,*HTTPEndpointSpec)

	// Spec of AMQP endpoint
	// +optional
	amqp?: null | #AMQPEndpointSpec @go(AMQP,*AMQPEndpointSpec)

	// Spec of Kafka endpoint
	// +optional
	kafka?: null | #KafkaEndpointSpec @go(Kafka,*KafkaEndpointSpec)
}

// HTTPEndpointSpec represent the spec of an HTTP endpoint of a Bucket Topic
#HTTPEndpointSpec: {
	// The URI of the HTTP endpoint to push notification to
	// +kubebuilder:validation:MinLength=1
	uri: string @go(URI)

	// Indicate whether the server certificate is validated by the client or not
	// +optional
	disableVerifySSL?: bool @go(DisableVerifySSL)

	// Send the notifications with the CloudEvents header: https://github.com/cloudevents/spec/blob/main/cloudevents/adapters/aws-s3.md
	// Supported for Ceph Quincy (v17) or newer.
	// +optional
	sendCloudEvents?: bool @go(SendCloudEvents)
}

// AMQPEndpointSpec represent the spec of an AMQP endpoint of a Bucket Topic
#AMQPEndpointSpec: {
	// The URI of the AMQP endpoint to push notification to
	// +kubebuilder:validation:MinLength=1
	uri: string @go(URI)

	// Name of the exchange that is used to route messages based on topics
	// +kubebuilder:validation:MinLength=1
	exchange: string @go(Exchange)

	// Indicate whether the server certificate is validated by the client or not
	// +optional
	disableVerifySSL?: bool @go(DisableVerifySSL)

	// The ack level required for this topic (none/broker/routeable)
	// +kubebuilder:validation:Enum=none;broker;routeable
	// +kubebuilder:default=broker
	// +optional
	ackLevel?: string @go(AckLevel)
}

// KafkaEndpointSpec represent the spec of a Kafka endpoint of a Bucket Topic
#KafkaEndpointSpec: {
	// The URI of the Kafka endpoint to push notification to
	// +kubebuilder:validation:MinLength=1
	uri: string @go(URI)

	// Indicate whether to use SSL when communicating with the broker
	// +optional
	useSSL?: bool @go(UseSSL)

	// Indicate whether the server certificate is validated by the client or not
	// +optional
	disableVerifySSL?: bool @go(DisableVerifySSL)

	// The ack level required for this topic (none/broker)
	// +kubebuilder:validation:Enum=none;broker
	// +kubebuilder:default=broker
	// +optional
	ackLevel?: string @go(AckLevel)
}

// CephBucketNotification represents a Bucket Notifications
// +kubebuilder:subresource:status
#CephBucketNotification: {
	metav1.#TypeMeta
	metadata: metav1.#ObjectMeta      @go(ObjectMeta)
	spec:     #BucketNotificationSpec @go(Spec)

	// +kubebuilder:pruning:PreserveUnknownFields
	// +optional
	status?: null | #Status @go(Status,*Status)
}

// CephBucketNotificationList represents a list Ceph Object Store Bucket Notification Topics
// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object
#CephBucketNotificationList: {
	metav1.#TypeMeta
	metadata: metav1.#ListMeta @go(ListMeta)
	items: [...#CephBucketNotification] @go(Items,[]CephBucketNotification)
}

// BucketNotificationSpec represent the event type of the bucket notification
// +kubebuilder:validation:Enum="s3:ObjectCreated:*";"s3:ObjectCreated:Put";"s3:ObjectCreated:Post";"s3:ObjectCreated:Copy";"s3:ObjectCreated:CompleteMultipartUpload";"s3:ObjectRemoved:*";"s3:ObjectRemoved:Delete";"s3:ObjectRemoved:DeleteMarkerCreated"
#BucketNotificationEvent: string

// BucketNotificationSpec represent the spec of a Bucket Notification
#BucketNotificationSpec: {
	// The name of the topic associated with this notification
	// +kubebuilder:validation:MinLength=1
	topic: string @go(Topic)

	// List of events that should trigger the notification
	// +optional
	events?: [...#BucketNotificationEvent] @go(Events,[]BucketNotificationEvent)

	// Spec of notification filter
	// +optional
	filter?: null | #NotificationFilterSpec @go(Filter,*NotificationFilterSpec)
}

// NotificationFilterRule represent a single rule in the Notification Filter spec
#NotificationFilterRule: {
	// Name of the metadata or tag
	// +kubebuilder:validation:MinLength=1
	name: string @go(Name)

	// Value to filter on
	value: string @go(Value)
}

// NotificationKeyFilterRule represent a single key rule in the Notification Filter spec
#NotificationKeyFilterRule: {
	// Name of the filter - prefix/suffix/regex
	// +kubebuilder:validation:Enum=prefix;suffix;regex
	name: string @go(Name)

	// Value to filter on
	value: string @go(Value)
}

// NotificationFilterSpec represent the spec of a Bucket Notification filter
#NotificationFilterSpec: {
	// Filters based on the object's key
	// +optional
	keyFilters?: [...#NotificationKeyFilterRule] @go(KeyFilters,[]NotificationKeyFilterRule)

	// Filters based on the object's metadata
	// +optional
	metadataFilters?: [...#NotificationFilterRule] @go(MetadataFilters,[]NotificationFilterRule)

	// Filters based on the object's tags
	// +optional
	tagFilters?: [...#NotificationFilterRule] @go(TagFilters,[]NotificationFilterRule)
}

// RGWServiceSpec represent the spec for RGW service
#RGWServiceSpec: {
	// The annotations-related configuration to add/set on each rgw service.
	// nullable
	// optional
	annotations?: #Annotations @go(Annotations)
}

// CephNFS represents a Ceph NFS
// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object
// +kubebuilder:subresource:status
#CephNFS: {
	metav1.#TypeMeta
	metadata: metav1.#ObjectMeta @go(ObjectMeta)
	spec:     #NFSGaneshaSpec    @go(Spec)

	// +kubebuilder:pruning:PreserveUnknownFields
	// +optional
	status?: null | #Status @go(Status,*Status)
}

// CephNFSList represents a list Ceph NFSes
// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object
#CephNFSList: {
	metav1.#TypeMeta
	metadata: metav1.#ListMeta @go(ListMeta)
	items: [...#CephNFS] @go(Items,[]CephNFS)
}

// NFSGaneshaSpec represents the spec of an nfs ganesha server
#NFSGaneshaSpec: {
	// RADOS is the Ganesha RADOS specification
	// +nullable
	// +optional
	rados?: #GaneshaRADOSSpec @go(RADOS)

	// Server is the Ganesha Server specification
	server: #GaneshaServerSpec @go(Server)

	// Security allows specifying security configurations for the NFS cluster
	// +nullable
	// +optional
	security?: null | #NFSSecuritySpec @go(Security,*NFSSecuritySpec)
}

// GaneshaRADOSSpec represents the specification of a Ganesha RADOS object
#GaneshaRADOSSpec: {
	// The Ceph pool used store the shared configuration for NFS-Ganesha daemons.
	// This setting is required for Ceph v15 and ignored for Ceph v16.
	// As of Ceph Pacific 16.2.7+, this is internally hardcoded to ".nfs".
	// +optional
	pool?: string @go(Pool)

	// The namespace inside the Ceph pool (set by 'pool') where shared NFS-Ganesha config is stored.
	// This setting is required for Ceph v15 and ignored for Ceph v16.
	// As of Ceph Pacific v16+, this is internally set to the name of the CephNFS.
	// +optional
	namespace?: string @go(Namespace)
}

// GaneshaServerSpec represents the specification of a Ganesha Server
#GaneshaServerSpec: {
	// The number of active Ganesha servers
	active: int @go(Active)

	// The affinity to place the ganesha pods
	// +kubebuilder:pruning:PreserveUnknownFields
	// +nullable
	// +optional
	placement?: #Placement @go(Placement)

	// The annotations-related configuration to add/set on each Pod related object.
	// +kubebuilder:pruning:PreserveUnknownFields
	// +nullable
	// +optional
	annotations?: #Annotations @go(Annotations)

	// The labels-related configuration to add/set on each Pod related object.
	// +kubebuilder:pruning:PreserveUnknownFields
	// +nullable
	// +optional
	labels?: #Labels @go(Labels)

	// Resources set resource requests and limits
	// +kubebuilder:pruning:PreserveUnknownFields
	// +nullable
	// +optional
	resources?: v1.#ResourceRequirements @go(Resources)

	// PriorityClassName sets the priority class on the pods
	// +optional
	priorityClassName?: string @go(PriorityClassName)

	// LogLevel set logging level
	// +optional
	logLevel?: string @go(LogLevel)

	// Whether host networking is enabled for the Ganesha server. If not set, the network settings from the cluster CR will be applied.
	// +nullable
	// +optional
	hostNetwork?: null | bool @go(HostNetwork,*bool)
}

// NFSSecuritySpec represents security configurations for an NFS server pod
#NFSSecuritySpec: {
	// SSSD enables integration with System Security Services Daemon (SSSD). SSSD can be used to
	// provide user ID mapping from a number of sources. See https://sssd.io for more information
	// about the SSSD project.
	// +optional
	// +nullable
	sssd?: null | #SSSDSpec @go(SSSD,*SSSDSpec)

	// Kerberos configures NFS-Ganesha to secure NFS client connections with Kerberos.
	// +optional
	// +nullable
	kerberos?: null | #KerberosSpec @go(Kerberos,*KerberosSpec)
}

// KerberosSpec represents configuration for Kerberos.
#KerberosSpec: {
	// PrincipalName corresponds directly to NFS-Ganesha's NFS_KRB5:PrincipalName config. In
	// practice, this is the service prefix of the principal name. The default is "nfs".
	// This value is combined with (a) the namespace and name of the CephNFS (with a hyphen between)
	// and (b) the Realm configured in the user-provided krb5.conf to determine the full principal
	// name: <principalName>/<namespace>-<name>@<realm>. e.g., nfs/rook-ceph-my-nfs@example.net.
	// See https://github.com/nfs-ganesha/nfs-ganesha/wiki/RPCSEC_GSS for more detail.
	// +optional
	// +kubebuilder:default="nfs"
	principalName: string @go(PrincipalName)

	// ConfigFiles defines where the Kerberos configuration should be sourced from. Config files
	// will be placed into the `/etc/krb5.conf.rook/` directory.
	//
	// If this is left empty, Rook will not add any files. This allows you to manage the files
	// yourself however you wish. For example, you may build them into your custom Ceph container
	// image or use the Vault agent injector to securely add the files via annotations on the
	// CephNFS spec (passed to the NFS server pods).
	//
	// Rook configures Kerberos to log to stderr. We suggest removing logging sections from config
	// files to avoid consuming unnecessary disk space from logging to files.
	// +optional
	configFiles: #KerberosConfigFiles @go(ConfigFiles)

	// KeytabFile defines where the Kerberos keytab should be sourced from. The keytab file will be
	// placed into `/etc/krb5.keytab`. If this is left empty, Rook will not add the file.
	// This allows you to manage the `krb5.keytab` file yourself however you wish. For example, you
	// may build it into your custom Ceph container image or use the Vault agent injector to
	// securely add the file via annotations on the CephNFS spec (passed to the NFS server pods).
	// +optional
	keytabFile: #KerberosKeytabFile @go(KeytabFile)
}

// KerberosConfigFiles represents the source(s) from which Kerberos configuration should come.
#KerberosConfigFiles: {
	// VolumeSource accepts a pared down version of the standard Kubernetes VolumeSource for
	// Kerberos configuration files like what is normally used to configure Volumes for a Pod. For
	// example, a ConfigMap, Secret, or HostPath. The volume may contain multiple files, all of
	// which will be loaded.
	volumeSource?: null | #ConfigFileVolumeSource @go(VolumeSource,*ConfigFileVolumeSource)
}

// KerberosKeytabFile represents the source(s) from which the Kerberos keytab file should come.
#KerberosKeytabFile: {
	// VolumeSource accepts a pared down version of the standard Kubernetes VolumeSource for the
	// Kerberos keytab file like what is normally used to configure Volumes for a Pod. For example,
	// a Secret or HostPath.
	// There are two requirements for the source's content:
	//   1. The config file must be mountable via `subPath: krb5.keytab`. For example, in a
	//      Secret, the data item must be named `krb5.keytab`, or `items` must be defined to
	//      select the key and give it path `krb5.keytab`. A HostPath directory must have the
	//      `krb5.keytab` file.
	//   2. The volume or config file must have mode 0600.
	volumeSource?: null | #ConfigFileVolumeSource @go(VolumeSource,*ConfigFileVolumeSource)
}

// SSSDSpec represents configuration for System Security Services Daemon (SSSD).
#SSSDSpec: {
	// Sidecar tells Rook to run SSSD in a sidecar alongside the NFS-Ganesha server in each NFS pod.
	// +optional
	sidecar?: null | #SSSDSidecar @go(Sidecar,*SSSDSidecar)
}

// SSSDSidecar represents configuration when SSSD is run in a sidecar.
#SSSDSidecar: {
	// Image defines the container image that should be used for the SSSD sidecar.
	// +kubebuilder:validation:Required
	// +kubebuilder:validation:MinLength=1
	image: string @go(Image)

	// SSSDConfigFile defines where the SSSD configuration should be sourced from. The config file
	// will be placed into `/etc/sssd/sssd.conf`. If this is left empty, Rook will not add the file.
	// This allows you to manage the `sssd.conf` file yourself however you wish. For example, you
	// may build it into your custom Ceph container image or use the Vault agent injector to
	// securely add the file via annotations on the CephNFS spec (passed to the NFS server pods).
	// +optional
	sssdConfigFile: #SSSDSidecarConfigFile @go(SSSDConfigFile)

	// AdditionalFiles defines any number of additional files that should be mounted into the SSSD
	// sidecar. These files may be referenced by the sssd.conf config file.
	// +optional
	additionalFiles?: [...#SSSDSidecarAdditionalFile] @go(AdditionalFiles,[]SSSDSidecarAdditionalFile)

	// Resources allow specifying resource requests/limits on the SSSD sidecar container.
	// +optional
	resources?: v1.#ResourceRequirements @go(Resources)

	// DebugLevel sets the debug level for SSSD. If unset or set to 0, Rook does nothing. Otherwise,
	// this may be a value between 1 and 10. See SSSD docs for more info:
	// https://sssd.io/troubleshooting/basics.html#sssd-debug-logs
	// +optional
	// +kubebuilder:validation:Minimum=0
	// +kubebuilder:validation:Maximum=10
	debugLevel?: int @go(DebugLevel)
}

// SSSDSidecarConfigFile represents the source(s) from which the SSSD configuration should come.
#SSSDSidecarConfigFile: {
	// VolumeSource accepts a pared down version of the standard Kubernetes VolumeSource for the
	// SSSD configuration file like what is normally used to configure Volumes for a Pod. For
	// example, a ConfigMap, Secret, or HostPath. There are two requirements for the source's
	// content:
	//   1. The config file must be mountable via `subPath: sssd.conf`. For example, in a ConfigMap,
	//      the data item must be named `sssd.conf`, or `items` must be defined to select the key
	//      and give it path `sssd.conf`. A HostPath directory must have the `sssd.conf` file.
	//   2. The volume or config file must have mode 0600.
	volumeSource?: null | #ConfigFileVolumeSource @go(VolumeSource,*ConfigFileVolumeSource)
}

// SSSDSidecarAdditionalFile represents the source from where additional files for the the SSSD
// configuration should come from and are made available.
#SSSDSidecarAdditionalFile: {
	// SubPath defines the sub-path in `/etc/sssd/rook-additional/` where the additional file(s)
	// will be placed. Each subPath definition must be unique and must not contain ':'.
	// +kubebuilder:validation:Required
	// +kubebuilder:validation:MinLength=1
	// +kubebuilder:validation:Pattern=`^[^:]+$`
	subPath: string @go(SubPath)

	// VolumeSource accepts a pared down version of the standard Kubernetes VolumeSource for the
	// additional file(s) like what is normally used to configure Volumes for a Pod. Fore example, a
	// ConfigMap, Secret, or HostPath. Each VolumeSource adds one or more additional files to the
	// SSSD sidecar container in the `/etc/sssd/rook-additional/<subPath>` directory.
	// Be aware that some files may need to have a specific file mode like 0600 due to requirements
	// by SSSD for some files. For example, CA or TLS certificates.
	volumeSource?: null | #ConfigFileVolumeSource @go(VolumeSource,*ConfigFileVolumeSource)
}

// NetworkSpec for Ceph includes backward compatibility code
#NetworkSpec: {
	// Provider is what provides network connectivity to the cluster e.g. "host" or "multus"
	// +nullable
	// +optional
	provider?: string @go(Provider)

	// Selectors string values describe what networks will be used to connect the cluster.
	// Meanwhile the keys describe each network respective responsibilities or any metadata
	// storage provider decide.
	// +nullable
	// +optional
	selectors?: {[string]: string} @go(Selectors,map[string]string)

	// Settings for network connections such as compression and encryption across the
	// wire.
	// +nullable
	// +optional
	connections?: null | #ConnectionsSpec @go(Connections,*ConnectionsSpec)

	// HostNetwork to enable host network
	// +optional
	hostNetwork?: bool @go(HostNetwork)

	// IPFamily is the single stack IPv6 or IPv4 protocol
	// +kubebuilder:validation:Enum=IPv4;IPv6
	// +nullable
	// +optional
	ipFamily?: #IPFamilyType @go(IPFamily)

	// DualStack determines whether Ceph daemons should listen on both IPv4 and IPv6
	// +optional
	dualStack?: bool @go(DualStack)

	// Enable multiClusterService to export the Services between peer clusters
	// +optional
	multiClusterService?: #MultiClusterServiceSpec @go(MultiClusterService)
}

#MultiClusterServiceSpec: {
	// Enable multiClusterService to export the mon and OSD services to peer cluster.
	// Ensure that peer clusters are connected using an MCS API compatible application,
	// like Globalnet Submariner.
	// +optional
	enabled?: bool @go(Enabled)

	// ClusterID uniquely identifies a cluster. It is used as a prefix to nslookup exported
	// services. For example: <clusterid>.<svc>.<ns>.svc.clusterset.local
	clusterID?: string @go(ClusterID)
}

#ConnectionsSpec: {
	// Encryption settings for the network connections.
	// +nullable
	// +optional
	encryption?: null | #EncryptionSpec @go(Encryption,*EncryptionSpec)

	// Compression settings for the network connections.
	// +nullable
	// +optional
	compression?: null | #CompressionSpec @go(Compression,*CompressionSpec)

	// Whether to require msgr2 (port 3300) even if compression or encryption are not enabled.
	// If true, the msgr1 port (6789) will be disabled.
	// Requires a kernel that supports msgr2 (kernel 5.11 or CentOS 8.4 or newer).
	// +optional
	requireMsgr2?: bool @go(RequireMsgr2)
}

#EncryptionSpec: {
	// Whether to encrypt the data in transit across the wire to prevent eavesdropping
	// the data on the network. The default is not set. Even if encryption is not enabled,
	// clients still establish a strong initial authentication for the connection
	// and data integrity is still validated with a crc check. When encryption is enabled,
	// all communication between clients and Ceph daemons, or between Ceph daemons will
	// be encrypted.
	// +optional
	enabled?: bool @go(Enabled)
}

#CompressionSpec: {
	// Whether to compress the data in transit across the wire.
	// The default is not set. Requires Ceph Quincy (v17) or newer.
	// +optional
	enabled?: bool @go(Enabled)
}

// DisruptionManagementSpec configures management of daemon disruptions
#DisruptionManagementSpec: {
	// This enables management of poddisruptionbudgets
	// +optional
	managePodBudgets?: bool @go(ManagePodBudgets)

	// OSDMaintenanceTimeout sets how many additional minutes the DOWN/OUT interval is for drained failure domains
	// it only works if managePodBudgets is true.
	// the default is 30 minutes
	// +optional
	osdMaintenanceTimeout?: time.#Duration @go(OSDMaintenanceTimeout)

	// PGHealthCheckTimeout is the time (in minutes) that the operator will wait for the placement groups to become
	// healthy (active+clean) after a drain was completed and OSDs came back up. Rook will continue with the next drain
	// if the timeout exceeds. It only works if managePodBudgets is true.
	// No values or 0 means that the operator will wait until the placement groups are healthy before unblocking the next drain.
	// +optional
	pgHealthCheckTimeout?: time.#Duration @go(PGHealthCheckTimeout)

	// Deprecated. This enables management of machinedisruptionbudgets.
	// +optional
	manageMachineDisruptionBudgets?: bool @go(ManageMachineDisruptionBudgets)

	// Deprecated. Namespace to look for MDBs by the machineDisruptionBudgetController
	// +optional
	machineDisruptionBudgetNamespace?: string @go(MachineDisruptionBudgetNamespace)
}

// CephClient represents a Ceph Client
// +kubebuilder:printcolumn:name="Phase",type=string,JSONPath=`.status.phase`
// +kubebuilder:subresource:status
#CephClient: {
	metav1.#TypeMeta
	metadata: metav1.#ObjectMeta @go(ObjectMeta)

	// Spec represents the specification of a Ceph Client
	spec: #ClientSpec @go(Spec)

	// Status represents the status of a Ceph Client
	// +kubebuilder:pruning:PreserveUnknownFields
	// +optional
	status?: null | #CephClientStatus @go(Status,*CephClientStatus)
}

// CephClientList represents a list of Ceph Clients
#CephClientList: {
	metav1.#TypeMeta
	metadata: metav1.#ListMeta @go(ListMeta)
	items: [...#CephClient] @go(Items,[]CephClient)
}

// ClientSpec represents the specification of a Ceph Client
#ClientSpec: {
	// +optional
	name?: string @go(Name)

	// +kubebuilder:pruning:PreserveUnknownFields
	caps: {[string]: string} @go(Caps,map[string]string)
}

// CephClientStatus represents the Status of Ceph Client
#CephClientStatus: {
	// +optional
	phase?: #ConditionType @go(Phase)

	// +optional
	// +nullable
	info?: {[string]: string} @go(Info,map[string]string)

	// ObservedGeneration is the latest generation observed by the controller.
	// +optional
	observedGeneration?: int64 @go(ObservedGeneration)
}

// CleanupPolicySpec represents a Ceph Cluster cleanup policy
#CleanupPolicySpec: {
	// Confirmation represents the cleanup confirmation
	// +optional
	// +nullable
	confirmation?: #CleanupConfirmationProperty @go(Confirmation)

	// SanitizeDisks represents way we sanitize disks
	// +optional
	// +nullable
	sanitizeDisks?: #SanitizeDisksSpec @go(SanitizeDisks)

	// AllowUninstallWithVolumes defines whether we can proceed with the uninstall if they are RBD images still present
	// +optional
	allowUninstallWithVolumes?: bool @go(AllowUninstallWithVolumes)
}

// CleanupConfirmationProperty represents the cleanup confirmation
// +kubebuilder:validation:Pattern=`^$|^yes-really-destroy-data$`
#CleanupConfirmationProperty: string // #enumCleanupConfirmationProperty

#enumCleanupConfirmationProperty:
	#DeleteDataDirOnHostsConfirmation

// SanitizeDataSourceProperty represents a sanitizing data source
#SanitizeDataSourceProperty: string // #enumSanitizeDataSourceProperty

#enumSanitizeDataSourceProperty:
	#SanitizeDataSourceZero |
	#SanitizeDataSourceRandom

// SanitizeMethodProperty represents a disk sanitizing method
#SanitizeMethodProperty: string // #enumSanitizeMethodProperty

#enumSanitizeMethodProperty:
	#SanitizeMethodComplete |
	#SanitizeMethodQuick

// SanitizeDisksSpec represents a disk sanitizing specification
#SanitizeDisksSpec: {
	// Method is the method we use to sanitize disks
	// +optional
	// +kubebuilder:validation:Enum=complete;quick
	method?: #SanitizeMethodProperty @go(Method)

	// DataSource is the data source to use to sanitize the disk with
	// +optional
	// +kubebuilder:validation:Enum=zero;random
	dataSource?: #SanitizeDataSourceProperty @go(DataSource)

	// Iteration is the number of pass to apply the sanitizing
	// +optional
	iteration?: int32 @go(Iteration)
}

// CephRBDMirror represents a Ceph RBD Mirror
// +kubebuilder:printcolumn:name="Phase",type=string,JSONPath=`.status.phase`
// +kubebuilder:subresource:status
#CephRBDMirror: {
	metav1.#TypeMeta
	metadata: metav1.#ObjectMeta @go(ObjectMeta)
	spec:     #RBDMirroringSpec  @go(Spec)

	// +kubebuilder:pruning:PreserveUnknownFields
	// +optional
	status?: null | #Status @go(Status,*Status)
}

// CephRBDMirrorList represents a list Ceph RBD Mirrors
// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object
#CephRBDMirrorList: {
	metav1.#TypeMeta
	metadata: metav1.#ListMeta @go(ListMeta)
	items: [...#CephRBDMirror] @go(Items,[]CephRBDMirror)
}

// RBDMirroringSpec represents the specification of an RBD mirror daemon
#RBDMirroringSpec: {
	// Count represents the number of rbd mirror instance to run
	// +kubebuilder:validation:Minimum=1
	count: int @go(Count)

	// Peers represents the peers spec
	// +nullable
	// +optional
	peers?: #MirroringPeerSpec @go(Peers)

	// The affinity to place the rgw pods (default is to place on any available node)
	// +kubebuilder:pruning:PreserveUnknownFields
	// +nullable
	// +optional
	placement?: #Placement @go(Placement)

	// The annotations-related configuration to add/set on each Pod related object.
	// +kubebuilder:pruning:PreserveUnknownFields
	// +nullable
	// +optional
	annotations?: #Annotations @go(Annotations)

	// The labels-related configuration to add/set on each Pod related object.
	// +kubebuilder:pruning:PreserveUnknownFields
	// +nullable
	// +optional
	labels?: #Labels @go(Labels)

	// The resource requirements for the rbd mirror pods
	// +kubebuilder:pruning:PreserveUnknownFields
	// +nullable
	// +optional
	resources?: v1.#ResourceRequirements @go(Resources)

	// PriorityClassName sets priority class on the rbd mirror pods
	// +optional
	priorityClassName?: string @go(PriorityClassName)
}

// MirroringPeerSpec represents the specification of a mirror peer
#MirroringPeerSpec: {
	// SecretNames represents the Kubernetes Secret names to add rbd-mirror or cephfs-mirror peers
	// +optional
	secretNames?: [...string] @go(SecretNames,[]string)
}

// CephFilesystemMirror is the Ceph Filesystem Mirror object definition
// +kubebuilder:printcolumn:name="Phase",type=string,JSONPath=`.status.phase`
// +kubebuilder:subresource:status
#CephFilesystemMirror: {
	metav1.#TypeMeta
	metadata: metav1.#ObjectMeta       @go(ObjectMeta)
	spec:     #FilesystemMirroringSpec @go(Spec)

	// +optional
	status?: null | #Status @go(Status,*Status)
}

// CephFilesystemMirrorList is a list of CephFilesystemMirror
#CephFilesystemMirrorList: {
	metav1.#TypeMeta
	metadata: metav1.#ListMeta @go(ListMeta)
	items: [...#CephFilesystemMirror] @go(Items,[]CephFilesystemMirror)
}

// FilesystemMirroringSpec is the filesystem mirroring specification
#FilesystemMirroringSpec: {
	// The affinity to place the rgw pods (default is to place on any available node)
	// +nullable
	// +optional
	placement?: #Placement @go(Placement)

	// The annotations-related configuration to add/set on each Pod related object.
	// +nullable
	// +optional
	annotations?: #Annotations @go(Annotations)

	// The labels-related configuration to add/set on each Pod related object.
	// +nullable
	// +optional
	labels?: #Labels @go(Labels)

	// The resource requirements for the cephfs-mirror pods
	// +nullable
	// +optional
	resources?: v1.#ResourceRequirements @go(Resources)

	// PriorityClassName sets priority class on the cephfs-mirror pods
	// +optional
	priorityClassName?: string @go(PriorityClassName)
}

// IPFamilyType represents the single stack Ipv4 or Ipv6 protocol.
#IPFamilyType: string // #enumIPFamilyType

#enumIPFamilyType:
	#IPv6 |
	#IPv4

// IPv6 internet protocol version
#IPv6: #IPFamilyType & "IPv6"

// IPv4 internet protocol version
#IPv4: #IPFamilyType & "IPv4"

#StorageScopeSpec: {
	// +nullable
	// +optional
	nodes?: [...#Node] @go(Nodes,[]Node)

	// +optional
	useAllNodes?: bool @go(UseAllNodes)

	// +optional
	onlyApplyOSDPlacement?: bool @go(OnlyApplyOSDPlacement)

	// +kubebuilder:pruning:PreserveUnknownFields
	// +nullable
	// +optional
	config?: {[string]: string} @go(Config,map[string]string)

	#Selection

	// +nullable
	// +optional
	storageClassDeviceSets?: [...#StorageClassDeviceSet] @go(StorageClassDeviceSets,[]StorageClassDeviceSet)
}

// Node is a storage nodes
// +nullable
#Node: {
	// +optional
	name?: string @go(Name)

	// +kubebuilder:pruning:PreserveUnknownFields
	// +nullable
	// +optional
	resources?: v1.#ResourceRequirements @go(Resources)

	// +kubebuilder:pruning:PreserveUnknownFields
	// +nullable
	// +optional
	config?: {[string]: string} @go(Config,map[string]string)

	#Selection
}

// Device represents a disk to use in the cluster
#Device: {
	// +optional
	name?: string @go(Name)

	// +optional
	fullpath?: string @go(FullPath)

	// +kubebuilder:pruning:PreserveUnknownFields
	// +nullable
	// +optional
	config?: {[string]: string} @go(Config,map[string]string)
}

#Selection: {
	// Whether to consume all the storage devices found on a machine
	// +optional
	useAllDevices?: null | bool @go(UseAllDevices,*bool)

	// A regular expression to allow more fine-grained selection of devices on nodes across the cluster
	// +optional
	deviceFilter?: string @go(DeviceFilter)

	// A regular expression to allow more fine-grained selection of devices with path names
	// +optional
	devicePathFilter?: string @go(DevicePathFilter)

	// List of devices to use as storage devices
	// +kubebuilder:pruning:PreserveUnknownFields
	// +nullable
	// +optional
	devices?: [...#Device] @go(Devices,[]Device)

	// PersistentVolumeClaims to use as storage
	// +optional
	volumeClaimTemplates?: [...v1.#PersistentVolumeClaim] @go(VolumeClaimTemplates,[]v1.PersistentVolumeClaim)
}

// PlacementSpec is the placement for core ceph daemons part of the CephCluster CRD
#PlacementSpec: {[string]: #Placement}

// Placement is the placement for an object
#Placement: {
	// NodeAffinity is a group of node affinity scheduling rules
	// +optional
	nodeAffinity?: null | v1.#NodeAffinity @go(NodeAffinity,*v1.NodeAffinity)

	// PodAffinity is a group of inter pod affinity scheduling rules
	// +optional
	podAffinity?: null | v1.#PodAffinity @go(PodAffinity,*v1.PodAffinity)

	// PodAntiAffinity is a group of inter pod anti affinity scheduling rules
	// +optional
	podAntiAffinity?: null | v1.#PodAntiAffinity @go(PodAntiAffinity,*v1.PodAntiAffinity)

	// The pod this Toleration is attached to tolerates any taint that matches
	// the triple <key,value,effect> using the matching operator <operator>
	// +optional
	tolerations?: [...v1.#Toleration] @go(Tolerations,[]v1.Toleration)

	// TopologySpreadConstraint specifies how to spread matching pods among the given topology
	// +optional
	topologySpreadConstraints?: [...v1.#TopologySpreadConstraint] @go(TopologySpreadConstraints,[]v1.TopologySpreadConstraint)
}

// ResourceSpec is a collection of ResourceRequirements that describes the compute resource requirements
#ResourceSpec: {[string]: v1.#ResourceRequirements}

// ProbeSpec is a wrapper around Probe so it can be enabled or disabled for a Ceph daemon
#ProbeSpec: {
	// Disabled determines whether probe is disable or not
	// +optional
	disabled?: bool @go(Disabled)

	// Probe describes a health check to be performed against a container to determine whether it is
	// alive or ready to receive traffic.
	// +optional
	probe?: null | v1.#Probe @go(Probe,*v1.Probe)
}

// PriorityClassNamesSpec is a map of priority class names to be assigned to components
#PriorityClassNamesSpec: {[string]: string}

// StorageClassDeviceSet is a storage class device set
// +nullable
#StorageClassDeviceSet: {
	// Name is a unique identifier for the set
	name: string @go(Name)

	// Count is the number of devices in this set
	// +kubebuilder:validation:Minimum=1
	count: int @go(Count)

	// +kubebuilder:pruning:PreserveUnknownFields
	// +nullable
	// +optional
	resources?: v1.#ResourceRequirements @go(Resources)

	// +kubebuilder:pruning:PreserveUnknownFields
	// +nullable
	// +optional
	placement?: #Placement @go(Placement)

	// +kubebuilder:pruning:PreserveUnknownFields
	// +nullable
	// +optional
	preparePlacement?: null | #Placement @go(PreparePlacement,*Placement)

	// Provider-specific device configuration
	// +kubebuilder:pruning:PreserveUnknownFields
	// +nullable
	// +optional
	config?: {[string]: string} @go(Config,map[string]string)

	// VolumeClaimTemplates is a list of PVC templates for the underlying storage devices
	volumeClaimTemplates: [...v1.#PersistentVolumeClaim] @go(VolumeClaimTemplates,[]v1.PersistentVolumeClaim)

	// Portable represents OSD portability across the hosts
	// +optional
	portable?: bool @go(Portable)

	// TuneSlowDeviceClass Tune the OSD when running on a slow Device Class
	// +optional
	tuneDeviceClass?: bool @go(TuneSlowDeviceClass)

	// TuneFastDeviceClass Tune the OSD when running on a fast Device Class
	// +optional
	tuneFastDeviceClass?: bool @go(TuneFastDeviceClass)

	// Scheduler name for OSD pod placement
	// +optional
	schedulerName?: string @go(SchedulerName)

	// Whether to encrypt the deviceSet
	// +optional
	encrypted?: bool @go(Encrypted)
}

// CephFilesystemSubVolumeGroup represents a Ceph Filesystem SubVolumeGroup
// +kubebuilder:printcolumn:name="Phase",type=string,JSONPath=`.status.phase`
// +kubebuilder:subresource:status
#CephFilesystemSubVolumeGroup: {
	metav1.#TypeMeta
	metadata: metav1.#ObjectMeta @go(ObjectMeta)

	// Spec represents the specification of a Ceph Filesystem SubVolumeGroup
	spec: #CephFilesystemSubVolumeGroupSpec @go(Spec)

	// Status represents the status of a CephFilesystem SubvolumeGroup
	// +kubebuilder:pruning:PreserveUnknownFields
	// +optional
	status?: null | #CephFilesystemSubVolumeGroupStatus @go(Status,*CephFilesystemSubVolumeGroupStatus)
}

// CephFilesystemSubVolumeGroup represents a list of Ceph Clients
#CephFilesystemSubVolumeGroupList: {
	metav1.#TypeMeta
	metadata: metav1.#ListMeta @go(ListMeta)
	items: [...#CephFilesystemSubVolumeGroup] @go(Items,[]CephFilesystemSubVolumeGroup)
}

// CephFilesystemSubVolumeGroupSpec represents the specification of a Ceph Filesystem SubVolumeGroup
#CephFilesystemSubVolumeGroupSpec: {
	// FilesystemName is the name of Ceph Filesystem SubVolumeGroup volume name. Typically it's the name of
	// the CephFilesystem CR. If not coming from the CephFilesystem CR, it can be retrieved from the
	// list of Ceph Filesystem volumes with `ceph fs volume ls`. To learn more about Ceph Filesystem
	// abstractions see https://docs.ceph.com/en/latest/cephfs/fs-volumes/#fs-volumes-and-subvolumes
	filesystemName: string @go(FilesystemName)
}

// CephFilesystemSubVolumeGroupStatus represents the Status of Ceph Filesystem SubVolumeGroup
#CephFilesystemSubVolumeGroupStatus: {
	// +optional
	phase?: #ConditionType @go(Phase)

	// +optional
	// +nullable
	info?: {[string]: string} @go(Info,map[string]string)

	// ObservedGeneration is the latest generation observed by the controller.
	// +optional
	observedGeneration?: int64 @go(ObservedGeneration)
}

// CephBlockPoolRadosNamespace represents a Ceph BlockPool Rados Namespace
// +kubebuilder:subresource:status
#CephBlockPoolRadosNamespace: {
	metav1.#TypeMeta
	metadata: metav1.#ObjectMeta @go(ObjectMeta)

	// Spec represents the specification of a Ceph BlockPool Rados Namespace
	spec: #CephBlockPoolRadosNamespaceSpec @go(Spec)

	// Status represents the status of a CephBlockPool Rados Namespace
	// +kubebuilder:pruning:PreserveUnknownFields
	// +optional
	status?: null | #CephBlockPoolRadosNamespaceStatus @go(Status,*CephBlockPoolRadosNamespaceStatus)
}

// CephBlockPoolRadosNamespaceList represents a list of Ceph BlockPool Rados Namespace
#CephBlockPoolRadosNamespaceList: {
	metav1.#TypeMeta
	metadata: metav1.#ListMeta @go(ListMeta)
	items: [...#CephBlockPoolRadosNamespace] @go(Items,[]CephBlockPoolRadosNamespace)
}

// CephBlockPoolRadosNamespaceSpec represents the specification of a CephBlockPool Rados Namespace
#CephBlockPoolRadosNamespaceSpec: {
	// BlockPoolName is the name of Ceph BlockPool. Typically it's the name of
	// the CephBlockPool CR.
	blockPoolName: string @go(BlockPoolName)
}

// CephBlockPoolRadosNamespaceStatus represents the Status of Ceph BlockPool
// Rados Namespace
#CephBlockPoolRadosNamespaceStatus: {
	// +optional
	phase?: #ConditionType @go(Phase)

	// +optional
	// +nullable
	info?: {[string]: string} @go(Info,map[string]string)
}

// Represents the source of a volume to mount.
// Only one of its members may be specified.
// This is a subset of the full Kubernetes API's VolumeSource that is reduced to what is most likely
// to be useful for mounting config files/dirs into Rook pods.
#ConfigFileVolumeSource: {
	// hostPath represents a pre-existing file or directory on the host
	// machine that is directly exposed to the container. This is generally
	// used for system agents or other privileged things that are allowed
	// to see the host machine. Most containers will NOT need this.
	// More info: https://kubernetes.io/docs/concepts/storage/volumes#hostpath
	// ---
	// +optional
	hostPath?: null | v1.#HostPathVolumeSource @go(HostPath,*v1.HostPathVolumeSource) @protobuf(1,bytes,opt)

	// emptyDir represents a temporary directory that shares a pod's lifetime.
	// More info: https://kubernetes.io/docs/concepts/storage/volumes#emptydir
	// +optional
	emptyDir?: null | v1.#EmptyDirVolumeSource @go(EmptyDir,*v1.EmptyDirVolumeSource) @protobuf(2,bytes,opt)

	// secret represents a secret that should populate this volume.
	// More info: https://kubernetes.io/docs/concepts/storage/volumes#secret
	// +optional
	secret?: null | v1.#SecretVolumeSource @go(Secret,*v1.SecretVolumeSource) @protobuf(6,bytes,opt)

	// persistentVolumeClaimVolumeSource represents a reference to a
	// PersistentVolumeClaim in the same namespace.
	// More info: https://kubernetes.io/docs/concepts/storage/persistent-volumes#persistentvolumeclaims
	// +optional
	persistentVolumeClaim?: null | v1.#PersistentVolumeClaimVolumeSource @go(PersistentVolumeClaim,*v1.PersistentVolumeClaimVolumeSource) @protobuf(10,bytes,opt)

	// configMap represents a configMap that should populate this volume
	// +optional
	configMap?: null | v1.#ConfigMapVolumeSource @go(ConfigMap,*v1.ConfigMapVolumeSource) @protobuf(19,bytes,opt)

	// projected items for all in one resources secrets, configmaps, and downward API
	projected?: null | v1.#ProjectedVolumeSource @go(Projected,*v1.ProjectedVolumeSource) @protobuf(26,bytes,opt)
}
