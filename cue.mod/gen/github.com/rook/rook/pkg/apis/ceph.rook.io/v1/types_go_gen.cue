// Code generated by cue get go. DO NOT EDIT.

//cue:generate cue get go github.com/rook/rook/pkg/apis/ceph.rook.io/v1

package v1

import (
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/api/core/v1"
	"k8s.io/apimachinery/pkg/api/resource"
	"k8s.io/apimachinery/pkg/types"
)

// CephCluster is a Ceph storage cluster
// +kubebuilder:printcolumn:name="DataDirHostPath",type=string,JSONPath=`.spec.dataDirHostPath`,description="Directory used on the K8s nodes"
// +kubebuilder:printcolumn:name="MonCount",type=string,JSONPath=`.spec.mon.count`,description="Number of MONs"
// +kubebuilder:printcolumn:name="Age",type=date,JSONPath=`.metadata.creationTimestamp`
// +kubebuilder:printcolumn:name="Phase",type=string,JSONPath=`.status.phase`
// +kubebuilder:printcolumn:name="Message",type=string,JSONPath=`.status.message`,description="Message"
// +kubebuilder:printcolumn:name="Health",type=string,JSONPath=`.status.ceph.health`,description="Ceph Health"
// +kubebuilder:printcolumn:name="External",type=boolean,JSONPath=`.spec.external.enable`
// +kubebuilder:printcolumn:name="FSID",type=string,JSONPath=`.status.ceph.fsid`,description="Ceph FSID"
// +kubebuilder:subresource:status
// +kubebuilder:resource:shortName=ceph
#CephCluster: {
	metav1.#TypeMeta
	metadata: metav1.#ObjectMeta @go(ObjectMeta)
	spec:     #ClusterSpec       @go(Spec)

	// +kubebuilder:pruning:PreserveUnknownFields
	// +optional
	// +nullable
	status?: #ClusterStatus @go(Status)
}

// CephClusterHealthCheckSpec represent the healthcheck for Ceph daemons
#CephClusterHealthCheckSpec: {
	// DaemonHealth is the health check for a given daemon
	// +optional
	// +nullable
	daemonHealth?: #DaemonHealthSpec @go(DaemonHealth)

	// LivenessProbe allows changing the livenessProbe configuration for a given daemon
	// +optional
	livenessProbe?: {[string]: null | #ProbeSpec} @go(LivenessProbe,map[KeyType]*ProbeSpec)

	// StartupProbe allows changing the startupProbe configuration for a given daemon
	// +optional
	startupProbe?: {[string]: null | #ProbeSpec} @go(StartupProbe,map[KeyType]*ProbeSpec)
}

// DaemonHealthSpec is a daemon health check
#DaemonHealthSpec: {
	// Status represents the health check settings for the Ceph health
	// +optional
	// +nullable
	status?: #HealthCheckSpec @go(Status)

	// Monitor represents the health check settings for the Ceph monitor
	// +optional
	// +nullable
	mon?: #HealthCheckSpec @go(Monitor)

	// ObjectStorageDaemon represents the health check settings for the Ceph OSDs
	// +optional
	// +nullable
	osd?: #HealthCheckSpec @go(ObjectStorageDaemon)
}

// CephClusterList is a list of CephCluster
#CephClusterList: {
	metav1.#TypeMeta
	metadata: metav1.#ListMeta @go(ListMeta)
	items: [...#CephCluster] @go(Items,[]CephCluster)
}

// ClusterSpec represents the specification of Ceph Cluster
#ClusterSpec: {
	// The version information that instructs Rook to orchestrate a particular version of Ceph.
	// +optional
	// +nullable
	cephVersion?: #CephVersionSpec @go(CephVersion)

	// A spec for available storage in the cluster and how it should be used
	// +optional
	// +nullable
	storage?: #StorageScopeSpec @go(Storage)

	// The annotations-related configuration to add/set on each Pod related object.
	// +nullable
	// +optional
	annotations?: #AnnotationsSpec @go(Annotations)

	// The labels-related configuration to add/set on each Pod related object.
	// +kubebuilder:pruning:PreserveUnknownFields
	// +nullable
	// +optional
	labels?: #LabelsSpec @go(Labels)

	// The placement-related configuration to pass to kubernetes (affinity, node selector, tolerations).
	// +kubebuilder:pruning:PreserveUnknownFields
	// +nullable
	// +optional
	placement?: #PlacementSpec @go(Placement)

	// Network related configuration
	// +kubebuilder:pruning:PreserveUnknownFields
	// +nullable
	// +optional
	network?: #NetworkSpec @go(Network)

	// Resources set resource requests and limits
	// +kubebuilder:pruning:PreserveUnknownFields
	// +nullable
	// +optional
	resources?: #ResourceSpec @go(Resources)

	// PriorityClassNames sets priority classes on components
	// +kubebuilder:pruning:PreserveUnknownFields
	// +nullable
	// +optional
	priorityClassNames?: #PriorityClassNamesSpec @go(PriorityClassNames)

	// The path on the host where config and data can be persisted
	// +kubebuilder:validation:Pattern=`^/(\S+)`
	// +kubebuilder:validation:XValidation:message="DataDirHostPath is immutable",rule="self == oldSelf"
	// +optional
	dataDirHostPath?: string @go(DataDirHostPath)

	// SkipUpgradeChecks defines if an upgrade should be forced even if one of the check fails
	// +optional
	skipUpgradeChecks?: bool @go(SkipUpgradeChecks)

	// ContinueUpgradeAfterChecksEvenIfNotHealthy defines if an upgrade should continue even if PGs are not clean
	// +optional
	continueUpgradeAfterChecksEvenIfNotHealthy?: bool @go(ContinueUpgradeAfterChecksEvenIfNotHealthy)

	// WaitTimeoutForHealthyOSDInMinutes defines the time the operator would wait before an OSD can be stopped for upgrade or restart.
	// If the timeout exceeds and OSD is not ok to stop, then the operator would skip upgrade for the current OSD and proceed with the next one
	// if `continueUpgradeAfterChecksEvenIfNotHealthy` is `false`. If `continueUpgradeAfterChecksEvenIfNotHealthy` is `true`, then operator would
	// continue with the upgrade of an OSD even if its not ok to stop after the timeout. This timeout won't be applied if `skipUpgradeChecks` is `true`.
	// The default wait timeout is 10 minutes.
	// +optional
	waitTimeoutForHealthyOSDInMinutes?: int @go(WaitTimeoutForHealthyOSDInMinutes,time.Duration)

	// UpgradeOSDRequiresHealthyPGs defines if OSD upgrade requires PGs are clean. If set to `true` OSD upgrade process won't start until PGs are healthy.
	// This configuration will be ignored if `skipUpgradeChecks` is `true`.
	// Default is false.
	// +optional
	upgradeOSDRequiresHealthyPGs?: bool @go(UpgradeOSDRequiresHealthyPGs)

	// A spec for configuring disruption management.
	// +nullable
	// +optional
	disruptionManagement?: #DisruptionManagementSpec @go(DisruptionManagement)

	// A spec for mon related options
	// +optional
	// +nullable
	mon?: #MonSpec @go(Mon)

	// A spec for the crash controller
	// +optional
	// +nullable
	crashCollector?: #CrashCollectorSpec @go(CrashCollector)

	// Dashboard settings
	// +optional
	// +nullable
	dashboard?: #DashboardSpec @go(Dashboard)

	// Prometheus based Monitoring settings
	// +optional
	// +nullable
	monitoring?: #MonitoringSpec @go(Monitoring)

	// Whether the Ceph Cluster is running external to this Kubernetes cluster
	// mon, mgr, osd, mds, and discover daemons will not be created for external clusters.
	// +optional
	// +nullable
	external?: #ExternalSpec @go(External)

	// A spec for mgr related options
	// +optional
	// +nullable
	mgr?: #MgrSpec @go(Mgr)

	// Remove the OSD that is out and safe to remove only if this option is true
	// +optional
	removeOSDsIfOutAndSafeToRemove?: bool @go(RemoveOSDsIfOutAndSafeToRemove)

	// Indicates user intent when deleting a cluster; blocks orchestration and should not be set if cluster
	// deletion is not imminent.
	// +optional
	// +nullable
	cleanupPolicy?: #CleanupPolicySpec @go(CleanupPolicy)

	// Internal daemon healthchecks and liveness probe
	// +optional
	// +nullable
	healthCheck?: #CephClusterHealthCheckSpec @go(HealthCheck)

	// Security represents security settings
	// +optional
	// +nullable
	security?: #ClusterSecuritySpec @go(Security)

	// Logging represents loggings settings
	// +optional
	// +nullable
	logCollector?: #LogCollectorSpec @go(LogCollector)

	// CSI Driver Options applied per cluster.
	// +optional
	csi?: #CSIDriverSpec @go(CSI)

	// Ceph Config options
	// +optional
	// +nullable
	cephConfig?: {[string]: [string]: string} @go(CephConfig,map[string]map[string]string)

	// CephConfigFromSecret works exactly like CephConfig but takes config value from Secret Key reference.
	// +optional
	// +nullable
	cephConfigFromSecret?: {[string]: [string]: v1.#SecretKeySelector} @go(CephConfigFromSecret,map[string]map[string]v1.SecretKeySelector)
}

// CSIDriverSpec defines CSI Driver settings applied per cluster.
#CSIDriverSpec: {
	// ReadAffinity defines the read affinity settings for CSI driver.
	// +optional
	readAffinity?: #ReadAffinitySpec @go(ReadAffinity)

	// CephFS defines CSI Driver settings for CephFS driver.
	// +optional
	cephfs?: #CSICephFSSpec @go(CephFS)

	// SkipUserCreation determines whether CSI users and their associated secrets should be skipped.
	// If set to true, the user must manually manage these secrets.
	// +optional
	skipUserCreation?: bool @go(SkipUserCreation)
}

// CSICephFSSpec defines the settings for CephFS CSI driver.
#CSICephFSSpec: {
	// KernelMountOptions defines the mount options for kernel mounter.
	// +optional
	kernelMountOptions?: string @go(KernelMountOptions)

	// FuseMountOptions defines the mount options for ceph fuse mounter.
	// +optional
	fuseMountOptions?: string @go(FuseMountOptions)
}

// ReadAffinitySpec defines the read affinity settings for CSI driver.
#ReadAffinitySpec: {
	// Enables read affinity for CSI driver.
	// +optional
	enabled?: bool @go(Enabled)

	// CrushLocationLabels defines which node labels to use
	// as CRUSH location. This should correspond to the values set in
	// the CRUSH map.
	// +optional
	crushLocationLabels?: [...string] @go(CrushLocationLabels,[]string)
}

// LogCollectorSpec is the logging spec
#LogCollectorSpec: {
	// Enabled represents whether the log collector is enabled
	// +optional
	enabled?: bool @go(Enabled)

	// Periodicity is the periodicity of the log rotation.
	// +kubebuilder:validation:Pattern=`^$|^(hourly|daily|weekly|monthly|1h|24h|1d)$`
	// +optional
	periodicity?: string @go(Periodicity)

	// MaxLogSize is the maximum size of the log per ceph daemons. Must be at least 1M.
	// +optional
	maxLogSize?: null | resource.#Quantity @go(MaxLogSize,*resource.Quantity)
}

// SecuritySpec is security spec to include various security items such as kms
#SecuritySpec: {
	// KeyManagementService is the main Key Management option
	// +optional
	// +nullable
	kms?: #KeyManagementServiceSpec @go(KeyManagementService)

	// KeyRotation defines options for Key Rotation.
	// +optional
	// +nullable
	keyRotation?: #KeyRotationSpec @go(KeyRotation)
}

// ClusterSecuritySpec is the CephCluster security spec to include various security items such as kms
#ClusterSecuritySpec: {
	// KeyManagementService is the main Key Management option
	// +optional
	// +nullable
	kms?: #KeyManagementServiceSpec @go(KeyManagementService)

	// KeyRotation defines options for rotation of OSD disk encryption keys.
	// +optional
	// +nullable
	keyRotation?: #KeyRotationSpec @go(KeyRotation)

	// CephX configures CephX key settings. More: https://docs.ceph.com/en/latest/dev/cephx/
	// +optional
	cephx?: #ClusterCephxConfig @go(CephX)
}

#ClusterCephxConfig: {
	// Daemon configures CephX key settings for local Ceph daemons managed by Rook and part of the
	// Ceph cluster. Daemon CephX keys can be rotated without affecting client connections.
	daemon?: #CephxConfig @go(Daemon)

	// RBDMirrorPeer configures CephX key settings of the `rbd-mirror-peer` user that is used for creating
	// bootstrap peer token used connect peer clusters. Rotating the `rbd-mirror-peer` user key will update
	// the mirror peer token.
	// Rotation will affect any existing peers connected to this cluster, so take care when exercising this option.
	rbdMirrorPeer?: #CephxConfig @go(RBDMirrorPeer)

	// CSI configures CephX key rotation settings for the Ceph-CSI daemons in the current Kubernetes cluster.
	// CSI key rotation can affect existing PV connections, so take care when exercising this option.
	csi?: #CephXConfigWithPriorCount @go(CSI)
}

#CephXConfigWithPriorCount: {
	#CephxConfig

	// KeepPriorKeyCountMax tells Rook how many prior keys to keep active.
	// Generally, this would be set to 1 to allow for a migration period for applications.
	// If desired, set this to 0 to delete prior keys after migration.
	// This config only applies to prior keys that already exist.
	// If PriorKeyCount is set to 2 while only a single key currently exists, only a single prior key will be kept,
	// and the reported status will only indicate the actual number of prior keys,
	// not necessarily a reflection of PriorKeyCount config here.
	// +optional
	// +kubebuilder:validation:Minimum=0
	// +kubebuilder:validation:Maximum=10
	keepPriorKeyCountMax?: uint8 @go(KeepPriorKeyCountMax)
}

#CephxConfig: {
	// KeyRotationPolicy controls if and when CephX keys are rotated after initial creation.
	// One of Disabled, or KeyGeneration. Default Disabled.
	// +optional
	// +kubebuilder:validation:Enum="";Disabled;KeyGeneration
	keyRotationPolicy?: #CephxKeyRotationPolicy @go(KeyRotationPolicy)

	// KeyGeneration specifies the desired CephX key generation. This is used when KeyRotationPolicy
	// is KeyGeneration and ignored for other policies. If this is set to greater than the current
	// key generation, relevant keys will be rotated, and the generation value will be updated to
	// this new value (generation values are not necessarily incremental, though that is the
	// intended use case). If this is set to less than or equal to the current key generation, keys
	// are not rotated.
	// +optional
	// +kubebuilder:validation:Minimum=0
	// +kubebuilder:validation:Maximum=4294967295
	// +kubebuilder:validation:XValidation:message="keyGeneration cannot be decreased",rule="self >= oldSelf"
	keyGeneration?: uint32 @go(KeyGeneration)
}

#CephxKeyRotationPolicy: string // #enumCephxKeyRotationPolicy

#enumCephxKeyRotationPolicy:
	#DisabledCephxKeyRotationPolicy |
	#KeyGenerationCephxKeyRotationPolicy

#DisabledCephxKeyRotationPolicy:      #CephxKeyRotationPolicy & "Disabled"
#KeyGenerationCephxKeyRotationPolicy: #CephxKeyRotationPolicy & "KeyGeneration"

// ObjectStoreSecuritySpec is spec to define security features like encryption
#ObjectStoreSecuritySpec: {
	// +optional
	// +nullable
	SecuritySpec?: #SecuritySpec

	// The settings for supporting AWS-SSE:S3 with RGW
	// +optional
	// +nullable
	s3?: #KeyManagementServiceSpec @go(ServerSideEncryptionS3)
}

// KeyManagementServiceSpec represent various details of the KMS server
#KeyManagementServiceSpec: {
	// ConnectionDetails contains the KMS connection details (address, port etc)
	// +optional
	// +nullable
	// +kubebuilder:pruning:PreserveUnknownFields
	connectionDetails?: {[string]: string} @go(ConnectionDetails,map[string]string)

	// TokenSecretName is the kubernetes secret containing the KMS token
	// +optional
	tokenSecretName?: string @go(TokenSecretName)
}

// KeyRotationSpec represents the settings for Key Rotation.
#KeyRotationSpec: {
	// Enabled represents whether the key rotation is enabled.
	// +optional
	// +kubebuilder:default=false
	enabled?: bool @go(Enabled)

	// Schedule represents the cron schedule for key rotation.
	// +optional
	schedule?: string @go(Schedule)
}

// CephVersionSpec represents the settings for the Ceph version that Rook is orchestrating.
#CephVersionSpec: {
	// Image is the container image used to launch the ceph daemons, such as quay.io/ceph/ceph:<tag>
	// The full list of images can be found at https://quay.io/repository/ceph/ceph?tab=tags
	// +optional
	image?: string @go(Image)

	// Whether to allow unsupported versions (do not set to true in production)
	// +optional
	allowUnsupported?: bool @go(AllowUnsupported)

	// ImagePullPolicy describes a policy for if/when to pull a container image
	// One of Always, Never, IfNotPresent.
	// +kubebuilder:validation:Enum=IfNotPresent;Always;Never;""
	// +optional
	imagePullPolicy?: v1.#PullPolicy @go(ImagePullPolicy)
}

// DashboardSpec represents the settings for the Ceph dashboard
#DashboardSpec: {
	// Enabled determines whether to enable the dashboard
	// +optional
	enabled?: bool @go(Enabled)

	// URLPrefix is a prefix for all URLs to use the dashboard with a reverse proxy
	// +optional
	urlPrefix?: string @go(URLPrefix)

	// Port is the dashboard webserver port
	// +kubebuilder:validation:Minimum=0
	// +kubebuilder:validation:Maximum=65535
	// +optional
	port?: int @go(Port)

	// SSL determines whether SSL should be used
	// +optional
	ssl?: bool @go(SSL)

	// Endpoint for the Prometheus host
	// +optional
	prometheusEndpoint?: string @go(PrometheusEndpoint)

	// Whether to verify the ssl endpoint for prometheus. Set to false for a self-signed cert.
	// +optional
	prometheusEndpointSSLVerify?: bool @go(PrometheusEndpointSSLVerify)
}

// MonitoringSpec represents the settings for Prometheus based Ceph monitoring
#MonitoringSpec: {
	// Enabled determines whether to create the prometheus rules for the ceph cluster. If true, the prometheus
	// types must exist or the creation will fail. Default is false.
	// +optional
	enabled?: bool @go(Enabled)

	// Whether to disable the metrics reported by Ceph. If false, the prometheus mgr module and Ceph exporter are enabled.
	// If true, the prometheus mgr module and Ceph exporter are both disabled. Default is false.
	// +optional
	metricsDisabled?: bool @go(MetricsDisabled)

	// ExternalMgrEndpoints points to an existing Ceph prometheus exporter endpoint
	// +optional
	// +nullable
	externalMgrEndpoints?: [...v1.#EndpointAddress] @go(ExternalMgrEndpoints,[]v1.EndpointAddress)

	// ExternalMgrPrometheusPort Prometheus exporter port
	// +kubebuilder:validation:Minimum=0
	// +kubebuilder:validation:Maximum=65535
	// +optional
	externalMgrPrometheusPort?: uint16 @go(ExternalMgrPrometheusPort)

	// Port is the prometheus server port
	// +kubebuilder:validation:Minimum=0
	// +kubebuilder:validation:Maximum=65535
	// +optional
	port?: int @go(Port)

	// Interval determines prometheus scrape interval
	// +optional
	interval?: null | metav1.#Duration @go(Interval,*metav1.Duration)

	// Ceph exporter configuration
	// +optional
	exporter?: null | #CephExporterSpec @go(Exporter,*CephExporterSpec)
}

#CephExporterSpec: {
	// Only performance counters greater than or equal to this option are fetched
	// +kubebuilder:default=5
	perfCountersPrioLimit?: int64 @go(PerfCountersPrioLimit)

	// Time to wait before sending requests again to exporter server (seconds)
	// +kubebuilder:default=5
	statsPeriodSeconds?: int64 @go(StatsPeriodSeconds)

	// Whether host networking is enabled for CephExporter. If not set, the network settings from CephCluster.spec.networking will be applied.
	// +nullable
	// +optional
	hostNetwork?: null | bool @go(HostNetwork,*bool)
}

// ClusterStatus represents the status of a Ceph cluster
#ClusterStatus: {
	state?:   #ClusterState  @go(State)
	phase?:   #ConditionType @go(Phase)
	message?: string         @go(Message)
	conditions?: [...#Condition] @go(Conditions,[]Condition)
	ceph?:    null | #CephStatus     @go(CephStatus,*CephStatus)
	cephx?:   #ClusterCephxStatus    @go(Cephx)
	storage?: null | #CephStorage    @go(CephStorage,*CephStorage)
	version?: null | #ClusterVersion @go(CephVersion,*ClusterVersion)

	// ObservedGeneration is the latest generation observed by the controller.
	// +optional
	observedGeneration?: int64 @go(ObservedGeneration)
}

// CephDaemonsVersions show the current ceph version for different ceph daemons
#CephDaemonsVersions: {
	// Mon shows Mon Ceph version
	// +optional
	mon?: {[string]: int} @go(Mon,map[string]int)

	// Mgr shows Mgr Ceph version
	// +optional
	mgr?: {[string]: int} @go(Mgr,map[string]int)

	// Osd shows Osd Ceph version
	// +optional
	osd?: {[string]: int} @go(Osd,map[string]int)

	// Rgw shows Rgw Ceph version
	// +optional
	rgw?: {[string]: int} @go(Rgw,map[string]int)

	// Mds shows Mds Ceph version
	// +optional
	mds?: {[string]: int} @go(Mds,map[string]int)

	// RbdMirror shows RbdMirror Ceph version
	// +optional
	"rbd-mirror"?: {[string]: int} @go(RbdMirror,map[string]int)

	// CephFSMirror shows CephFSMirror Ceph version
	// +optional
	"cephfs-mirror"?: {[string]: int} @go(CephFSMirror,map[string]int)

	// Overall shows overall Ceph version
	// +optional
	overall?: {[string]: int} @go(Overall,map[string]int)
}

// CephStatus is the details health of a Ceph Cluster
#CephStatus: {
	health?: string @go(Health)
	details?: {[string]: #CephHealthMessage} @go(Details,map[string]CephHealthMessage)
	lastChecked?:    string    @go(LastChecked)
	lastChanged?:    string    @go(LastChanged)
	previousHealth?: string    @go(PreviousHealth)
	capacity?:       #Capacity @go(Capacity)

	// +optional
	versions?: null | #CephDaemonsVersions @go(Versions,*CephDaemonsVersions)
	fsid?:     string                      @go(FSID)
}

// Capacity is the capacity information of a Ceph Cluster
#Capacity: {
	bytesTotal?:     uint64 @go(TotalBytes)
	bytesUsed?:      uint64 @go(UsedBytes)
	bytesAvailable?: uint64 @go(AvailableBytes)
	lastUpdated?:    string @go(LastUpdated)
}

// CephStorage represents flavors of Ceph Cluster Storage
#CephStorage: {
	deviceClasses?: [...#DeviceClasses] @go(DeviceClasses,[]DeviceClasses)
	osd?: #OSDStatus @go(OSD)
	deprecatedOSDs?: {[string]: [...int]} @go(DeprecatedOSDs,map[string][]int)
}

// DeviceClasses represents device classes of a Ceph Cluster
#DeviceClasses: {
	name?: string @go(Name)
}

// OSDStatus represents OSD status of the ceph Cluster
#OSDStatus: {
	// StoreType is a mapping between the OSD backend stores and number of OSDs using these stores
	storeType?: {[string]: int} @go(StoreType,map[string]int)
	migrationStatus?: #MigrationStatus @go(MigrationStatus)
}

// MigrationStatus status represents the current status of any OSD migration.
#MigrationStatus: {
	pending?: int @go(Pending)
}

// ClusterVersion represents the version of a Ceph Cluster
#ClusterVersion: {
	image?:   string @go(Image)
	version?: string @go(Version)
}

// CephHealthMessage represents the health message of a Ceph Cluster
#CephHealthMessage: {
	severity: string @go(Severity)
	message:  string @go(Message)
}

// Condition represents a status condition on any Rook-Ceph Custom Resource.
#Condition: {
	type?:               #ConditionType      @go(Type)
	status?:             v1.#ConditionStatus @go(Status)
	reason?:             #ConditionReason    @go(Reason)
	message?:            string              @go(Message)
	lastHeartbeatTime?:  metav1.#Time        @go(LastHeartbeatTime)
	lastTransitionTime?: metav1.#Time        @go(LastTransitionTime)
}

// ConditionReason is a reason for a condition
#ConditionReason: string // #enumConditionReason

#enumConditionReason:
	#ClusterCreatedReason |
	#ClusterConnectedReason |
	#ClusterProgressingReason |
	#ClusterDeletingReason |
	#ClusterConnectingReason |
	#ReconcileSucceeded |
	#ReconcileFailed |
	#ReconcileStarted |
	#ReconcileRequeuing |
	#DeletingReason |
	#ObjectHasDependentsReason |
	#ObjectHasNoDependentsReason |
	#PoolNotEmptyReason |
	#PoolEmptyReason |
	#RadosNamespaceNotEmptyReason |
	#RadosNamespaceEmptyReason

// ClusterCreatedReason is cluster created reason
#ClusterCreatedReason: #ConditionReason & "ClusterCreated"

// ClusterConnectedReason is cluster connected reason
#ClusterConnectedReason: #ConditionReason & "ClusterConnected"

// ClusterProgressingReason is cluster progressing reason
#ClusterProgressingReason: #ConditionReason & "ClusterProgressing"

// ClusterDeletingReason is cluster deleting reason
#ClusterDeletingReason: #ConditionReason & "ClusterDeleting"

// ClusterConnectingReason is cluster connecting reason
#ClusterConnectingReason: #ConditionReason & "ClusterConnecting"

// ReconcileSucceeded represents when a resource reconciliation was successful.
#ReconcileSucceeded: #ConditionReason & "ReconcileSucceeded"

// ReconcileFailed represents when a resource reconciliation failed.
#ReconcileFailed: #ConditionReason & "ReconcileFailed"

// ReconcileStarted represents when a resource reconciliation started.
#ReconcileStarted: #ConditionReason & "ReconcileStarted"

// ReconcileRequeuing represents when a resource reconciliation requeue.
#ReconcileRequeuing: #ConditionReason & "ReconcileRequeuing"

// DeletingReason represents when Rook has detected a resource object should be deleted.
#DeletingReason: #ConditionReason & "Deleting"

// ObjectHasDependentsReason represents when a resource object has dependents that are blocking
// deletion.
#ObjectHasDependentsReason: #ConditionReason & "ObjectHasDependents"

// ObjectHasNoDependentsReason represents when a resource object has no dependents that are
// blocking deletion.
#ObjectHasNoDependentsReason: #ConditionReason & "ObjectHasNoDependents"

// PoolNotEmptyReason represents when a pool contains images or snapshots that are blocking
// deletion.
#PoolNotEmptyReason: #ConditionReason & "PoolNotEmpty"

// PoolEmptyReason represents when a pool does not contain images or snapshots that are blocking
// deletion.
#PoolEmptyReason: #ConditionReason & "PoolEmpty"

// RadosNamespaceNotEmptyReason represents when a rados namespace contains images or snapshots that are blocking
// deletion.
#RadosNamespaceNotEmptyReason: #ConditionReason & "RadosNamespaceNotEmpty"

// RadosNamespaceEmptyReason represents when a rados namespace does not contain images or snapshots that are blocking
// deletion.
#RadosNamespaceEmptyReason: #ConditionReason & "RadosNamespaceEmpty"

// ConditionType represent a resource's status
#ConditionType: string // #enumConditionType

#enumConditionType:
	#ConditionConnecting |
	#ConditionConnected |
	#ConditionProgressing |
	#ConditionReady |
	#ConditionFailure |
	#ConditionDeleting |
	#ConditionDeletionIsBlocked |
	#ConditionPoolDeletionIsBlocked |
	#ConditionRadosNSDeletionIsBlocked

// ConditionConnecting represents Connecting state of an object
#ConditionConnecting: #ConditionType & "Connecting"

// ConditionConnected represents Connected state of an object
#ConditionConnected: #ConditionType & "Connected"

// ConditionProgressing represents Progressing state of an object
#ConditionProgressing: #ConditionType & "Progressing"

// ConditionReady represents Ready state of an object
#ConditionReady: #ConditionType & "Ready"

// ConditionFailure represents Failure state of an object
#ConditionFailure: #ConditionType & "Failure"

// ConditionDeleting represents Deleting state of an object
#ConditionDeleting: #ConditionType & "Deleting"

// ConditionDeletionIsBlocked represents when deletion of the object is blocked.
#ConditionDeletionIsBlocked: #ConditionType & "DeletionIsBlocked"

// ConditionPoolDeletionIsBlocked represents when deletion of the object is blocked.
#ConditionPoolDeletionIsBlocked: #ConditionType & "PoolDeletionIsBlocked"

// ConditionRadosNSDeletionIsBlocked represents when deletion of the object is blocked.
#ConditionRadosNSDeletionIsBlocked: #ConditionType & "RadosNamespaceDeletionIsBlocked"

// ClusterState represents the state of a Ceph Cluster
#ClusterState: string // #enumClusterState

#enumClusterState:
	#ClusterStateCreating |
	#ClusterStateCreated |
	#ClusterStateUpdating |
	#ClusterStateConnecting |
	#ClusterStateConnected |
	#ClusterStateError

// ClusterStateCreating represents the Creating state of a Ceph Cluster
#ClusterStateCreating: #ClusterState & "Creating"

// ClusterStateCreated represents the Created state of a Ceph Cluster
#ClusterStateCreated: #ClusterState & "Created"

// ClusterStateUpdating represents the Updating state of a Ceph Cluster
#ClusterStateUpdating: #ClusterState & "Updating"

// ClusterStateConnecting represents the Connecting state of a Ceph Cluster
#ClusterStateConnecting: #ClusterState & "Connecting"

// ClusterStateConnected represents the Connected state of a Ceph Cluster
#ClusterStateConnected: #ClusterState & "Connected"

// ClusterStateError represents the Error state of a Ceph Cluster
#ClusterStateError: #ClusterState & "Error"

#CephxStatus: {
	// KeyGeneration represents the CephX key generation for the last successful reconcile.
	// For all newly-created resources, this field is set to `1`.
	// When keys are rotated due to any rotation policy, the generation is incremented or updated to
	// the configured policy generation.
	// Generation `0` indicates that keys existed prior to the implementation of key tracking.
	keyGeneration?: uint32 @go(KeyGeneration)

	// KeyCephVersion reports the Ceph version that created the current generation's keys. This is
	// same string format as reported by `CephCluster.status.version.version` to allow them to be
	// compared. E.g., `20.2.0-0`.
	// For all newly-created resources, this field set to the version of Ceph that created the key.
	// The special value "Uninitialized" indicates that keys are being created for the first time.
	// An empty string indicates that the version is unknown, as expected in brownfield deployments.
	keyCephVersion?: string @go(KeyCephVersion)
}

#CephxStatusWithKeyCount: {
	#CephxStatus

	// PriorKeyCount reports the number of prior-generation CephX keys that remain active for the related component
	priorKeyCount?: uint8 @go(PriorKeyCount)
}

#UninitializedCephxKeyCephVersion: "Uninitialized"

#LocalCephxStatus: {
	// Daemon shows the CephX key status for local Ceph daemons associated with this resources.
	daemon?: #CephxStatus @go(Daemon)
}

// ClusterCephxStatus defines the cephx key rotation status of various daemons on the cephCluster resource
#ClusterCephxStatus: {
	// Admin shows the CephX key status for the client.admin key
	admin?: #CephxStatus @go(Admin)

	// Mon represents the CephX key status of the Monitor daemons
	mon?: #CephxStatus @go(Mon)

	// Mgr represents the cephx key rotation status of the ceph manager daemon
	mgr?: #CephxStatus @go(Mgr)

	// OSD shows the CephX key status of of OSDs
	osd?: #CephxStatus @go(OSD)

	// CSI shows the CephX key status for Ceph-CSI components.
	csi?: #CephxStatusWithKeyCount @go(CSI)

	// RBDMirrorPeer represents the cephx key rotation status of the `rbd-mirror-peer` user
	rbdMirrorPeer?: #CephxStatus @go(RBDMirrorPeer)

	// Crash Collector represents the cephx key rotation status of the crash collector daemon
	crashCollector?: #CephxStatus @go(CrashCollector)

	// Ceph Exporter represents the cephx key rotation status of the ceph exporter daemon
	cephExporter?: #CephxStatus @go(CephExporter)
}

// MonSpec represents the specification of the monitor
// +kubebuilder:validation:XValidation:message="zones must be less than or equal to count",rule="!has(self.zones) || (has(self.zones) && (size(self.zones) <= self.count))"
// +kubebuilder:validation:XValidation:message="stretchCluster zones must be equal to 3",rule="!has(self.stretchCluster) || (has(self.stretchCluster) && (size(self.stretchCluster.zones) > 0) && (size(self.stretchCluster.zones) == 3))"
#MonSpec: {
	// Count is the number of Ceph monitors
	// +kubebuilder:validation:Minimum=0
	// +kubebuilder:validation:Maximum=9
	// +optional
	count?: int @go(Count)

	// AllowMultiplePerNode determines if we can run multiple monitors on the same node (not recommended)
	// +optional
	allowMultiplePerNode?: bool @go(AllowMultiplePerNode)

	// +optional
	failureDomainLabel?: string @go(FailureDomainLabel)

	// Zones are specified when we want to provide zonal awareness to mons
	// +optional
	zones?: [...#MonZoneSpec] @go(Zones,[]MonZoneSpec)

	// StretchCluster is the stretch cluster specification
	// +optional
	stretchCluster?: null | #StretchClusterSpec @go(StretchCluster,*StretchClusterSpec)

	// VolumeClaimTemplate is the PVC definition
	// +kubebuilder:pruning:PreserveUnknownFields
	// +optional
	volumeClaimTemplate?: null | #VolumeClaimTemplate @go(VolumeClaimTemplate,*VolumeClaimTemplate)

	// ExternalMonIDs - optional list of monitor IDs which are deployed externally and not managed by Rook.
	// If set, Rook will not remove mons with given IDs from quorum.
	// This parameter is used only for local Rook cluster running in normal mode
	// and will be ignored if external or stretched mode is used.
	// leading
	// +optional
	externalMonIDs?: [...string] @go(ExternalMonIDs,[]string)
}

// VolumeClaimTemplate is a simplified version of K8s corev1's PVC. It has no type meta or status.
#VolumeClaimTemplate: {
	// Standard object's metadata.
	// More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata
	// +optional
	metadata?: metav1.#ObjectMeta @go(ObjectMeta) @protobuf(1,bytes,opt)

	// spec defines the desired characteristics of a volume requested by a pod author.
	// More info: https://kubernetes.io/docs/concepts/storage/persistent-volumes#persistentvolumeclaims
	// +optional
	spec?: v1.#PersistentVolumeClaimSpec @go(Spec) @protobuf(2,bytes,opt)
}

// StretchClusterSpec represents the specification of a stretched Ceph Cluster
#StretchClusterSpec: {
	// FailureDomainLabel the failure domain name (e,g: zone)
	// +optional
	failureDomainLabel?: string @go(FailureDomainLabel)

	// SubFailureDomain is the failure domain within a zone
	// +optional
	subFailureDomain?: string @go(SubFailureDomain)

	// Zones is the list of zones
	// +optional
	// +nullable
	zones?: [...#MonZoneSpec] @go(Zones,[]MonZoneSpec)
}

// MonZoneSpec represents the specification of a zone in a Ceph Cluster
#MonZoneSpec: {
	// Name is the name of the zone
	// +optional
	name?: string @go(Name)

	// Arbiter determines if the zone contains the arbiter used for stretch cluster mode
	// +optional
	arbiter?: bool @go(Arbiter)

	// VolumeClaimTemplate is the PVC template
	// +kubebuilder:pruning:PreserveUnknownFields
	// +optional
	volumeClaimTemplate?: null | #VolumeClaimTemplate @go(VolumeClaimTemplate,*VolumeClaimTemplate)
}

// MgrSpec represents options to configure a ceph mgr
#MgrSpec: {
	// Count is the number of manager daemons to run
	// +kubebuilder:validation:Minimum=0
	// +kubebuilder:validation:Maximum=5
	// +optional
	count?: int @go(Count)

	// AllowMultiplePerNode allows to run multiple managers on the same node (not recommended)
	// +optional
	allowMultiplePerNode?: bool @go(AllowMultiplePerNode)

	// Modules is the list of ceph manager modules to enable/disable
	// +optional
	// +nullable
	modules?: [...#Module] @go(Modules,[]Module)
}

// Module represents mgr modules that the user wants to enable or disable
#Module: {
	// Name is the name of the ceph manager module
	// +optional
	name?: string @go(Name)

	// Enabled determines whether a module should be enabled or not
	// +optional
	enabled?: bool @go(Enabled)

	// Settings to further configure the module
	settings?: #ModuleSettings @go(Settings)
}

#ModuleSettings: {
	// BalancerMode sets the `balancer` module with different modes like `upmap`, `crush-compact` etc
	// +kubebuilder:validation:Enum="";crush-compat;upmap;read;upmap-read
	balancerMode?: string @go(BalancerMode)
}

// ExternalSpec represents the options supported by an external cluster
// +kubebuilder:pruning:PreserveUnknownFields
// +nullable
#ExternalSpec: {
	// Enable determines whether external mode is enabled or not
	// +optional
	enable?: bool @go(Enable)
}

// CrashCollectorSpec represents options to configure the crash controller
#CrashCollectorSpec: {
	// Disable determines whether we should enable the crash collector
	// +optional
	disable?: bool @go(Disable)

	// DaysToRetain represents the number of days to retain crash until they get pruned
	// +optional
	daysToRetain?: uint @go(DaysToRetain)
}

// CephBlockPool represents a Ceph Storage Pool
// +kubebuilder:printcolumn:name="Phase",type=string,JSONPath=`.status.phase`
// +kubebuilder:printcolumn:name="Type",type=string,JSONPath=`.status.info.type`
// +kubebuilder:printcolumn:name="FailureDomain",type=string,JSONPath=`.status.info.failureDomain`
// +kubebuilder:printcolumn:name="Replication",type=integer,JSONPath=`.spec.replicated.size`,priority=1
// +kubebuilder:printcolumn:name="EC-CodingChunks",type=integer,JSONPath=`.spec.erasureCoded.codingChunks`,priority=1
// +kubebuilder:printcolumn:name="EC-DataChunks",type=integer,JSONPath=`.spec.erasureCoded.dataChunks`,priority=1
// +kubebuilder:printcolumn:name="Age",type=date,JSONPath=`.metadata.creationTimestamp`
// +kubebuilder:subresource:status
// +kubebuilder:resource:shortName=cephbp
#CephBlockPool: {
	metav1.#TypeMeta
	metadata: metav1.#ObjectMeta  @go(ObjectMeta)
	spec:     #NamedBlockPoolSpec @go(Spec)

	// +kubebuilder:pruning:PreserveUnknownFields
	status?: null | #CephBlockPoolStatus @go(Status,*CephBlockPoolStatus)
}

// CephBlockPoolList is a list of Ceph Storage Pools
// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object
#CephBlockPoolList: {
	metav1.#TypeMeta
	metadata: metav1.#ListMeta @go(ListMeta)
	items: [...#CephBlockPool] @go(Items,[]CephBlockPool)
}

// DefaultFailureDomain for PoolSpec
#DefaultFailureDomain: "host"

// DefaultCRUSHRoot is the default name of the CRUSH root bucket
#DefaultCRUSHRoot: "default"

// PoolSpec represents the spec of ceph pool
#PoolSpec: {
	// The failure domain: osd/host/(region or zone if available) - technically also any type in the crush map
	// +optional
	failureDomain?: string @go(FailureDomain)

	// The root of the crush hierarchy utilized by the pool
	// +optional
	// +nullable
	crushRoot?: string @go(CrushRoot)

	// The device class the OSD should set to for use in the pool
	// +optional
	// +nullable
	deviceClass?: string @go(DeviceClass)

	// Allow rook operator to change the pool CRUSH tunables once the pool is created
	// +optional
	enableCrushUpdates?: bool @go(EnableCrushUpdates)

	// DEPRECATED: use Parameters instead, e.g., Parameters["compression_mode"] = "force"
	// The inline compression mode in Bluestore OSD to set to (options are: none, passive, aggressive, force)
	// +kubebuilder:validation:Enum=none;passive;aggressive;force;""
	// Do NOT set a default value for kubebuilder as this will override the Parameters
	// +optional
	// +nullable
	compressionMode?: string @go(CompressionMode)

	// The replication settings
	// +optional
	replicated?: #ReplicatedSpec @go(Replicated)

	// The erasure code settings
	// +optional
	erasureCoded?: #ErasureCodedSpec @go(ErasureCoded)

	// Parameters is a list of properties to enable on a given pool
	// +kubebuilder:pruning:PreserveUnknownFields
	// +optional
	// +nullable
	parameters?: {[string]: string} @go(Parameters,map[string]string)

	// EnableRBDStats is used to enable gathering of statistics for all RBD images in the pool
	enableRBDStats?: bool @go(EnableRBDStats)

	// The mirroring settings
	mirroring?: #MirroringSpec @go(Mirroring)

	// The mirroring statusCheck
	// +kubebuilder:pruning:PreserveUnknownFields
	statusCheck?: #MirrorHealthCheckSpec @go(StatusCheck)

	// The quota settings
	// +optional
	// +nullable
	quotas?: #QuotaSpec @go(Quotas)

	// The application name to set on the pool. Only expected to be set for rgw pools.
	// +optional
	application?: string @go(Application)
}

// NamedBlockPoolSpec allows a block pool to be created with a non-default name.
// This is more specific than the NamedPoolSpec so we get schema validation on the
// allowed pool names that can be specified.
#NamedBlockPoolSpec: {
	// The desired name of the pool if different from the CephBlockPool CR name.
	// +kubebuilder:validation:Enum=.rgw.root;.nfs;.mgr
	// +optional
	name?: string @go(Name)

	#PoolSpec
}

// NamedPoolSpec represents the named ceph pool spec
#NamedPoolSpec: {
	// Name of the pool
	name?: string @go(Name)

	#PoolSpec
}

// MirrorHealthCheckSpec represents the health specification of a Ceph Storage Pool mirror
#MirrorHealthCheckSpec: {
	// +optional
	// +nullable
	mirror?: #HealthCheckSpec @go(Mirror)
}

// CephBlockPoolStatus represents the mirroring status of Ceph Storage Pool
#CephBlockPoolStatus: {
	// +optional
	phase?: #ConditionType @go(Phase)

	// +optional
	cephx?: #PeerTokenCephxStatus @go(Cephx)

	// +optional
	mirroringStatus?: null | #MirroringStatusSpec @go(MirroringStatus,*MirroringStatusSpec)

	// +optional
	mirroringInfo?: null | #MirroringInfoSpec @go(MirroringInfo,*MirroringInfoSpec)

	// optional
	poolID?: int @go(PoolID)

	// +optional
	snapshotScheduleStatus?: null | #SnapshotScheduleStatusSpec @go(SnapshotScheduleStatus,*SnapshotScheduleStatusSpec)

	// +optional
	// +nullable
	info?: {[string]: string} @go(Info,map[string]string)

	// ObservedGeneration is the latest generation observed by the controller.
	// +optional
	observedGeneration?: int64 @go(ObservedGeneration)
	conditions?: [...#Condition] @go(Conditions,[]Condition)
}

// PeerTokenCephxStatus represents the cephx key rotation status for peer tokens
#PeerTokenCephxStatus: {
	// PeerToken shows the rotation status of the peer token associated with the `rbd-mirror-peer` user.
	peerToken?: #CephxStatus @go(PeerToken)
}

// MirroringStatusSpec is the status of the pool/radosNamespace mirroring
#MirroringStatusSpec: {
	#MirroringStatus

	// LastChecked is the last time time the status was checked
	// +optional
	lastChecked?: string @go(LastChecked)

	// LastChanged is the last time time the status last changed
	// +optional
	lastChanged?: string @go(LastChanged)

	// Details contains potential status errors
	// +optional
	details?: string @go(Details)
}

// MirroringStatus is the pool/radosNamespace mirror status
#MirroringStatus: {
	// Summary is the mirroring status summary
	// +optional
	summary?: null | #MirroringStatusSummarySpec @go(Summary,*MirroringStatusSummarySpec)
}

// MirroringStatusSummarySpec is the summary output of the command
#MirroringStatusSummarySpec: {
	// Health is the mirroring health
	// +optional
	health?: string @go(Health)

	// DaemonHealth is the health of the mirroring daemon
	// +optional
	daemon_health?: string @go(DaemonHealth)

	// ImageHealth is the health of the mirrored image
	// +optional
	image_health?: string @go(ImageHealth)

	// States is the various state for all mirrored images
	// +optional
	// +nullable
	states?: #StatesSpec @go(States)

	// ImageStates is the various state for all mirrored images
	// +optional
	// +nullable
	image_states?: null | #StatesSpec @go(ImageStates,*StatesSpec)

	// GroupHealth is the health of the mirrored image group
	// +optional
	// +nullable
	group_health?: string @go(GroupHealth)

	// GroupStates is the various state for all mirrored image groups
	// +optional
	// +nullable
	group_states?: #StatesSpec @go(GroupStates)
}

// StatesSpec are rbd images mirroring state
#StatesSpec: {
	// StartingReplay is when the replay of the mirroring journal starts
	// +optional
	starting_replay?: int @go(StartingReplay)

	// Replaying is when the replay of the mirroring journal is on-going
	// +optional
	replaying?: int @go(Replaying)

	// Syncing is when the image is syncing
	// +optional
	syncing?: int @go(Syncing)

	// StopReplaying is when the replay of the mirroring journal stops
	// +optional
	stopping_replay?: int @go(StopReplaying)

	// Stopped is when the mirroring state is stopped
	// +optional
	stopped?: int @go(Stopped)

	// Unknown is when the mirroring state is unknown
	// +optional
	unknown?: int @go(Unknown)

	// Error is when the mirroring state is errored
	// +optional
	error?: int @go(Error)
}

// MirroringInfoSpec is the status of the pool/radosnamespace mirroring
#MirroringInfoSpec: {
	#MirroringInfo

	// +optional
	lastChecked?: string @go(LastChecked)

	// +optional
	lastChanged?: string @go(LastChanged)

	// +optional
	details?: string @go(Details)
}

// MirroringInfo is the mirroring info of a given pool/radosnamespace
#MirroringInfo: {
	// Mode is the mirroring mode
	// +optional
	mode?: string @go(Mode)

	// SiteName is the current site name
	// +optional
	site_name?: string @go(SiteName)

	// Peers are the list of peer sites connected to that cluster
	// +optional
	peers?: [...#PeersSpec] @go(Peers,[]PeersSpec)
}

// PeersSpec contains peer details
#PeersSpec: {
	// UUID is the peer UUID
	// +optional
	uuid?: string @go(UUID)

	// Direction is the peer mirroring direction
	// +optional
	direction?: string @go(Direction)

	// SiteName is the current site name
	// +optional
	site_name?: string @go(SiteName)

	// MirrorUUID is the mirror UUID
	// +optional
	mirror_uuid?: string @go(MirrorUUID)

	// ClientName is the CephX user used to connect to the peer
	// +optional
	client_name?: string @go(ClientName)
}

// SnapshotScheduleStatusSpec is the status of the snapshot schedule
#SnapshotScheduleStatusSpec: {
	// SnapshotSchedules is the list of snapshots scheduled
	// +nullable
	// +optional
	snapshotSchedules?: [...#SnapshotSchedulesSpec] @go(SnapshotSchedules,[]SnapshotSchedulesSpec)

	// LastChecked is the last time time the status was checked
	// +optional
	lastChecked?: string @go(LastChecked)

	// LastChanged is the last time time the status last changed
	// +optional
	lastChanged?: string @go(LastChanged)

	// Details contains potential status errors
	// +optional
	details?: string @go(Details)
}

// SnapshotSchedulesSpec is the list of snapshot scheduled for images in a pool
#SnapshotSchedulesSpec: {
	// Pool is the pool name
	// +optional
	pool?: string @go(Pool)

	// Namespace is the RADOS namespace the image is part of
	// +optional
	namespace?: string @go(Namespace)

	// Image is the mirrored image
	// +optional
	image?: string @go(Image)

	// Items is the list schedules times for a given snapshot
	// +optional
	items?: [...#SnapshotSchedule] @go(Items,[]SnapshotSchedule)
}

// SnapshotSchedule is a schedule
#SnapshotSchedule: {
	// Interval is the interval in which snapshots will be taken
	// +optional
	interval?: string @go(Interval)

	// StartTime is the snapshot starting time
	// +optional
	start_time?: string @go(StartTime)
}

// Status represents the status of an object
#Status: {
	// +optional
	phase?: string @go(Phase)

	// ObservedGeneration is the latest generation observed by the controller.
	// +optional
	observedGeneration?: int64 @go(ObservedGeneration)
	conditions?: [...#Condition] @go(Conditions,[]Condition)
}

// ReplicatedSpec represents the spec for replication in a pool
#ReplicatedSpec: {
	// Size - Number of copies per object in a replicated storage pool, including the object itself (required for replicated pool type)
	// +kubebuilder:validation:Minimum=0
	size: uint @go(Size)

	// TargetSizeRatio gives a hint (%) to Ceph in terms of expected consumption of the total cluster capacity
	// +kubebuilder:validation:Minimum=0
	// +optional
	targetSizeRatio?: float64 @go(TargetSizeRatio)

	// RequireSafeReplicaSize if false allows you to set replica 1
	// +optional
	requireSafeReplicaSize?: bool @go(RequireSafeReplicaSize)

	// ReplicasPerFailureDomain the number of replica in the specified failure domain
	// +kubebuilder:validation:Minimum=1
	// +optional
	replicasPerFailureDomain?: uint @go(ReplicasPerFailureDomain)

	// SubFailureDomain the name of the sub-failure domain
	// +optional
	subFailureDomain?: string @go(SubFailureDomain)

	// HybridStorage represents hybrid storage tier settings
	// +optional
	// +nullable
	hybridStorage?: null | #HybridStorageSpec @go(HybridStorage,*HybridStorageSpec)
}

// HybridStorageSpec represents the settings for hybrid storage pool
#HybridStorageSpec: {
	// PrimaryDeviceClass represents high performance tier (for example SSD or NVME) for Primary OSD
	// +kubebuilder:validation:MinLength=1
	// +kubebuilder:validation:Required
	// +required
	primaryDeviceClass: string @go(PrimaryDeviceClass)

	// SecondaryDeviceClass represents low performance tier (for example HDDs) for remaining OSDs
	// +kubebuilder:validation:MinLength=1
	// +kubebuilder:validation:Required
	// +required
	secondaryDeviceClass: string @go(SecondaryDeviceClass)
}

// MirroringSpec represents the setting for a mirrored pool
#MirroringSpec: {
	// Enabled whether this pool is mirrored or not
	// +optional
	enabled?: bool @go(Enabled)

	// Mode is the mirroring mode: pool, image or init-only.
	// +kubebuilder:validation:Enum=pool;image;init-only
	// +optional
	mode?: string @go(Mode)

	// SnapshotSchedules is the scheduling of snapshot for mirrored images/pools
	// +optional
	snapshotSchedules?: [...#SnapshotScheduleSpec] @go(SnapshotSchedules,[]SnapshotScheduleSpec)

	// Peers represents the peers spec
	// +nullable
	// +optional
	peers?: null | #MirroringPeerSpec @go(Peers,*MirroringPeerSpec)
}

// SnapshotScheduleSpec represents the snapshot scheduling settings of a mirrored pool
#SnapshotScheduleSpec: {
	// Path is the path to snapshot, only valid for CephFS
	// +optional
	path?: string @go(Path)

	// Interval represent the periodicity of the snapshot.
	// +optional
	interval?: string @go(Interval)

	// StartTime indicates when to start the snapshot
	// +optional
	startTime?: string @go(StartTime)
}

// QuotaSpec represents the spec for quotas in a pool
#QuotaSpec: {
	// MaxBytes represents the quota in bytes
	// Deprecated in favor of MaxSize
	// +optional
	maxBytes?: null | uint64 @go(MaxBytes,*uint64)

	// MaxSize represents the quota in bytes as a string
	// +kubebuilder:validation:Pattern=`^[0-9]+[\.]?[0-9]*([KMGTPE]i|[kMGTPE])?$`
	// +optional
	maxSize?: null | string @go(MaxSize,*string)

	// MaxObjects represents the quota in objects
	// +optional
	maxObjects?: null | uint64 @go(MaxObjects,*uint64)
}

// ErasureCodedSpec represents the spec for erasure code in a pool
#ErasureCodedSpec: {
	// Number of coding chunks per object in an erasure coded storage pool (required for erasure-coded pool type).
	// This is the number of OSDs that can be lost simultaneously before data cannot be recovered.
	// +kubebuilder:validation:Minimum=0
	codingChunks: uint @go(CodingChunks)

	// Number of data chunks per object in an erasure coded storage pool (required for erasure-coded pool type).
	// The number of chunks required to recover an object when any single OSD is lost is the same
	// as dataChunks so be aware that the larger the number of data chunks, the higher the cost of recovery.
	// +kubebuilder:validation:Minimum=0
	dataChunks: uint @go(DataChunks)

	// The algorithm for erasure coding.
	// If absent, defaults to the plugin specified in osd_pool_default_erasure_code_profile.
	// +kubebuilder:validation:Enum=isa;jerasure
	// +optional
	algorithm?: string @go(Algorithm)
}

// CephFilesystem represents a Ceph Filesystem
// +kubebuilder:printcolumn:name="ActiveMDS",type=string,JSONPath=`.spec.metadataServer.activeCount`,description="Number of desired active MDS daemons"
// +kubebuilder:printcolumn:name="Age",type=date,JSONPath=`.metadata.creationTimestamp`
// +kubebuilder:printcolumn:name="Phase",type=string,JSONPath=`.status.phase`
// +kubebuilder:subresource:status
// +kubebuilder:resource:shortName=cephfs
#CephFilesystem: {
	metav1.#TypeMeta
	metadata: metav1.#ObjectMeta @go(ObjectMeta)
	spec:     #FilesystemSpec    @go(Spec)

	// +kubebuilder:pruning:PreserveUnknownFields
	status?: null | #CephFilesystemStatus @go(Status,*CephFilesystemStatus)
}

// CephFilesystemList represents a list of Ceph Filesystems
#CephFilesystemList: {
	metav1.#TypeMeta
	metadata: metav1.#ListMeta @go(ListMeta)
	items: [...#CephFilesystem] @go(Items,[]CephFilesystem)
}

// FilesystemSpec represents the spec of a file system
#FilesystemSpec: {
	// The metadata pool settings
	// +nullable
	metadataPool: #NamedPoolSpec @go(MetadataPool)

	// The data pool settings, with optional predefined pool name.
	// +nullable
	dataPools: [...#NamedPoolSpec] @go(DataPools,[]NamedPoolSpec)

	// Preserve pool names as specified
	// +optional
	preservePoolNames?: bool @go(PreservePoolNames)

	// Preserve pools on filesystem deletion
	// +optional
	preservePoolsOnDelete?: bool @go(PreservePoolsOnDelete)

	// Preserve the fs in the cluster on CephFilesystem CR deletion. Setting this to true automatically implies PreservePoolsOnDelete is true.
	// +optional
	preserveFilesystemOnDelete?: bool @go(PreserveFilesystemOnDelete)

	// The mds pod info
	metadataServer: #MetadataServerSpec @go(MetadataServer)

	// The mirroring settings
	// +nullable
	// +optional
	mirroring?: null | #FSMirroringSpec @go(Mirroring,*FSMirroringSpec)

	// The mirroring statusCheck
	// +kubebuilder:pruning:PreserveUnknownFields
	statusCheck?: #MirrorHealthCheckSpec @go(StatusCheck)
}

// MetadataServerSpec represents the specification of a Ceph Metadata Server
#MetadataServerSpec: {
	// The number of metadata servers that are active. The remaining servers in the cluster will be in standby mode.
	// +kubebuilder:validation:Minimum=1
	// +kubebuilder:validation:Maximum=50
	activeCount: int32 @go(ActiveCount)

	// Whether each active MDS instance will have an active standby with a warm metadata cache for faster failover.
	// If false, standbys will still be available, but will not have a warm metadata cache.
	// +optional
	activeStandby?: bool @go(ActiveStandby)

	// The affinity to place the mds pods (default is to place on all available node) with a daemonset
	// +kubebuilder:pruning:PreserveUnknownFields
	// +nullable
	// +optional
	placement?: #Placement @go(Placement)

	// The annotations-related configuration to add/set on each Pod related object.
	// +kubebuilder:pruning:PreserveUnknownFields
	// +nullable
	// +optional
	annotations?: #Annotations @go(Annotations)

	// The labels-related configuration to add/set on each Pod related object.
	// +kubebuilder:pruning:PreserveUnknownFields
	// +nullable
	// +optional
	labels?: #Labels @go(Labels)

	// The resource requirements for the mds pods
	// +kubebuilder:pruning:PreserveUnknownFields
	// +nullable
	// +optional
	resources?: v1.#ResourceRequirements @go(Resources)

	// PriorityClassName sets priority classes on components
	// +optional
	priorityClassName?: string @go(PriorityClassName)

	// +optional
	livenessProbe?: null | #ProbeSpec @go(LivenessProbe,*ProbeSpec)

	// +optional
	startupProbe?: null | #ProbeSpec @go(StartupProbe,*ProbeSpec)
}

// FSMirroringSpec represents the setting for a mirrored filesystem
#FSMirroringSpec: {
	// Enabled whether this filesystem is mirrored or not
	// +optional
	enabled?: bool @go(Enabled)

	// Peers represents the peers spec
	// +nullable
	// +optional
	peers?: null | #MirroringPeerSpec @go(Peers,*MirroringPeerSpec)

	// SnapshotSchedules is the scheduling of snapshot for mirrored filesystems
	// +optional
	snapshotSchedules?: [...#SnapshotScheduleSpec] @go(SnapshotSchedules,[]SnapshotScheduleSpec)

	// Retention is the retention policy for a snapshot schedule
	// One path has exactly one retention policy.
	// A policy can however contain multiple count-time period pairs in order to specify complex retention policies
	// +optional
	snapshotRetention?: [...#SnapshotScheduleRetentionSpec] @go(SnapshotRetention,[]SnapshotScheduleRetentionSpec)
}

// SnapshotScheduleRetentionSpec is a retention policy
#SnapshotScheduleRetentionSpec: {
	// Path is the path to snapshot
	// +optional
	path?: string @go(Path)

	// Duration represents the retention duration for a snapshot
	// +optional
	duration?: string @go(Duration)
}

// CephFilesystemStatus represents the status of a Ceph Filesystem
#CephFilesystemStatus: {
	// +optional
	phase?: #ConditionType @go(Phase)

	// +optional
	snapshotScheduleStatus?: null | #FilesystemSnapshotScheduleStatusSpec @go(SnapshotScheduleStatus,*FilesystemSnapshotScheduleStatusSpec)

	// Use only info and put mirroringStatus in it?
	// +optional
	// +nullable
	info?: {[string]: string} @go(Info,map[string]string)
	cephx?: #LocalCephxStatus @go(Cephx)

	// MirroringStatus is the filesystem mirroring status
	// +optional
	mirroringStatus?: null | #FilesystemMirroringInfoSpec @go(MirroringStatus,*FilesystemMirroringInfoSpec)
	conditions?: [...#Condition] @go(Conditions,[]Condition)

	// ObservedGeneration is the latest generation observed by the controller.
	// +optional
	observedGeneration?: int64 @go(ObservedGeneration)
}

// FilesystemMirroringInfo is the status of the pool mirroring
#FilesystemMirroringInfoSpec: {
	// PoolMirroringStatus is the mirroring status of a filesystem
	// +nullable
	// +optional
	daemonsStatus?: [...#FilesystemMirroringInfo] @go(FilesystemMirroringAllInfo,[]FilesystemMirroringInfo)

	// LastChecked is the last time time the status was checked
	// +optional
	lastChecked?: string @go(LastChecked)

	// LastChanged is the last time time the status last changed
	// +optional
	lastChanged?: string @go(LastChanged)

	// Details contains potential status errors
	// +optional
	details?: string @go(Details)
}

// FilesystemSnapshotScheduleStatusSpec is the status of the snapshot schedule
#FilesystemSnapshotScheduleStatusSpec: {
	// SnapshotSchedules is the list of snapshots scheduled
	// +nullable
	// +optional
	snapshotSchedules?: [...#FilesystemSnapshotSchedulesSpec] @go(SnapshotSchedules,[]FilesystemSnapshotSchedulesSpec)

	// LastChecked is the last time time the status was checked
	// +optional
	lastChecked?: string @go(LastChecked)

	// LastChanged is the last time time the status last changed
	// +optional
	lastChanged?: string @go(LastChanged)

	// Details contains potential status errors
	// +optional
	details?: string @go(Details)
}

// FilesystemSnapshotSchedulesSpec is the list of snapshot scheduled for images in a pool
#FilesystemSnapshotSchedulesSpec: {
	// Fs is the name of the Ceph Filesystem
	// +optional
	fs?: string @go(Fs)

	// Subvol is the name of the sub volume
	// +optional
	subvol?: string @go(Subvol)

	// Path is the path on the filesystem
	// +optional
	path?: string @go(Path)

	// +optional
	rel_path?: string @go(RelPath)

	// +optional
	schedule?: string @go(Schedule)

	// +optional
	retention?: #FilesystemSnapshotScheduleStatusRetention @go(Retention)
}

// FilesystemSnapshotScheduleStatusRetention is the retention specification for a filesystem snapshot schedule
#FilesystemSnapshotScheduleStatusRetention: {
	// Start is when the snapshot schedule starts
	// +optional
	start?: string @go(Start)

	// Created is when the snapshot schedule was created
	// +optional
	created?: string @go(Created)

	// First is when the first snapshot schedule was taken
	// +optional
	first?: string @go(First)

	// Last is when the last snapshot schedule was taken
	// +optional
	last?: string @go(Last)

	// LastPruned is when the last snapshot schedule was pruned
	// +optional
	last_pruned?: string @go(LastPruned)

	// CreatedCount is total amount of snapshots
	// +optional
	created_count?: int @go(CreatedCount)

	// PrunedCount is total amount of pruned snapshots
	// +optional
	pruned_count?: int @go(PrunedCount)

	// Active is whether the scheduled is active or not
	// +optional
	active?: bool @go(Active)
}

// FilesystemMirrorInfoSpec is the filesystem mirror status of a given filesystem
#FilesystemMirroringInfo: {
	// DaemonID is the cephfs-mirror name
	// +optional
	daemon_id?: int @go(DaemonID)

	// Filesystems is the list of filesystems managed by a given cephfs-mirror daemon
	// +optional
	filesystems?: [...#FilesystemsSpec] @go(Filesystems,[]FilesystemsSpec)
}

// FilesystemsSpec is spec for the mirrored filesystem
#FilesystemsSpec: {
	// FilesystemID is the filesystem identifier
	// +optional
	filesystem_id?: int @go(FilesystemID)

	// Name is name of the filesystem
	// +optional
	name?: string @go(Name)

	// DirectoryCount is the number of directories in the filesystem
	// +optional
	directory_count?: int @go(DirectoryCount)

	// Peers represents the mirroring peers
	// +optional
	peers?: [...#FilesystemMirrorInfoPeerSpec] @go(Peers,[]FilesystemMirrorInfoPeerSpec)
}

// FilesystemMirrorInfoPeerSpec is the specification of a filesystem peer mirror
#FilesystemMirrorInfoPeerSpec: {
	// UUID is the peer unique identifier
	// +optional
	uuid?: string @go(UUID)

	// Remote are the remote cluster information
	// +optional
	remote?: null | #PeerRemoteSpec @go(Remote,*PeerRemoteSpec)

	// Stats are the stat a peer mirror
	// +optional
	stats?: null | #PeerStatSpec @go(Stats,*PeerStatSpec)
}

#PeerRemoteSpec: {
	// ClientName is cephx name
	// +optional
	client_name?: string @go(ClientName)

	// ClusterName is the name of the cluster
	// +optional
	cluster_name?: string @go(ClusterName)

	// FsName is the filesystem name
	// +optional
	fs_name?: string @go(FsName)
}

// PeerStatSpec are the mirror stat with a given peer
#PeerStatSpec: {
	// FailureCount is the number of mirroring failure
	// +optional
	failure_count?: int @go(FailureCount)

	// RecoveryCount is the number of recovery attempted after failures
	// +optional
	recovery_count?: int @go(RecoveryCount)
}

// CephObjectStore represents a Ceph Object Store Gateway
// +kubebuilder:printcolumn:name="Phase",type=string,JSONPath=`.status.phase`
// +kubebuilder:printcolumn:name="Endpoint",type=string,JSONPath=`.status.info.endpoint`
// +kubebuilder:printcolumn:name="SecureEndpoint",type=string,JSONPath=`.status.info.secureEndpoint`
// +kubebuilder:printcolumn:name="Age",type=date,JSONPath=`.metadata.creationTimestamp`
// +kubebuilder:subresource:status
// +kubebuilder:resource:shortName=cephos
#CephObjectStore: {
	metav1.#TypeMeta
	metadata: metav1.#ObjectMeta @go(ObjectMeta)
	spec:     #ObjectStoreSpec   @go(Spec)

	// +kubebuilder:pruning:PreserveUnknownFields
	status?: null | #ObjectStoreStatus @go(Status,*ObjectStoreStatus)
}

// CephObjectStoreList represents a Ceph Object Store Gateways
#CephObjectStoreList: {
	metav1.#TypeMeta
	metadata: metav1.#ListMeta @go(ListMeta)
	items: [...#CephObjectStore] @go(Items,[]CephObjectStore)
}

// ObjectStoreSpec represent the spec of a pool
// +kubebuilder:validation:XValidation:rule="!(has(self.defaultRealm) && self.defaultRealm == true && has(self.zone) && size(self.zone.name) > 0)",message="defaultRealm must not be true when zone.name is set (multisite configuration)"
#ObjectStoreSpec: {
	// The metadata pool settings
	// +optional
	// +nullable
	metadataPool?: #PoolSpec @go(MetadataPool)

	// The data pool settings
	// +optional
	// +nullable
	dataPool?: #PoolSpec @go(DataPool)

	// The pool information when configuring RADOS namespaces in existing pools.
	// +optional
	// +nullable
	sharedPools?: #ObjectSharedPoolsSpec @go(SharedPools)

	// Preserve pools on object store deletion
	// +optional
	preservePoolsOnDelete?: bool @go(PreservePoolsOnDelete)

	// The rgw pod info
	// +optional
	// +nullable
	gateway?: #GatewaySpec @go(Gateway)

	// The protocol specification
	// +optional
	protocols?: #ProtocolSpec @go(Protocols)

	// The authentication configuration
	// +optional
	auth?: #AuthSpec @go(Auth)

	// The multisite info
	// +optional
	// +nullable
	zone?: #ZoneSpec @go(Zone)

	// The RGW health probes
	// +optional
	// +nullable
	healthCheck?: #ObjectHealthCheckSpec @go(HealthCheck)

	// Security represents security settings
	// +optional
	// +nullable
	security?: null | #ObjectStoreSecuritySpec @go(Security,*ObjectStoreSecuritySpec)

	// The list of allowed namespaces in addition to the object store namespace
	// where ceph object store users may be created. Specify "*" to allow all
	// namespaces, otherwise list individual namespaces that are to be allowed.
	// This is useful for applications that need object store credentials
	// to be created in their own namespace, where neither OBCs nor COSI
	// is being used to create buckets. The default is empty.
	// +optional
	allowUsersInNamespaces?: [...string] @go(AllowUsersInNamespaces,[]string)

	// Hosting settings for the object store.
	// A common use case for hosting configuration is to inform Rook of endpoints that support DNS
	// wildcards, which in turn allows virtual host-style bucket addressing.
	// +nullable
	// +optional
	hosting?: null | #ObjectStoreHostingSpec @go(Hosting,*ObjectStoreHostingSpec)

	// Set this realm as the default in Ceph. Only one realm should be default.
	// Do not set this true on more than one CephObjectStore.
	// This may not be set when zone is also specified; in this case, the realm
	// referenced by the zone's zonegroup should configure defaulting behavior.
	// +optional
	defaultRealm?: bool @go(DefaultRealm)
}

// ObjectSharedPoolsSpec represents object store pool info when configuring RADOS namespaces in existing pools.
#ObjectSharedPoolsSpec: {
	// The metadata pool used for creating RADOS namespaces in the object store
	// +kubebuilder:validation:XValidation:message="object store shared metadata pool is immutable",rule="self == oldSelf"
	// +optional
	metadataPoolName?: string @go(MetadataPoolName)

	// The data pool used for creating RADOS namespaces in the object store
	// +kubebuilder:validation:XValidation:message="object store shared data pool is immutable",rule="self == oldSelf"
	// +optional
	dataPoolName?: string @go(DataPoolName)

	// Whether the RADOS namespaces should be preserved on deletion of the object store
	// +optional
	preserveRadosNamespaceDataOnDelete?: bool @go(PreserveRadosNamespaceDataOnDelete)

	// PoolPlacements control which Pools are associated with a particular RGW bucket.
	// Once PoolPlacements are defined, RGW client will be able to associate pool
	// with ObjectStore bucket by providing "<LocationConstraint>" during s3 bucket creation
	// or "X-Storage-Policy" header during swift container creation.
	// See: https://docs.ceph.com/en/latest/radosgw/placement/#placement-targets
	// PoolPlacement with name: "default" will be used as a default pool if no option
	// is provided during bucket creation.
	// If default placement is not provided, spec.sharedPools.dataPoolName and spec.sharedPools.MetadataPoolName will be used as default pools.
	// If spec.sharedPools are also empty, then RGW pools (spec.dataPool and spec.metadataPool) will be used as defaults.
	// +optional
	poolPlacements?: [...#PoolPlacementSpec] @go(PoolPlacements,[]PoolPlacementSpec)
}

#PoolPlacementSpec: {
	// Pool placement name. Name can be arbitrary. Placement with name "default" will be used as default.
	// +kubebuilder:validation:Required
	// +kubebuilder:validation:MinLength=1
	// +kubebuilder:validation:Pattern=`^[a-zA-Z0-9._/-]+$`
	name: string @go(Name)

	// Sets given placement as default. Only one placement in the list can be marked as default.
	// Default is false.
	// +optional
	default?: bool @go(Default)

	// The metadata pool used to store ObjectStore bucket index.
	// +kubebuilder:validation:Required
	// +kubebuilder:validation:MinLength=1
	metadataPoolName: string @go(MetadataPoolName)

	// The data pool used to store ObjectStore objects data.
	// +kubebuilder:validation:Required
	// +kubebuilder:validation:MinLength=1
	dataPoolName: string @go(DataPoolName)

	// The data pool used to store ObjectStore data that cannot use erasure coding (ex: multi-part uploads).
	// If dataPoolName is not erasure coded, then there is no need for dataNonECPoolName.
	// +optional
	dataNonECPoolName?: string @go(DataNonECPoolName)

	// StorageClasses can be selected by user to override dataPoolName during object creation.
	// Each placement has default STANDARD StorageClass pointing to dataPoolName.
	// This list allows defining additional StorageClasses on top of default STANDARD storage class.
	// +optional
	storageClasses?: [...#PlacementStorageClassSpec] @go(StorageClasses,[]PlacementStorageClassSpec)
}

#PlacementStorageClassSpec: {
	// Name is the StorageClass name. Ceph allows arbitrary name for StorageClasses,
	// however most clients/libs insist on AWS names so it is recommended to use
	// one of the valid x-amz-storage-class values for better compatibility:
	// REDUCED_REDUNDANCY | STANDARD_IA | ONEZONE_IA | INTELLIGENT_TIERING | GLACIER | DEEP_ARCHIVE | OUTPOSTS | GLACIER_IR | SNOW | EXPRESS_ONEZONE
	// See AWS docs: https://aws.amazon.com/de/s3/storage-classes/
	// +kubebuilder:validation:Required
	// +kubebuilder:validation:MinLength=1
	// +kubebuilder:validation:Pattern=`^[a-zA-Z0-9._/-]+$`
	name: string @go(Name)

	// DataPoolName is the data pool used to store ObjectStore objects data.
	// +kubebuilder:validation:Required
	// +kubebuilder:validation:MinLength=1
	dataPoolName: string @go(DataPoolName)
}

// ObjectHealthCheckSpec represents the health check of an object store
#ObjectHealthCheckSpec: {
	// +optional
	readinessProbe?: null | #ProbeSpec @go(ReadinessProbe,*ProbeSpec)

	// +optional
	startupProbe?: null | #ProbeSpec @go(StartupProbe,*ProbeSpec)
}

// HealthCheckSpec represents the health check of an object store bucket
#HealthCheckSpec: {
	// +optional
	disabled?: bool @go(Disabled)

	// Interval is the internal in second or minute for the health check to run like 60s for 60 seconds
	// +optional
	interval?: null | metav1.#Duration @go(Interval,*metav1.Duration)

	// +optional
	timeout?: string @go(Timeout)
}

// GatewaySpec represents the specification of Ceph Object Store Gateway
#GatewaySpec: {
	// The port the rgw service will be listening on (http)
	// +optional
	port?: int32 @go(Port)

	// The port the rgw service will be listening on (https)
	// +kubebuilder:validation:Minimum=0
	// +kubebuilder:validation:Maximum=65535
	// +nullable
	// +optional
	securePort?: int32 @go(SecurePort)

	// The number of pods in the rgw replicaset.
	// +nullable
	// +optional
	instances?: int32 @go(Instances)

	// The name of the secret that stores the ssl certificate for secure rgw connections
	// +nullable
	// +optional
	sslCertificateRef?: string @go(SSLCertificateRef)

	// The name of the secret that stores custom ca-bundle with root and intermediate certificates.
	// +nullable
	// +optional
	caBundleRef?: string @go(CaBundleRef)

	// The affinity to place the rgw pods (default is to place on any available node)
	// +kubebuilder:pruning:PreserveUnknownFields
	// +nullable
	// +optional
	placement?: #Placement @go(Placement)

	// DisableMultisiteSyncTraffic, when true, prevents this object store's gateways from
	// transmitting multisite replication data. Note that this value does not affect whether
	// gateways receive multisite replication traffic: see ObjectZone.spec.customEndpoints for that.
	// If false or unset, this object store's gateways will be able to transmit multisite
	// replication data.
	// +optional
	disableMultisiteSyncTraffic?: bool @go(DisableMultisiteSyncTraffic)

	// The annotations-related configuration to add/set on each Pod related object.
	// +kubebuilder:pruning:PreserveUnknownFields
	// +nullable
	// +optional
	annotations?: #Annotations @go(Annotations)

	// The labels-related configuration to add/set on each Pod related object.
	// +kubebuilder:pruning:PreserveUnknownFields
	// +nullable
	// +optional
	labels?: #Labels @go(Labels)

	// The resource requirements for the rgw pods
	// +kubebuilder:pruning:PreserveUnknownFields
	// +nullable
	// +optional
	resources?: v1.#ResourceRequirements @go(Resources)

	// PriorityClassName sets priority classes on the rgw pods
	// +optional
	priorityClassName?: string @go(PriorityClassName)

	// ExternalRgwEndpoints points to external RGW endpoint(s). Multiple endpoints can be given, but
	// for stability of ObjectBucketClaims, we highly recommend that users give only a single
	// external RGW endpoint that is a load balancer that sends requests to the multiple RGWs.
	// +nullable
	// +optional
	externalRgwEndpoints?: [...#EndpointAddress] @go(ExternalRgwEndpoints,[]EndpointAddress)

	// The configuration related to add/set on each rgw service.
	// +optional
	// +nullable
	service?: null | #RGWServiceSpec @go(Service,*RGWServiceSpec)

	// Enable enhanced operation Logs for S3 in a sidecar named ops-log
	// +optional
	// +nullable
	opsLogSidecar?: null | #OpsLogSidecar @go(OpsLogSidecar,*OpsLogSidecar)

	// Whether host networking is enabled for the rgw daemon. If not set, the network settings from the cluster CR will be applied.
	// +kubebuilder:pruning:PreserveUnknownFields
	// +nullable
	// +optional
	hostNetwork?: null | bool @go(HostNetwork,*bool)

	// Whether rgw dashboard is enabled for the rgw daemon. If not set, the rgw dashboard will be enabled.
	// +kubebuilder:pruning:PreserveUnknownFields
	// +nullable
	// +optional
	dashboardEnabled?: null | bool @go(DashboardEnabled,*bool)

	// AdditionalVolumeMounts allows additional volumes to be mounted to the RGW pod.
	// The root directory for each additional volume mount is `/var/rgw`.
	// Example: for an additional mount at subPath `ldap`, mounted from a secret that has key
	// `bindpass.secret`, the file would reside at `/var/rgw/ldap/bindpass.secret`.
	additionalVolumeMounts?: #AdditionalVolumeMounts @go(AdditionalVolumeMounts)

	// RgwConfig sets Ceph RGW config values for the gateway clients that serve this object store.
	// Values are modified at runtime without RGW restart.
	// This feature is intended for advanced users. It allows breaking configurations to be easily
	// applied. Use with caution.
	// +nullable
	// +optional
	rgwConfig?: {[string]: string} @go(RgwConfig,map[string]string)

	// RgwConfigFromSecret works exactly like RgwConfig but takes config value from Secret Key reference.
	// Values are modified at runtime without RGW restart.
	// This feature is intended for advanced users. It allows breaking configurations to be easily
	// applied. Use with caution.
	// +nullable
	// +optional
	rgwConfigFromSecret?: {[string]: v1.#SecretKeySelector} @go(RgwConfigFromSecret,map[string]v1.SecretKeySelector)

	// RgwCommandFlags sets Ceph RGW config values for the gateway clients that serve this object
	// store. Values are modified at RGW startup, resulting in RGW pod restarts.
	// This feature is intended for advanced users. It allows breaking configurations to be easily
	// applied. Use with caution.
	// +nullable
	// +optional
	rgwCommandFlags?: {[string]: string} @go(RgwCommandFlags,map[string]string)

	// ReadAffinity defines the RGW read affinity policy to optimize the read requests for the RGW clients
	// Note: Only supported from Ceph Tentacle (v20)
	// +optional
	readAffinity?: null | #RgwReadAffinity @go(ReadAffinity,*RgwReadAffinity)
}

#RgwReadAffinity: {
	// Type defines the RGW ReadAffinity type
	// localize: read from the nearest OSD based on crush location of the RGW client
	// balance: picks a random OSD from the PG's active set
	// default: read from the primary OSD
	// +kubebuilder:validation:Enum=localize;balance;default
	// +required
	type: string @go(Type)
}

// RGWLoggingSpec is intended to extend the s3/swift logging for client operations
#OpsLogSidecar: {
	// Resources represents the way to specify resource requirements for the ops-log sidecar
	// +optional
	resources?: v1.#ResourceRequirements @go(Resources)
}

// EndpointAddress is a tuple that describes a single IP address or host name. This is a subset of
// Kubernetes's v1.EndpointAddress.
// +structType=atomic
#EndpointAddress: {
	// The IP of this endpoint. As a legacy behavior, this supports being given a DNS-addressable hostname as well.
	// +optional
	ip?: string @go(IP) @protobuf(1,bytes,opt)

	// The DNS-addressable Hostname of this endpoint. This field will be preferred over IP if both are given.
	// +optional
	hostname?: string @go(Hostname) @protobuf(3,bytes,opt)
}

// ProtocolSpec represents a Ceph Object Store protocol specification
#ProtocolSpec: {
	// Represents RGW 'rgw_enable_apis' config option. See: https://docs.ceph.com/en/reef/radosgw/config-ref/#confval-rgw_enable_apis
	// If no value provided then all APIs will be enabled: s3, s3website, swift, swift_auth, admin, sts, iam, notifications
	// If enabled APIs are set, all remaining APIs will be disabled.
	// This option overrides S3.Enabled value.
	// +optional
	// +nullable
	enableAPIs?: [...#ObjectStoreAPI] @go(EnableAPIs,[]ObjectStoreAPI)

	// The spec for S3
	// +optional
	// +nullable
	s3?: null | #S3Spec @go(S3,*S3Spec)

	// The spec for Swift
	// +optional
	// +nullable
	swift?: null | #SwiftSpec @go(Swift,*SwiftSpec)
}

// +kubebuilder:validation:Enum=s3;s3website;swift;swift_auth;admin;sts;iam;notifications
#ObjectStoreAPI: string

// S3Spec represents Ceph Object Store specification for the S3 API
#S3Spec: {
	// Deprecated: use protocol.enableAPIs instead.
	// Whether to enable S3. This defaults to true (even if protocols.s3 is not present in the CRD). This maintains backwards compatibility – by default S3 is enabled.
	// +nullable
	// +optional
	enabled?: null | bool @go(Enabled,*bool)

	// Whether to use Keystone for authentication. This option maps directly to the rgw_s3_auth_use_keystone option. Enabling it allows generating S3 credentials via an OpenStack API call, see the docs. If not given, the defaults of the corresponding RGW option apply.
	// +nullable
	// +optional
	authUseKeystone?: null | bool @go(AuthUseKeystone,*bool)
}

// SwiftSpec represents Ceph Object Store specification for the Swift API
#SwiftSpec: {
	// Whether or not the Swift account name should be included in the Swift API URL. If set to false (the default), then the Swift API will listen on a URL formed like http://host:port/<rgw_swift_url_prefix>/v1. If set to true, the Swift API URL will be http://host:port/<rgw_swift_url_prefix>/v1/AUTH_<account_name>. You must set this option to true (and update the Keystone service catalog) if you want radosgw to support publicly-readable containers and temporary URLs.
	// +nullable
	// +optional
	accountInUrl?: null | bool @go(AccountInUrl,*bool)

	// The URL prefix for the Swift API, to distinguish it from the S3 API endpoint. The default is swift, which makes the Swift API available at the URL http://host:port/swift/v1 (or http://host:port/swift/v1/AUTH_%(tenant_id)s if rgw swift account in url is enabled).
	// +nullable
	// +optional
	urlPrefix?: null | string @go(UrlPrefix,*string)

	// Enables the Object Versioning of OpenStack Object Storage API. This allows clients to put the X-Versions-Location attribute on containers that should be versioned.
	// +nullable
	// +optional
	versioningEnabled?: null | bool @go(VersioningEnabled,*bool)
}

// AuthSpec represents the authentication protocol configuration of a Ceph Object Store Gateway
#AuthSpec: {
	// The spec for Keystone
	// +optional
	// +nullable
	keystone?: null | #KeystoneSpec @go(Keystone,*KeystoneSpec)
}

// KeystoneSpec represents the Keystone authentication configuration of a Ceph Object Store Gateway
#KeystoneSpec: {
	// The URL for the Keystone server.
	url: string @go(Url)

	// The name of the secret containing the credentials for the service user account used by RGW. It has to be in the same namespace as the object store resource.
	serviceUserSecretName: string @go(ServiceUserSecretName)

	// The roles requires to serve requests.
	acceptedRoles: [...string] @go(AcceptedRoles,[]string)

	// Create new users in their own tenants of the same name. Possible values are true, false, swift and s3. The latter have the effect of splitting the identity space such that only the indicated protocol will use implicit tenants.
	// +optional
	implicitTenants?: #ImplicitTenantSetting @go(ImplicitTenants)

	// The maximum number of entries in each Keystone token cache.
	// +optional
	// +nullable
	tokenCacheSize?: null | int @go(TokenCacheSize,*int)

	// The number of seconds between token revocation checks.
	// +optional
	// +nullable
	revocationInterval?: null | int @go(RevocationInterval,*int)
}

#ImplicitTenantSetting: string // #enumImplicitTenantSetting

#enumImplicitTenantSetting:
	#ImplicitTenantSwift |
	#ImplicitTenantS3 |
	#ImplicitTenantTrue |
	#ImplicitTenantFalse |
	#ImplicitTenantDefault

#ImplicitTenantSwift:   #ImplicitTenantSetting & "swift"
#ImplicitTenantS3:      #ImplicitTenantSetting & "s3"
#ImplicitTenantTrue:    #ImplicitTenantSetting & "true"
#ImplicitTenantFalse:   #ImplicitTenantSetting & "false"
#ImplicitTenantDefault: #ImplicitTenantSetting & ""

// ZoneSpec represents a Ceph Object Store Gateway Zone specification
#ZoneSpec: {
	// CephObjectStoreZone name this CephObjectStore is part of
	name: string @go(Name)
}

// ObjectStoreStatus represents the status of a Ceph Object Store resource
#ObjectStoreStatus: {
	// +optional
	phase?: #ConditionType @go(Phase)

	// +optional
	message?: string @go(Message)

	// +optional
	endpoints?: #ObjectEndpoints @go(Endpoints)

	// +optional
	// +nullable
	info?: {[string]: string} @go(Info,map[string]string)
	cephx?: #LocalCephxStatus @go(Cephx)
	conditions?: [...#Condition] @go(Conditions,[]Condition)

	// ObservedGeneration is the latest generation observed by the controller.
	// +optional
	observedGeneration?: int64 @go(ObservedGeneration)
}

#ObjectEndpoints: {
	// +optional
	// +nullable
	insecure?: [...string] @go(Insecure,[]string)

	// +optional
	// +nullable
	secure?: [...string] @go(Secure,[]string)
}

// ObjectStoreHostingSpec represents the hosting settings for the object store
#ObjectStoreHostingSpec: {
	// AdvertiseEndpoint is the default endpoint Rook will return for resources dependent on this
	// object store. This endpoint will be returned to CephObjectStoreUsers, Object Bucket Claims,
	// and COSI Buckets/Accesses.
	// By default, Rook returns the endpoint for the object store's Kubernetes service using HTTPS
	// with `gateway.securePort` if it is defined (otherwise, HTTP with `gateway.port`).
	// +nullable
	// +optional
	advertiseEndpoint?: null | #ObjectEndpointSpec @go(AdvertiseEndpoint,*ObjectEndpointSpec)

	// A list of DNS host names on which object store gateways will accept client S3 connections.
	// When specified, object store gateways will reject client S3 connections to hostnames that are
	// not present in this list, so include all endpoints.
	// The object store's advertiseEndpoint and Kubernetes service endpoint, plus CephObjectZone
	// `customEndpoints` are automatically added to the list but may be set here again if desired.
	// Each DNS name must be valid according RFC-1123.
	// If the DNS name corresponds to an endpoint with DNS wildcard support, do not include the
	// wildcard itself in the list of hostnames.
	// E.g., use "mystore.example.com" instead of "*.mystore.example.com".
	// +optional
	dnsNames?: [...string] @go(DNSNames,[]string)
}

// ObjectEndpointSpec represents an object store endpoint
#ObjectEndpointSpec: {
	// DnsName is the DNS name (in RFC-1123 format) of the endpoint.
	// If the DNS name corresponds to an endpoint with DNS wildcard support, do not include the
	// wildcard itself in the list of hostnames.
	// E.g., use "mystore.example.com" instead of "*.mystore.example.com".
	// +kubebuilder:validation:MinLength=1
	// +required
	dnsName: string @go(DnsName)

	// Port is the port on which S3 connections can be made for this endpoint.
	// +kubebuilder:validation:Minimum=1
	// +kubebuilder:validation:Maximum=65535
	// +required
	port: int32 @go(Port)

	// UseTls defines whether the endpoint uses TLS (HTTPS) or not (HTTP).
	// +required
	useTls: bool @go(UseTls)
}

// CephObjectStoreUser represents a Ceph Object Store Gateway User
// +kubebuilder:resource:shortName=rcou;objectuser;cephosu
// +kubebuilder:printcolumn:name="Phase",type=string,JSONPath=`.status.phase`
// +kubebuilder:printcolumn:name="Age",type=date,JSONPath=`.metadata.creationTimestamp`
// +kubebuilder:subresource:status
#CephObjectStoreUser: {
	metav1.#TypeMeta
	metadata: metav1.#ObjectMeta   @go(ObjectMeta)
	spec:     #ObjectStoreUserSpec @go(Spec)

	// +kubebuilder:pruning:PreserveUnknownFields
	// +optional
	status?: null | #ObjectStoreUserStatus @go(Status,*ObjectStoreUserStatus)
}

// ObjectStoreUserStatus represents the status Ceph Object Store Gateway User
#ObjectStoreUserStatus: {
	// +optional
	phase?: string @go(Phase)

	// +optional
	// +nullable
	info?: {[string]: string} @go(Info,map[string]string)

	// ObservedGeneration is the latest generation observed by the controller.
	// +optional
	observedGeneration?: int64 @go(ObservedGeneration)

	// +optional
	// +nullable
	keys?: [...#SecretReference] @go(Keys,[]SecretReference)
}

#SecretReference: {
	SecretReference:  v1.#SecretReference
	uid?:             types.#UID @go(UID)
	resourceVersion?: string     @go(ResourceVersion)
}

// CephObjectStoreUserList represents a list Ceph Object Store Gateway Users
#CephObjectStoreUserList: {
	metav1.#TypeMeta
	metadata: metav1.#ListMeta @go(ListMeta)
	items: [...#CephObjectStoreUser] @go(Items,[]CephObjectStoreUser)
}

// ObjectStoreUserSpec represent the spec of an Objectstoreuser
#ObjectStoreUserSpec: {
	// The store the user will be created in
	// +optional
	store?: string @go(Store)

	// The display name for the ceph users
	// +optional
	displayName?: string @go(DisplayName)

	// +optional
	// +nullable
	capabilities?: null | #ObjectUserCapSpec @go(Capabilities,*ObjectUserCapSpec)

	// +optional
	// +nullable
	quotas?: null | #ObjectUserQuotaSpec @go(Quotas,*ObjectUserQuotaSpec)

	// Allows specifying credentials for the user. If not provided, the operator
	// will generate them.
	// +optional
	keys?: [...#ObjectUserKey] @go(Keys,[]ObjectUserKey)

	// The namespace where the parent CephCluster and CephObjectStore are found
	// +optional
	clusterNamespace?: string @go(ClusterNamespace)
}

// Additional admin-level capabilities for the Ceph object store user
#ObjectUserCapSpec: {
	// +optional
	// +kubebuilder:validation:Enum={"*","read","write","read, write"}
	// Admin capabilities to read/write Ceph object store users. Documented in https://docs.ceph.com/en/latest/radosgw/admin/?#add-remove-admin-capabilities
	user?: string @go(User)

	// +optional
	// +kubebuilder:validation:Enum={"*","read","write","read, write"}
	// Admin capabilities to read/write Ceph object store users. Documented in https://docs.ceph.com/en/latest/radosgw/admin/?#add-remove-admin-capabilities
	users?: string @go(Users)

	// +optional
	// +kubebuilder:validation:Enum={"*","read","write","read, write"}
	// Admin capabilities to read/write Ceph object store buckets. Documented in https://docs.ceph.com/en/latest/radosgw/admin/?#add-remove-admin-capabilities
	bucket?: string @go(Bucket)

	// +optional
	// +kubebuilder:validation:Enum={"*","read","write","read, write"}
	// Admin capabilities to read/write Ceph object store buckets. Documented in https://docs.ceph.com/en/latest/radosgw/admin/?#add-remove-admin-capabilities
	buckets?: string @go(Buckets)

	// +optional
	// +kubebuilder:validation:Enum={"*","read","write","read, write"}
	// Admin capabilities to read/write Ceph object store metadata. Documented in https://docs.ceph.com/en/latest/radosgw/admin/?#add-remove-admin-capabilities
	metadata?: string @go(MetaData)

	// +optional
	// +kubebuilder:validation:Enum={"*","read","write","read, write"}
	// Admin capabilities to read/write Ceph object store usage. Documented in https://docs.ceph.com/en/latest/radosgw/admin/?#add-remove-admin-capabilities
	usage?: string @go(Usage)

	// +optional
	// +kubebuilder:validation:Enum={"*","read","write","read, write"}
	// Admin capabilities to read/write Ceph object store zones. Documented in https://docs.ceph.com/en/latest/radosgw/admin/?#add-remove-admin-capabilities
	zone?: string @go(Zone)

	// +optional
	// +kubebuilder:validation:Enum={"*","read","write","read, write"}
	// Admin capabilities to read/write roles for user. Documented in https://docs.ceph.com/en/latest/radosgw/admin/?#add-remove-admin-capabilities
	roles?: string @go(Roles)

	// +optional
	// +kubebuilder:validation:Enum={"*","read","write","read, write"}
	// Admin capabilities to read/write information about the user. Documented in https://docs.ceph.com/en/latest/radosgw/admin/?#add-remove-admin-capabilities
	info?: string @go(Info)

	// +optional
	// +kubebuilder:validation:Enum={"*","read","write","read, write"}
	// Add capabilities for user to send request to RGW Cache API header. Documented in https://docs.ceph.com/en/latest/radosgw/rgw-cache/#cache-api
	"amz-cache"?: string @go(AMZCache)

	// +optional
	// +kubebuilder:validation:Enum={"*","read","write","read, write"}
	// Add capabilities for user to change bucket index logging. Documented in https://docs.ceph.com/en/latest/radosgw/admin/?#add-remove-admin-capabilities
	bilog?: string @go(BiLog)

	// +optional
	// +kubebuilder:validation:Enum={"*","read","write","read, write"}
	// Add capabilities for user to change metadata logging. Documented in https://docs.ceph.com/en/latest/radosgw/admin/?#add-remove-admin-capabilities
	mdlog?: string @go(MdLog)

	// +optional
	// +kubebuilder:validation:Enum={"*","read","write","read, write"}
	// Add capabilities for user to change data logging. Documented in https://docs.ceph.com/en/latest/radosgw/admin/?#add-remove-admin-capabilities
	datalog?: string @go(DataLog)

	// +optional
	// +kubebuilder:validation:Enum={"*","read","write","read, write"}
	// Add capabilities for user to change user policies. Documented in https://docs.ceph.com/en/latest/radosgw/admin/?#add-remove-admin-capabilities
	"user-policy"?: string @go(UserPolicy)

	// +optional
	// +kubebuilder:validation:Enum={"*","read","write","read, write"}
	// Add capabilities for user to change oidc provider. Documented in https://docs.ceph.com/en/latest/radosgw/admin/?#add-remove-admin-capabilities
	"oidc-provider"?: string @go(OidcProvider)

	// +optional
	// +kubebuilder:validation:Enum={"*","read","write","read, write"}
	// Add capabilities for user to set rate limiter for user and bucket. Documented in https://docs.ceph.com/en/latest/radosgw/admin/?#add-remove-admin-capabilities
	ratelimit?: string @go(RateLimit)
}

// ObjectUserQuotaSpec can be used to set quotas for the object store user to limit their usage. See the [Ceph docs](https://docs.ceph.com/en/latest/radosgw/admin/?#quota-management) for more
#ObjectUserQuotaSpec: {
	// Maximum bucket limit for the ceph user
	// +optional
	// +nullable
	maxBuckets?: null | int @go(MaxBuckets,*int)

	// Maximum size limit of all objects across all the user's buckets
	// See https://pkg.go.dev/k8s.io/apimachinery/pkg/api/resource#Quantity for more info.
	// +optional
	// +nullable
	maxSize?: null | resource.#Quantity @go(MaxSize,*resource.Quantity)

	// Maximum number of objects across all the user's buckets
	// +optional
	// +nullable
	maxObjects?: null | int64 @go(MaxObjects,*int64)
}

// ObjectUserKey defines a set of rgw user access credentials to be retrieved
// from secret resources.
#ObjectUserKey: {
	// Secret key selector for the access_key (commonly referred to as AWS_ACCESS_KEY_ID).
	accessKeyRef?: null | v1.#SecretKeySelector @go(AccessKeyRef,*v1.SecretKeySelector)

	// Secret key selector for the secret_key (commonly referred to as AWS_SECRET_ACCESS_KEY).
	secretKeyRef?: null | v1.#SecretKeySelector @go(SecretKeyRef,*v1.SecretKeySelector)
}

// CephObjectRealm represents a Ceph Object Store Gateway Realm
// +kubebuilder:subresource:status
// +kubebuilder:resource:shortName=cephor
#CephObjectRealm: {
	metav1.#TypeMeta
	metadata: metav1.#ObjectMeta @go(ObjectMeta)

	// +nullable
	// +optional
	spec?: #ObjectRealmSpec @go(Spec)

	// +kubebuilder:pruning:PreserveUnknownFields
	// +optional
	status?: null | #Status @go(Status,*Status)
}

// CephObjectRealmList represents a list Ceph Object Store Gateway Realms
// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object
#CephObjectRealmList: {
	metav1.#TypeMeta
	metadata: metav1.#ListMeta @go(ListMeta)
	items: [...#CephObjectRealm] @go(Items,[]CephObjectRealm)
}

// ObjectRealmSpec represent the spec of an ObjectRealm
#ObjectRealmSpec: {
	pull?: #PullSpec @go(Pull)

	// Set this realm as the default in Ceph. Only one realm should be default.
	// +optional
	defaultRealm?: bool @go(DefaultRealm)
}

// PullSpec represents the pulling specification of a Ceph Object Storage Gateway Realm
#PullSpec: {
	// +kubebuilder:validation:Pattern=`^https*://`
	endpoint?: string @go(Endpoint)
}

// CephObjectZoneGroup represents a Ceph Object Store Gateway Zone Group
// +kubebuilder:printcolumn:name="Phase",type=string,JSONPath=`.status.phase`
// +kubebuilder:printcolumn:name="Age",type=date,JSONPath=`.metadata.creationTimestamp`
// +kubebuilder:subresource:status
// +kubebuilder:resource:shortName=cephozg
#CephObjectZoneGroup: {
	metav1.#TypeMeta
	metadata: metav1.#ObjectMeta   @go(ObjectMeta)
	spec:     #ObjectZoneGroupSpec @go(Spec)

	// +kubebuilder:pruning:PreserveUnknownFields
	// +optional
	status?: null | #Status @go(Status,*Status)
}

// CephObjectZoneGroupList represents a list Ceph Object Store Gateway Zone Groups
// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object
#CephObjectZoneGroupList: {
	metav1.#TypeMeta
	metadata: metav1.#ListMeta @go(ListMeta)
	items: [...#CephObjectZoneGroup] @go(Items,[]CephObjectZoneGroup)
}

// ObjectZoneGroupSpec represent the spec of an ObjectZoneGroup
#ObjectZoneGroupSpec: {
	// The display name for the ceph users
	realm: string @go(Realm)
}

// CephObjectZone represents a Ceph Object Store Gateway Zone
// +kubebuilder:printcolumn:name="Phase",type=string,JSONPath=`.status.phase`
// +kubebuilder:printcolumn:name="Age",type=date,JSONPath=`.metadata.creationTimestamp`
// +kubebuilder:subresource:status
// +kubebuilder:resource:shortName=cephoz
#CephObjectZone: {
	metav1.#TypeMeta
	metadata: metav1.#ObjectMeta @go(ObjectMeta)
	spec:     #ObjectZoneSpec    @go(Spec)

	// +kubebuilder:pruning:PreserveUnknownFields
	// +optional
	status?: null | #Status @go(Status,*Status)
}

// CephObjectZoneList represents a list Ceph Object Store Gateway Zones
// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object
#CephObjectZoneList: {
	metav1.#TypeMeta
	metadata: metav1.#ListMeta @go(ListMeta)
	items: [...#CephObjectZone] @go(Items,[]CephObjectZone)
}

// ObjectZoneSpec represent the spec of an ObjectZone
#ObjectZoneSpec: {
	// The display name for the ceph users
	zoneGroup: string @go(ZoneGroup)

	// The metadata pool settings
	// +optional
	// +nullable
	metadataPool?: #PoolSpec @go(MetadataPool)

	// The data pool settings
	// +optional
	// +nullable
	dataPool?: #PoolSpec @go(DataPool)

	// The pool information when configuring RADOS namespaces in existing pools.
	// +optional
	// +nullable
	sharedPools?: #ObjectSharedPoolsSpec @go(SharedPools)

	// If this zone cannot be accessed from other peer Ceph clusters via the ClusterIP Service
	// endpoint created by Rook, you must set this to the externally reachable endpoint(s). You may
	// include the port in the definition. For example: "https://my-object-store.my-domain.net:443".
	// In many cases, you should set this to the endpoint of the ingress resource that makes the
	// CephObjectStore associated with this CephObjectStoreZone reachable to peer clusters.
	// The list can have one or more endpoints pointing to different RGW servers in the zone.
	//
	// If a CephObjectStore endpoint is omitted from this list, that object store's gateways will
	// not receive multisite replication data
	// (see CephObjectStore.spec.gateway.disableMultisiteSyncTraffic).
	// +nullable
	// +optional
	customEndpoints?: [...string] @go(CustomEndpoints,[]string)

	// Preserve pools on object zone deletion
	// +optional
	// +kubebuilder:default=true
	preservePoolsOnDelete?: bool @go(PreservePoolsOnDelete)
}

// CephBucketTopic represents a Ceph Object Topic for Bucket Notifications
// +kubebuilder:printcolumn:name="Phase",type=string,JSONPath=`.status.phase`
// +kubebuilder:printcolumn:name="Age",type=date,JSONPath=`.metadata.creationTimestamp`
// +kubebuilder:subresource:status
// +kubebuilder:resource:shortName=cephbt
#CephBucketTopic: {
	metav1.#TypeMeta
	metadata: metav1.#ObjectMeta @go(ObjectMeta)
	spec:     #BucketTopicSpec   @go(Spec)

	// +kubebuilder:pruning:PreserveUnknownFields
	// +optional
	status?: null | #BucketTopicStatus @go(Status,*BucketTopicStatus)
}

// BucketTopicStatus represents the Status of a CephBucketTopic
#BucketTopicStatus: {
	// +optional
	phase?: string @go(Phase)

	// The ARN of the topic generated by the RGW
	// +optional
	// +nullable
	ARN?: null | string @go(,*string)

	// ObservedGeneration is the latest generation observed by the controller.
	// +optional
	observedGeneration?: int64 @go(ObservedGeneration)

	// +optional
	secrets?: [...#SecretReference] @go(Secrets,[]SecretReference)
}

// CephBucketTopicList represents a list Ceph Object Store Bucket Notification Topics
// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object
#CephBucketTopicList: {
	metav1.#TypeMeta
	metadata: metav1.#ListMeta @go(ListMeta)
	items: [...#CephBucketTopic] @go(Items,[]CephBucketTopic)
}

// BucketTopicSpec represent the spec of a Bucket Topic
#BucketTopicSpec: {
	// The name of the object store on which to define the topic
	// +kubebuilder:validation:MinLength=1
	objectStoreName: string @go(ObjectStoreName)

	// The namespace of the object store on which to define the topic
	// +kubebuilder:validation:MinLength=1
	objectStoreNamespace: string @go(ObjectStoreNamespace)

	// Data which is sent in each event
	// +optional
	opaqueData?: string @go(OpaqueData)

	// Indication whether notifications to this endpoint are persistent or not
	// +optional
	persistent?: bool @go(Persistent)

	// Contains the endpoint spec of the topic
	endpoint: #TopicEndpointSpec @go(Endpoint)
}

// TopicEndpointSpec contains exactly one of the endpoint specs of a Bucket Topic
#TopicEndpointSpec: {
	// Spec of HTTP endpoint
	// +optional
	http?: null | #HTTPEndpointSpec @go(HTTP,*HTTPEndpointSpec)

	// Spec of AMQP endpoint
	// +optional
	amqp?: null | #AMQPEndpointSpec @go(AMQP,*AMQPEndpointSpec)

	// Spec of Kafka endpoint
	// +optional
	kafka?: null | #KafkaEndpointSpec @go(Kafka,*KafkaEndpointSpec)
}

// HTTPEndpointSpec represent the spec of an HTTP endpoint of a Bucket Topic
#HTTPEndpointSpec: {
	// The URI of the HTTP endpoint to push notification to
	// +kubebuilder:validation:MinLength=1
	uri: string @go(URI)

	// Indicate whether the server certificate is validated by the client or not
	// +optional
	disableVerifySSL?: bool @go(DisableVerifySSL)

	// Send the notifications with the CloudEvents header: https://github.com/cloudevents/spec/blob/main/cloudevents/adapters/aws-s3.md
	// +optional
	sendCloudEvents?: bool @go(SendCloudEvents)
}

// AMQPEndpointSpec represent the spec of an AMQP endpoint of a Bucket Topic
#AMQPEndpointSpec: {
	// The URI of the AMQP endpoint to push notification to
	// +kubebuilder:validation:MinLength=1
	uri: string @go(URI)

	// Name of the exchange that is used to route messages based on topics
	// +kubebuilder:validation:MinLength=1
	exchange: string @go(Exchange)

	// Indicate whether the server certificate is validated by the client or not
	// +optional
	disableVerifySSL?: bool @go(DisableVerifySSL)

	// The ack level required for this topic (none/broker/routeable)
	// +kubebuilder:validation:Enum=none;broker;routeable
	// +kubebuilder:default=broker
	// +optional
	ackLevel?: string @go(AckLevel)
}

// KafkaEndpointSpec represent the spec of a Kafka endpoint of a Bucket Topic
#KafkaEndpointSpec: {
	// The URI of the Kafka endpoint to push notification to
	// +kubebuilder:validation:MinLength=1
	uri: string @go(URI)

	// Indicate whether to use SSL when communicating with the broker
	// +optional
	useSSL?: bool @go(UseSSL)

	// Indicate whether the server certificate is validated by the client or not
	// +optional
	disableVerifySSL?: bool @go(DisableVerifySSL)

	// The ack level required for this topic (none/broker)
	// +kubebuilder:validation:Enum=none;broker
	// +kubebuilder:default=broker
	// +optional
	ackLevel?: string @go(AckLevel)

	// The authentication mechanism for this topic (PLAIN/SCRAM-SHA-512/SCRAM-SHA-256/GSSAPI/OAUTHBEARER)
	// +kubebuilder:validation:Enum=PLAIN;SCRAM-SHA-512;SCRAM-SHA-256;GSSAPI;OAUTHBEARER
	// +kubebuilder:default=PLAIN
	// +optional
	mechanism?: string @go(Mechanism)

	// The kafka user name to use for authentication
	// +optional
	userSecretRef?: null | v1.#SecretKeySelector @go(UserSecretRef,*v1.SecretKeySelector)

	// The kafka password to use for authentication
	// +optional
	passwordSecretRef?: null | v1.#SecretKeySelector @go(PasswordSecretRef,*v1.SecretKeySelector)
}

// CephBucketNotification represents a Bucket Notifications
// +kubebuilder:subresource:status
// +kubebuilder:resource:shortName=cephbn
#CephBucketNotification: {
	metav1.#TypeMeta
	metadata: metav1.#ObjectMeta      @go(ObjectMeta)
	spec:     #BucketNotificationSpec @go(Spec)

	// +kubebuilder:pruning:PreserveUnknownFields
	// +optional
	status?: null | #Status @go(Status,*Status)
}

// CephBucketNotificationList represents a list Ceph Object Store Bucket Notification Topics
// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object
#CephBucketNotificationList: {
	metav1.#TypeMeta
	metadata: metav1.#ListMeta @go(ListMeta)
	items: [...#CephBucketNotification] @go(Items,[]CephBucketNotification)
}

// BucketNotificationSpec represent the event type of the bucket notification
// +kubebuilder:validation:Enum="s3:ObjectCreated:*";"s3:ObjectCreated:Put";"s3:ObjectCreated:Post";"s3:ObjectCreated:Copy";"s3:ObjectCreated:CompleteMultipartUpload";"s3:ObjectRemoved:*";"s3:ObjectRemoved:Delete";"s3:ObjectRemoved:DeleteMarkerCreated"
#BucketNotificationEvent: string

// BucketNotificationSpec represent the spec of a Bucket Notification
#BucketNotificationSpec: {
	// The name of the topic associated with this notification
	// +kubebuilder:validation:MinLength=1
	topic: string @go(Topic)

	// List of events that should trigger the notification
	// +optional
	events?: [...#BucketNotificationEvent] @go(Events,[]BucketNotificationEvent)

	// Spec of notification filter
	// +optional
	filter?: null | #NotificationFilterSpec @go(Filter,*NotificationFilterSpec)
}

// NotificationFilterRule represent a single rule in the Notification Filter spec
#NotificationFilterRule: {
	// Name of the metadata or tag
	// +kubebuilder:validation:MinLength=1
	name: string @go(Name)

	// Value to filter on
	value: string @go(Value)
}

// NotificationKeyFilterRule represent a single key rule in the Notification Filter spec
#NotificationKeyFilterRule: {
	// Name of the filter - prefix/suffix/regex
	// +kubebuilder:validation:Enum=prefix;suffix;regex
	name: string @go(Name)

	// Value to filter on
	value: string @go(Value)
}

// NotificationFilterSpec represent the spec of a Bucket Notification filter
#NotificationFilterSpec: {
	// Filters based on the object's key
	// +optional
	keyFilters?: [...#NotificationKeyFilterRule] @go(KeyFilters,[]NotificationKeyFilterRule)

	// Filters based on the object's metadata
	// +optional
	metadataFilters?: [...#NotificationFilterRule] @go(MetadataFilters,[]NotificationFilterRule)

	// Filters based on the object's tags
	// +optional
	tagFilters?: [...#NotificationFilterRule] @go(TagFilters,[]NotificationFilterRule)
}

// RGWServiceSpec represent the spec for RGW service
#RGWServiceSpec: {
	// The annotations-related configuration to add/set on each rgw service.
	// nullable
	// optional
	annotations?: #Annotations @go(Annotations)
}

// CephNFS represents a Ceph NFS
// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object
// +kubebuilder:subresource:status
#CephNFS: {
	metav1.#TypeMeta
	metadata: metav1.#ObjectMeta @go(ObjectMeta)
	spec:     #NFSGaneshaSpec    @go(Spec)

	// +kubebuilder:pruning:PreserveUnknownFields
	// +optional
	status?: null | #Status @go(Status,*Status)
}

// CephNFSList represents a list Ceph NFSes
// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object
#CephNFSList: {
	metav1.#TypeMeta
	metadata: metav1.#ListMeta @go(ListMeta)
	items: [...#CephNFS] @go(Items,[]CephNFS)
}

// NFSGaneshaSpec represents the spec of an nfs ganesha server
#NFSGaneshaSpec: {
	// RADOS is the Ganesha RADOS specification
	// +nullable
	// +optional
	rados?: #GaneshaRADOSSpec @go(RADOS)

	// Server is the Ganesha Server specification
	server: #GaneshaServerSpec @go(Server)

	// Security allows specifying security configurations for the NFS cluster
	// +nullable
	// +optional
	security?: null | #NFSSecuritySpec @go(Security,*NFSSecuritySpec)
}

// GaneshaRADOSSpec represents the specification of a Ganesha RADOS object
#GaneshaRADOSSpec: {
	// The Ceph pool used store the shared configuration for NFS-Ganesha daemons.
	// This setting is deprecated, as it is internally required to be ".nfs".
	// +optional
	pool?: string @go(Pool)

	// The namespace inside the Ceph pool (set by 'pool') where shared NFS-Ganesha config is stored.
	// This setting is deprecated as it is internally set to the name of the CephNFS.
	// +optional
	namespace?: string @go(Namespace)
}

// GaneshaServerSpec represents the specification of a Ganesha Server
#GaneshaServerSpec: {
	// The number of active Ganesha servers
	active: int @go(Active)

	// The affinity to place the ganesha pods
	// +kubebuilder:pruning:PreserveUnknownFields
	// +nullable
	// +optional
	placement?: #Placement @go(Placement)

	// The annotations-related configuration to add/set on each Pod related object.
	// +kubebuilder:pruning:PreserveUnknownFields
	// +nullable
	// +optional
	annotations?: #Annotations @go(Annotations)

	// The labels-related configuration to add/set on each Pod related object.
	// +kubebuilder:pruning:PreserveUnknownFields
	// +nullable
	// +optional
	labels?: #Labels @go(Labels)

	// Resources set resource requests and limits
	// +kubebuilder:pruning:PreserveUnknownFields
	// +nullable
	// +optional
	resources?: v1.#ResourceRequirements @go(Resources)

	// PriorityClassName sets the priority class on the pods
	// +optional
	priorityClassName?: string @go(PriorityClassName)

	// LogLevel set logging level
	// +optional
	logLevel?: string @go(LogLevel)

	// Whether host networking is enabled for the Ganesha server. If not set, the network settings from the cluster CR will be applied.
	// +nullable
	// +optional
	hostNetwork?: null | bool @go(HostNetwork,*bool)

	// A liveness-probe to verify that Ganesha server has valid run-time state.
	// If LivenessProbe.Disabled is false and LivenessProbe.Probe is nil uses default probe.
	// +optional
	livenessProbe?: null | #ProbeSpec @go(LivenessProbe,*ProbeSpec)
}

// NFSSecuritySpec represents security configurations for an NFS server pod
#NFSSecuritySpec: {
	// SSSD enables integration with System Security Services Daemon (SSSD). SSSD can be used to
	// provide user ID mapping from a number of sources. See https://sssd.io for more information
	// about the SSSD project.
	// +optional
	// +nullable
	sssd?: null | #SSSDSpec @go(SSSD,*SSSDSpec)

	// Kerberos configures NFS-Ganesha to secure NFS client connections with Kerberos.
	// +optional
	// +nullable
	kerberos?: null | #KerberosSpec @go(Kerberos,*KerberosSpec)
}

// KerberosSpec represents configuration for Kerberos.
#KerberosSpec: {
	// PrincipalName corresponds directly to NFS-Ganesha's NFS_KRB5:PrincipalName config. In
	// practice, this is the service prefix of the principal name. The default is "nfs".
	// This value is combined with (a) the namespace and name of the CephNFS (with a hyphen between)
	// and (b) the Realm configured in the user-provided krb5.conf to determine the full principal
	// name: <principalName>/<namespace>-<name>@<realm>. e.g., nfs/rook-ceph-my-nfs@example.net.
	// See https://github.com/nfs-ganesha/nfs-ganesha/wiki/RPCSEC_GSS for more detail.
	// +optional
	// +kubebuilder:default="nfs"
	principalName?: string @go(PrincipalName)

	// DomainName should be set to the Kerberos Realm.
	// +optional
	domainName?: string @go(DomainName)

	// ConfigFiles defines where the Kerberos configuration should be sourced from. Config files
	// will be placed into the `/etc/krb5.conf.rook/` directory.
	//
	// If this is left empty, Rook will not add any files. This allows you to manage the files
	// yourself however you wish. For example, you may build them into your custom Ceph container
	// image or use the Vault agent injector to securely add the files via annotations on the
	// CephNFS spec (passed to the NFS server pods).
	//
	// Rook configures Kerberos to log to stderr. We suggest removing logging sections from config
	// files to avoid consuming unnecessary disk space from logging to files.
	// +optional
	configFiles?: #KerberosConfigFiles @go(ConfigFiles)

	// KeytabFile defines where the Kerberos keytab should be sourced from. The keytab file will be
	// placed into `/etc/krb5.keytab`. If this is left empty, Rook will not add the file.
	// This allows you to manage the `krb5.keytab` file yourself however you wish. For example, you
	// may build it into your custom Ceph container image or use the Vault agent injector to
	// securely add the file via annotations on the CephNFS spec (passed to the NFS server pods).
	// +optional
	keytabFile?: #KerberosKeytabFile @go(KeytabFile)
}

// KerberosConfigFiles represents the source(s) from which Kerberos configuration should come.
#KerberosConfigFiles: {
	// VolumeSource accepts a pared down version of the standard Kubernetes VolumeSource for
	// Kerberos configuration files like what is normally used to configure Volumes for a Pod. For
	// example, a ConfigMap, Secret, or HostPath. The volume may contain multiple files, all of
	// which will be loaded.
	volumeSource?: null | #ConfigFileVolumeSource @go(VolumeSource,*ConfigFileVolumeSource)
}

// KerberosKeytabFile represents the source(s) from which the Kerberos keytab file should come.
#KerberosKeytabFile: {
	// VolumeSource accepts a pared down version of the standard Kubernetes VolumeSource for the
	// Kerberos keytab file like what is normally used to configure Volumes for a Pod. For example,
	// a Secret or HostPath.
	// There are two requirements for the source's content:
	//   1. The config file must be mountable via `subPath: krb5.keytab`. For example, in a
	//      Secret, the data item must be named `krb5.keytab`, or `items` must be defined to
	//      select the key and give it path `krb5.keytab`. A HostPath directory must have the
	//      `krb5.keytab` file.
	//   2. The volume or config file must have mode 0600.
	volumeSource?: null | #ConfigFileVolumeSource @go(VolumeSource,*ConfigFileVolumeSource)
}

// SSSDSpec represents configuration for System Security Services Daemon (SSSD).
#SSSDSpec: {
	// Sidecar tells Rook to run SSSD in a sidecar alongside the NFS-Ganesha server in each NFS pod.
	// +optional
	sidecar?: null | #SSSDSidecar @go(Sidecar,*SSSDSidecar)
}

// SSSDSidecar represents configuration when SSSD is run in a sidecar.
#SSSDSidecar: {
	// Image defines the container image that should be used for the SSSD sidecar.
	// +kubebuilder:validation:Required
	// +kubebuilder:validation:MinLength=1
	image: string @go(Image)

	// SSSDConfigFile defines where the SSSD configuration should be sourced from. The config file
	// will be placed into `/etc/sssd/sssd.conf`. If this is left empty, Rook will not add the file.
	// This allows you to manage the `sssd.conf` file yourself however you wish. For example, you
	// may build it into your custom Ceph container image or use the Vault agent injector to
	// securely add the file via annotations on the CephNFS spec (passed to the NFS server pods).
	// +optional
	sssdConfigFile?: #SSSDSidecarConfigFile @go(SSSDConfigFile)

	// AdditionalFiles defines any number of additional files that should be mounted into the SSSD
	// sidecar with a directory root of `/etc/sssd/rook-additional/`.
	// These files may be referenced by the sssd.conf config file.
	// +optional
	additionalFiles?: #AdditionalVolumeMounts @go(AdditionalFiles)

	// Resources allow specifying resource requests/limits on the SSSD sidecar container.
	// +optional
	resources?: v1.#ResourceRequirements @go(Resources)

	// DebugLevel sets the debug level for SSSD. If unset or set to 0, Rook does nothing. Otherwise,
	// this may be a value between 1 and 10. See SSSD docs for more info:
	// https://sssd.io/troubleshooting/basics.html#sssd-debug-logs
	// +optional
	// +kubebuilder:validation:Minimum=0
	// +kubebuilder:validation:Maximum=10
	debugLevel?: int @go(DebugLevel)
}

// SSSDSidecarConfigFile represents the source(s) from which the SSSD configuration should come.
#SSSDSidecarConfigFile: {
	// VolumeSource accepts a pared down version of the standard Kubernetes VolumeSource for the
	// SSSD configuration file like what is normally used to configure Volumes for a Pod. For
	// example, a ConfigMap, Secret, or HostPath. There are two requirements for the source's
	// content:
	//   1. The config file must be mountable via `subPath: sssd.conf`. For example, in a ConfigMap,
	//      the data item must be named `sssd.conf`, or `items` must be defined to select the key
	//      and give it path `sssd.conf`. A HostPath directory must have the `sssd.conf` file.
	//   2. The volume or config file must have mode 0600.
	volumeSource?: null | #ConfigFileVolumeSource @go(VolumeSource,*ConfigFileVolumeSource)
}

// AdditionalVolumeMount represents the source from where additional files in pod containers
// should come from and what subdirectory they are made available in.
#AdditionalVolumeMount: {
	// SubPath defines the sub-path (subdirectory) of the directory root where the volumeSource will
	// be mounted. All files/keys in the volume source's volume will be mounted to the subdirectory.
	// This is not the same as the Kubernetes `subPath` volume mount option.
	// Each subPath definition must be unique and must not contain ':'.
	// +kubebuilder:validation:Required
	// +kubebuilder:validation:MinLength=1
	// +kubebuilder:validation:Pattern=`^[^:]+$`
	subPath: string @go(SubPath)

	// VolumeSource accepts a pared down version of the standard Kubernetes VolumeSource for the
	// additional file(s) like what is normally used to configure Volumes for a Pod. Fore example, a
	// ConfigMap, Secret, or HostPath. Each VolumeSource adds one or more additional files to the
	// container `<directory-root>/<subPath>` directory.
	// Be aware that some files may need to have a specific file mode like 0600 due to application
	// requirements. For example, CA or TLS certificates.
	volumeSource?: null | #ConfigFileVolumeSource @go(VolumeSource,*ConfigFileVolumeSource)
}

#AdditionalVolumeMounts: [...#AdditionalVolumeMount]

// NetworkSpec for Ceph includes backward compatibility code
// +kubebuilder:validation:XValidation:message="at least one network selector must be specified when using multus",rule="!has(self.provider) || (self.provider != 'multus' || (self.provider == 'multus' && size(self.selectors) > 0))"
// +kubebuilder:validation:XValidation:message=`the legacy hostNetwork setting can only be set if the network.provider is set to the empty string`,rule=`!has(self.hostNetwork) || self.hostNetwork == false || !has(self.provider) || self.provider == ""`
#NetworkSpec: {
	// Provider is what provides network connectivity to the cluster e.g. "host" or "multus".
	// If the Provider is updated from being empty to "host" on a running cluster, then the operator will automatically fail over all the mons to apply the "host" network settings.
	// +kubebuilder:validation:XValidation:message="network provider must be disabled (reverted to empty string) before a new provider is enabled",rule="self == '' || oldSelf == '' || self == oldSelf"
	// +nullable
	// +optional
	provider?: #NetworkProviderType @go(Provider)

	// Selectors define NetworkAttachmentDefinitions to be used for Ceph public and/or cluster
	// networks when the "multus" network provider is used. This config section is not used for
	// other network providers.
	//
	// Valid keys are "public" and "cluster". Refer to Ceph networking documentation for more:
	// https://docs.ceph.com/en/latest/rados/configuration/network-config-ref/
	//
	// Refer to Multus network annotation documentation for help selecting values:
	// https://github.com/k8snetworkplumbingwg/multus-cni/blob/master/docs/how-to-use.md#run-pod-with-network-annotation
	//
	// Rook will make a best-effort attempt to automatically detect CIDR address ranges for given
	// network attachment definitions. Rook's methods are robust but may be imprecise for
	// sufficiently complicated networks. Rook's auto-detection process obtains a new IP address
	// lease for each CephCluster reconcile. If Rook fails to detect, incorrectly detects, only
	// partially detects, or if underlying networks do not support reusing old IP addresses, it is
	// best to use the 'addressRanges' config section to specify CIDR ranges for the Ceph cluster.
	//
	// As a contrived example, one can use a theoretical Kubernetes-wide network for Ceph client
	// traffic and a theoretical Rook-only network for Ceph replication traffic as shown:
	//   selectors:
	//     public: "default/cluster-fast-net"
	//     cluster: "rook-ceph/ceph-backend-net"
	//
	// +nullable
	// +optional
	selectors?: {[string]: string} @go(Selectors,map[CephNetworkType]string)

	// AddressRanges specify a list of CIDRs that Rook will apply to Ceph's 'public_network' and/or
	// 'cluster_network' configurations. This config section may be used for the "host" or "multus"
	// network providers.
	// +nullable
	// +optional
	addressRanges?: null | #AddressRangesSpec @go(AddressRanges,*AddressRangesSpec)

	// Settings for network connections such as compression and encryption across the
	// wire.
	// +nullable
	// +optional
	connections?: null | #ConnectionsSpec @go(Connections,*ConnectionsSpec)

	// HostNetwork to enable host network.
	// If host networking is enabled or disabled on a running cluster, then the operator will automatically fail over all the mons to
	// apply the new network settings.
	// +optional
	hostNetwork?: bool @go(HostNetwork)

	// IPFamily is the single stack IPv6 or IPv4 protocol
	// +kubebuilder:validation:Enum=IPv4;IPv6
	// +nullable
	// +optional
	ipFamily?: #IPFamilyType @go(IPFamily)

	// DualStack determines whether Ceph daemons should listen on both IPv4 and IPv6
	// +optional
	dualStack?: bool @go(DualStack)

	// Enable multiClusterService to export the Services between peer clusters
	// +optional
	multiClusterService?: #MultiClusterServiceSpec @go(MultiClusterService)
}

// NetworkProviderType defines valid network providers for Rook.
// +kubebuilder:validation:Enum="";host;multus
#NetworkProviderType: string // #enumNetworkProviderType

#enumNetworkProviderType:
	#NetworkProviderDefault |
	#NetworkProviderHost |
	#NetworkProviderMultus

#NetworkProviderDefault: #NetworkProviderType & ""
#NetworkProviderHost:    #NetworkProviderType & "host"
#NetworkProviderMultus:  #NetworkProviderType & "multus"

// CephNetworkType should be "public" or "cluster".
// Allow any string so that over-specified legacy clusters do not break on CRD update.
#CephNetworkType: string // #enumCephNetworkType

#enumCephNetworkType:
	#CephNetworkPublic |
	#CephNetworkCluster

#CephNetworkPublic:  #CephNetworkType & "public"
#CephNetworkCluster: #CephNetworkType & "cluster"

#AddressRangesSpec: {
	// Public defines a list of CIDRs to use for Ceph public network communication.
	// +optional
	public?: #CIDRList @go(Public)

	// Cluster defines a list of CIDRs to use for Ceph cluster network communication.
	// +optional
	cluster?: #CIDRList @go(Cluster)
}

// An IPv4 or IPv6 network CIDR.
//
// This naive kubebuilder regex provides immediate feedback for some typos and for a common problem
// case where the range spec is forgotten (e.g., /24). Rook does in-depth validation in code.
// +kubebuilder:validation:Pattern=`^[0-9a-fA-F:.]{2,}\/[0-9]{1,3}$`
#CIDR: string

// A list of CIDRs.
#CIDRList: [...#CIDR]

#MultiClusterServiceSpec: {
	// Enable multiClusterService to export the mon and OSD services to peer cluster.
	// Ensure that peer clusters are connected using an MCS API compatible application,
	// like Globalnet Submariner.
	// +optional
	enabled?: bool @go(Enabled)

	// ClusterID uniquely identifies a cluster. It is used as a prefix to nslookup exported
	// services. For example: <clusterid>.<svc>.<ns>.svc.clusterset.local
	clusterID?: string @go(ClusterID)
}

#ConnectionsSpec: {
	// Encryption settings for the network connections.
	// +nullable
	// +optional
	encryption?: null | #EncryptionSpec @go(Encryption,*EncryptionSpec)

	// Compression settings for the network connections.
	// +nullable
	// +optional
	compression?: null | #CompressionSpec @go(Compression,*CompressionSpec)

	// Whether to require msgr2 (port 3300) even if compression or encryption are not enabled.
	// If true, the msgr1 port (6789) will be disabled.
	// Requires a kernel that supports msgr2 (kernel 5.11 or CentOS 8.4 or newer).
	// +optional
	requireMsgr2?: bool @go(RequireMsgr2)
}

#EncryptionSpec: {
	// Whether to encrypt the data in transit across the wire to prevent eavesdropping
	// the data on the network. The default is not set. Even if encryption is not enabled,
	// clients still establish a strong initial authentication for the connection
	// and data integrity is still validated with a crc check. When encryption is enabled,
	// all communication between clients and Ceph daemons, or between Ceph daemons will
	// be encrypted.
	// +optional
	enabled?: bool @go(Enabled)
}

#CompressionSpec: {
	// Whether to compress the data in transit across the wire.
	// The default is not set.
	// +optional
	enabled?: bool @go(Enabled)
}

// DisruptionManagementSpec configures management of daemon disruptions
#DisruptionManagementSpec: {
	// This enables management of poddisruptionbudgets
	// +optional
	managePodBudgets?: bool @go(ManagePodBudgets)

	// OSDMaintenanceTimeout sets how many additional minutes the DOWN/OUT interval is for drained failure domains
	// it only works if managePodBudgets is true.
	// the default is 30 minutes
	// +optional
	osdMaintenanceTimeout?: int @go(OSDMaintenanceTimeout,time.Duration)

	// DEPRECATED: PGHealthCheckTimeout is no longer implemented
	// +optional
	pgHealthCheckTimeout?: int @go(PGHealthCheckTimeout,time.Duration)

	// PgHealthyRegex is the regular expression that is used to determine which PG states should be considered healthy.
	// The default is `^(active\+clean|active\+clean\+scrubbing|active\+clean\+scrubbing\+deep)$`
	// +optional
	pgHealthyRegex?: string @go(PGHealthyRegex)

	// Deprecated. This enables management of machinedisruptionbudgets.
	// +optional
	manageMachineDisruptionBudgets?: bool @go(ManageMachineDisruptionBudgets)

	// Deprecated. Namespace to look for MDBs by the machineDisruptionBudgetController
	// +optional
	machineDisruptionBudgetNamespace?: string @go(MachineDisruptionBudgetNamespace)
}

// CephClient represents a Ceph Client
// +kubebuilder:printcolumn:name="Phase",type=string,JSONPath=`.status.phase`
// +kubebuilder:printcolumn:name="Age",type=date,JSONPath=`.metadata.creationTimestamp`
// +kubebuilder:subresource:status
// +kubebuilder:resource:shortName=cephcl
#CephClient: {
	metav1.#TypeMeta
	metadata: metav1.#ObjectMeta @go(ObjectMeta)

	// Spec represents the specification of a Ceph Client
	spec: #ClientSpec @go(Spec)

	// Status represents the status of a Ceph Client
	// +kubebuilder:pruning:PreserveUnknownFields
	// +optional
	status?: null | #CephClientStatus @go(Status,*CephClientStatus)
}

// CephClientList represents a list of Ceph Clients
#CephClientList: {
	metav1.#TypeMeta
	metadata: metav1.#ListMeta @go(ListMeta)
	items: [...#CephClient] @go(Items,[]CephClient)
}

// ClientSpec represents the specification of a Ceph Client
#ClientSpec: {
	// +optional
	name?: string @go(Name)

	// SecretName is the name of the secret created for this ceph client.
	// If not specified, the default name is "rook-ceph-client-" as a prefix to the CR name.
	// +kubebuilder:validation:XValidation:message="SecretName is immutable and cannot be changed",rule="self == oldSelf"
	// +optional
	secretName?: string @go(SecretName)

	// RemoveSecret indicates whether the current secret for this ceph client should be removed or not.
	// If true, the K8s secret will be deleted, but the cephx keyring will remain until the CR is deleted.
	// +optional
	removeSecret?: bool @go(RemoveSecret)

	// +kubebuilder:pruning:PreserveUnknownFields
	caps: {[string]: string} @go(Caps,map[string]string)

	// Security represents security settings
	// +optional
	security?: #ClientSecuritySpec @go(Security)
}

// ClinetSecuritySpec represents security settings for a Ceph Client
#ClientSecuritySpec: {
	// CephX configures CephX key settings. More: https://docs.ceph.com/en/latest/dev/cephx/
	// +optional
	cephx?: #CephxConfig @go(CephX)
}

// CephClientStatus represents the Status of Ceph Client
#CephClientStatus: {
	// +optional
	phase?: #ConditionType @go(Phase)

	// +optional
	// +nullable
	info?: {[string]: string} @go(Info,map[string]string)

	// ObservedGeneration is the latest generation observed by the controller.
	// +optional
	observedGeneration?: int64 @go(ObservedGeneration)

	// +optional
	cephx?: #CephxStatus @go(Cephx)
}

// CleanupPolicySpec represents a Ceph Cluster cleanup policy
#CleanupPolicySpec: {
	// Confirmation represents the cleanup confirmation
	// +optional
	// +nullable
	confirmation?: #CleanupConfirmationProperty @go(Confirmation)

	// SanitizeDisks represents way we sanitize disks
	// +optional
	// +nullable
	sanitizeDisks?: #SanitizeDisksSpec @go(SanitizeDisks)

	// AllowUninstallWithVolumes defines whether we can proceed with the uninstall if they are RBD images still present
	// +optional
	allowUninstallWithVolumes?: bool @go(AllowUninstallWithVolumes)

	// WipeDevicesFromOtherClusters wipes the OSD disks belonging to other clusters. This is useful in scenarios where ceph cluster
	// was reinstalled but OSD disk still contains the metadata from previous ceph cluster.
	// +optional
	wipeDevicesFromOtherClusters?: bool @go(WipeDevicesFromOtherClusters)
}

// CleanupConfirmationProperty represents the cleanup confirmation
// +kubebuilder:validation:Pattern=`^$|^yes-really-destroy-data$`
#CleanupConfirmationProperty: string // #enumCleanupConfirmationProperty

#enumCleanupConfirmationProperty:
	#DeleteDataDirOnHostsConfirmation

// SanitizeDataSourceProperty represents a sanitizing data source
#SanitizeDataSourceProperty: string // #enumSanitizeDataSourceProperty

#enumSanitizeDataSourceProperty:
	#SanitizeDataSourceZero |
	#SanitizeDataSourceRandom

// SanitizeMethodProperty represents a disk sanitizing method
#SanitizeMethodProperty: string // #enumSanitizeMethodProperty

#enumSanitizeMethodProperty:
	#SanitizeMethodComplete |
	#SanitizeMethodQuick

// SanitizeDisksSpec represents a disk sanitizing specification
#SanitizeDisksSpec: {
	// Method is the method we use to sanitize disks
	// +optional
	// +kubebuilder:validation:Enum=complete;quick
	method?: #SanitizeMethodProperty @go(Method)

	// DataSource is the data source to use to sanitize the disk with
	// +optional
	// +kubebuilder:validation:Enum=zero;random
	dataSource?: #SanitizeDataSourceProperty @go(DataSource)

	// Iteration is the number of pass to apply the sanitizing
	// +optional
	iteration?: int32 @go(Iteration)
}

// CephRBDMirror represents a Ceph RBD Mirror
// +kubebuilder:printcolumn:name="Phase",type=string,JSONPath=`.status.phase`
// +kubebuilder:printcolumn:name="Age",type=date,JSONPath=`.metadata.creationTimestamp`
// +kubebuilder:subresource:status
// +kubebuilder:resource:shortName=cephrbdm
#CephRBDMirror: {
	metav1.#TypeMeta
	metadata: metav1.#ObjectMeta @go(ObjectMeta)
	spec:     #RBDMirroringSpec  @go(Spec)

	// +kubebuilder:pruning:PreserveUnknownFields
	// +optional
	status?: null | #RBDMirrorStatus @go(Status,*RBDMirrorStatus)
}

// RBDMirrorStatus represents the status of the RBD mirror resource
#RBDMirrorStatus: {
	#Status
	cephx?: #LocalCephxStatus @go(Cephx)
}

// CephRBDMirrorList represents a list Ceph RBD Mirrors
// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object
#CephRBDMirrorList: {
	metav1.#TypeMeta
	metadata: metav1.#ListMeta @go(ListMeta)
	items: [...#CephRBDMirror] @go(Items,[]CephRBDMirror)
}

// RBDMirroringSpec represents the specification of an RBD mirror daemon
#RBDMirroringSpec: {
	// Count represents the number of rbd mirror instance to run
	// +kubebuilder:validation:Minimum=1
	count: int @go(Count)

	// Peers represents the peers spec
	// +nullable
	// +optional
	peers?: #MirroringPeerSpec @go(Peers)

	// The affinity to place the rgw pods (default is to place on any available node)
	// +kubebuilder:pruning:PreserveUnknownFields
	// +nullable
	// +optional
	placement?: #Placement @go(Placement)

	// The annotations-related configuration to add/set on each Pod related object.
	// +kubebuilder:pruning:PreserveUnknownFields
	// +nullable
	// +optional
	annotations?: #Annotations @go(Annotations)

	// The labels-related configuration to add/set on each Pod related object.
	// +kubebuilder:pruning:PreserveUnknownFields
	// +nullable
	// +optional
	labels?: #Labels @go(Labels)

	// The resource requirements for the rbd mirror pods
	// +kubebuilder:pruning:PreserveUnknownFields
	// +nullable
	// +optional
	resources?: v1.#ResourceRequirements @go(Resources)

	// PriorityClassName sets priority class on the rbd mirror pods
	// +optional
	priorityClassName?: string @go(PriorityClassName)
}

// MirroringPeerSpec represents the specification of a mirror peer
#MirroringPeerSpec: {
	// SecretNames represents the Kubernetes Secret names to add rbd-mirror or cephfs-mirror peers
	// +optional
	secretNames?: [...string] @go(SecretNames,[]string)
}

// CephFilesystemMirror is the Ceph Filesystem Mirror object definition
// +kubebuilder:printcolumn:name="Phase",type=string,JSONPath=`.status.phase`
// +kubebuilder:printcolumn:name="Age",type=date,JSONPath=`.metadata.creationTimestamp`
// +kubebuilder:subresource:status
// +kubebuilder:resource:shortName=cephfsm
#CephFilesystemMirror: {
	metav1.#TypeMeta
	metadata: metav1.#ObjectMeta       @go(ObjectMeta)
	spec:     #FilesystemMirroringSpec @go(Spec)

	// +optional
	status?: null | #FileMirrorStatus @go(Status,*FileMirrorStatus)
}

// FileMirrorStatus represents the status of the FileSystem mirror resource
#FileMirrorStatus: {
	#Status
	cephx?: #LocalCephxStatus @go(Cephx)
}

// CephFilesystemMirrorList is a list of CephFilesystemMirror
#CephFilesystemMirrorList: {
	metav1.#TypeMeta
	metadata: metav1.#ListMeta @go(ListMeta)
	items: [...#CephFilesystemMirror] @go(Items,[]CephFilesystemMirror)
}

// FilesystemMirroringSpec is the filesystem mirroring specification
#FilesystemMirroringSpec: {
	// The affinity to place the rgw pods (default is to place on any available node)
	// +nullable
	// +optional
	placement?: #Placement @go(Placement)

	// The annotations-related configuration to add/set on each Pod related object.
	// +nullable
	// +optional
	annotations?: #Annotations @go(Annotations)

	// The labels-related configuration to add/set on each Pod related object.
	// +nullable
	// +optional
	labels?: #Labels @go(Labels)

	// The resource requirements for the cephfs-mirror pods
	// +nullable
	// +optional
	resources?: v1.#ResourceRequirements @go(Resources)

	// PriorityClassName sets priority class on the cephfs-mirror pods
	// +optional
	priorityClassName?: string @go(PriorityClassName)
}

// IPFamilyType represents the single stack Ipv4 or Ipv6 protocol.
#IPFamilyType: string // #enumIPFamilyType

#enumIPFamilyType:
	#IPv6 |
	#IPv4

// IPv6 internet protocol version
#IPv6: #IPFamilyType & "IPv6"

// IPv4 internet protocol version
#IPv4: #IPFamilyType & "IPv4"

#StorageScopeSpec: {
	// +nullable
	// +optional
	nodes?: [...#Node] @go(Nodes,[]Node)

	// +optional
	useAllNodes?: bool @go(UseAllNodes)

	// +optional
	// Whether to always schedule OSDs on a node even if the node is not currently scheduleable or ready
	scheduleAlways?: bool @go(ScheduleAlways)

	// +optional
	onlyApplyOSDPlacement?: bool @go(OnlyApplyOSDPlacement)

	// +kubebuilder:pruning:PreserveUnknownFields
	// +nullable
	// +optional
	config?: {[string]: string} @go(Config,map[string]string)

	#Selection

	// +nullable
	// +optional
	storageClassDeviceSets?: [...#StorageClassDeviceSet] @go(StorageClassDeviceSets,[]StorageClassDeviceSet)

	// Migration handles the OSD migration
	// +optional
	migration?: #Migration @go(Migration)

	// +optional
	store?: #OSDStore @go(Store)

	// +optional
	// FlappingRestartIntervalHours defines the time for which the OSD pods, that failed with zero exit code, will sleep before restarting.
	// This is needed for OSD flapping where OSD daemons are marked down more than 5 times in 600 seconds by Ceph.
	// Preventing the OSD pods to restart immediately in such scenarios will prevent Rook from marking OSD as `up` and thus
	// peering of the PGs mapped to the OSD.
	// User needs to manually restart the OSD pod if they manage to fix the underlying OSD flapping issue before the restart interval.
	// The sleep will be disabled if this interval is set to 0.
	flappingRestartIntervalHours?: int @go(FlappingRestartIntervalHours)

	// FullRatio is the ratio at which the cluster is considered full and ceph will stop accepting writes. Default is 0.95.
	// +kubebuilder:validation:Minimum=0.0
	// +kubebuilder:validation:Maximum=1.0
	// +optional
	// +nullable
	fullRatio?: null | float64 @go(FullRatio,*float64)

	// NearFullRatio is the ratio at which the cluster is considered nearly full and will raise a ceph health warning. Default is 0.85.
	// +kubebuilder:validation:Minimum=0.0
	// +kubebuilder:validation:Maximum=1.0
	// +optional
	// +nullable
	nearFullRatio?: null | float64 @go(NearFullRatio,*float64)

	// BackfillFullRatio is the ratio at which the cluster is too full for backfill. Backfill will be disabled if above this threshold. Default is 0.90.
	// +kubebuilder:validation:Minimum=0.0
	// +kubebuilder:validation:Maximum=1.0
	// +optional
	// +nullable
	backfillFullRatio?: null | float64 @go(BackfillFullRatio,*float64)

	// Whether to allow updating the device class after the OSD is initially provisioned
	// +optional
	allowDeviceClassUpdate?: bool @go(AllowDeviceClassUpdate)

	// Whether Rook will resize the OSD CRUSH weight when the OSD PVC size is increased.
	// This allows cluster data to be rebalanced to make most effective use of new OSD space.
	// The default is false since data rebalancing can cause temporary cluster slowdown.
	// +optional
	allowOsdCrushWeightUpdate?: bool @go(AllowOsdCrushWeightUpdate)
}

// Migration handles the OSD migration
#Migration: {
	// A user confirmation to migrate the OSDs. It destroys each OSD one at a time, cleans up the backing disk
	// and prepares OSD with same ID on that disk
	// +optional
	// +kubebuilder:validation:Pattern=`^$|^yes-really-migrate-osds$`
	confirmation?: string @go(Confirmation)
}

// OSDStore is the backend storage type used for creating the OSDs
#OSDStore: {
	// Type of backend storage to be used while creating OSDs. If empty, then bluestore will be used
	// +optional
	// +kubebuilder:validation:Enum=bluestore;bluestore-rdr;
	type?: string @go(Type)

	// UpdateStore updates the backend store for existing OSDs. It destroys each OSD one at a time, cleans up the backing disk
	// and prepares same OSD on that disk
	// +optional
	// +kubebuilder:validation:Pattern=`^$|^yes-really-update-store$`
	updateStore?: string @go(UpdateStore)
}

// Node is a storage nodes
// +nullable
#Node: {
	// +optional
	name?: string @go(Name)

	// +kubebuilder:pruning:PreserveUnknownFields
	// +nullable
	// +optional
	resources?: v1.#ResourceRequirements @go(Resources)

	// +kubebuilder:pruning:PreserveUnknownFields
	// +nullable
	// +optional
	config?: {[string]: string} @go(Config,map[string]string)

	#Selection
}

// Device represents a disk to use in the cluster
#Device: {
	// +optional
	name?: string @go(Name)

	// +optional
	fullpath?: string @go(FullPath)

	// +kubebuilder:pruning:PreserveUnknownFields
	// +nullable
	// +optional
	config?: {[string]: string} @go(Config,map[string]string)
}

#Selection: {
	// Whether to consume all the storage devices found on a machine
	// +optional
	useAllDevices?: null | bool @go(UseAllDevices,*bool)

	// A regular expression to allow more fine-grained selection of devices on nodes across the cluster
	// +optional
	deviceFilter?: string @go(DeviceFilter)

	// A regular expression to allow more fine-grained selection of devices with path names
	// +optional
	devicePathFilter?: string @go(DevicePathFilter)

	// List of devices to use as storage devices
	// +kubebuilder:pruning:PreserveUnknownFields
	// +nullable
	// +optional
	devices?: [...#Device] @go(Devices,[]Device)

	// PersistentVolumeClaims to use as storage
	// +optional
	volumeClaimTemplates?: [...#VolumeClaimTemplate] @go(VolumeClaimTemplates,[]VolumeClaimTemplate)
}

// PlacementSpec is the placement for core ceph daemons part of the CephCluster CRD
#PlacementSpec: {[string]: #Placement}

// Placement is the placement for an object
#Placement: {
	// NodeAffinity is a group of node affinity scheduling rules
	// +optional
	nodeAffinity?: null | v1.#NodeAffinity @go(NodeAffinity,*v1.NodeAffinity)

	// PodAffinity is a group of inter pod affinity scheduling rules
	// +optional
	podAffinity?: null | v1.#PodAffinity @go(PodAffinity,*v1.PodAffinity)

	// PodAntiAffinity is a group of inter pod anti affinity scheduling rules
	// +optional
	podAntiAffinity?: null | v1.#PodAntiAffinity @go(PodAntiAffinity,*v1.PodAntiAffinity)

	// The pod this Toleration is attached to tolerates any taint that matches
	// the triple <key,value,effect> using the matching operator <operator>
	// +optional
	tolerations?: [...v1.#Toleration] @go(Tolerations,[]v1.Toleration)

	// TopologySpreadConstraints specifies how to spread matching pods among the given topology
	// +optional
	topologySpreadConstraints?: [...v1.#TopologySpreadConstraint] @go(TopologySpreadConstraints,[]v1.TopologySpreadConstraint)
}

// ResourceSpec is a collection of ResourceRequirements that describes the compute resource requirements
#ResourceSpec: {[string]: v1.#ResourceRequirements}

// ProbeSpec is a wrapper around Probe so it can be enabled or disabled for a Ceph daemon
#ProbeSpec: {
	// Disabled determines whether probe is disable or not
	// +optional
	disabled?: bool @go(Disabled)

	// Probe describes a health check to be performed against a container to determine whether it is
	// alive or ready to receive traffic.
	// +optional
	probe?: null | v1.#Probe @go(Probe,*v1.Probe)
}

// PriorityClassNamesSpec is a map of priority class names to be assigned to components
#PriorityClassNamesSpec: {[string]: string}

// StorageClassDeviceSet is a storage class device set
// +nullable
#StorageClassDeviceSet: {
	// Name is a unique identifier for the set
	name: string @go(Name)

	// Count is the number of devices in this set
	// +kubebuilder:validation:Minimum=1
	count: int @go(Count)

	// +kubebuilder:pruning:PreserveUnknownFields
	// +nullable
	// +optional
	resources?: v1.#ResourceRequirements @go(Resources)

	// +kubebuilder:pruning:PreserveUnknownFields
	// +nullable
	// +optional
	placement?: #Placement @go(Placement)

	// +kubebuilder:pruning:PreserveUnknownFields
	// +nullable
	// +optional
	preparePlacement?: null | #Placement @go(PreparePlacement,*Placement)

	// Provider-specific device configuration
	// +kubebuilder:pruning:PreserveUnknownFields
	// +nullable
	// +optional
	config?: {[string]: string} @go(Config,map[string]string)

	// VolumeClaimTemplates is a list of PVC templates for the underlying storage devices
	volumeClaimTemplates: [...#VolumeClaimTemplate] @go(VolumeClaimTemplates,[]VolumeClaimTemplate)

	// Portable represents OSD portability across the hosts
	// +optional
	portable?: bool @go(Portable)

	// TuneSlowDeviceClass Tune the OSD when running on a slow Device Class
	// +optional
	tuneDeviceClass?: bool @go(TuneSlowDeviceClass)

	// TuneFastDeviceClass Tune the OSD when running on a fast Device Class
	// +optional
	tuneFastDeviceClass?: bool @go(TuneFastDeviceClass)

	// Scheduler name for OSD pod placement
	// +optional
	schedulerName?: string @go(SchedulerName)

	// Whether to encrypt the deviceSet
	// +optional
	encrypted?: bool @go(Encrypted)
}

// CephFilesystemSubVolumeGroup represents a Ceph Filesystem SubVolumeGroup
// +kubebuilder:printcolumn:name="Phase",type=string,JSONPath=`.status.phase`
// +kubebuilder:printcolumn:name="Filesystem",type=string,JSONPath=`.spec.filesystemName`,description="Name of the CephFileSystem"
// +kubebuilder:printcolumn:name="Quota",type=string,JSONPath=`.spec.quota`
// +kubebuilder:printcolumn:name="Pinning",type=string,JSONPath=`.status.info.pinning`,priority=1
// +kubebuilder:printcolumn:name="Age",type=date,JSONPath=`.metadata.creationTimestamp`
// +kubebuilder:subresource:status
// +kubebuilder:resource:shortName=cephfssvg;cephsvg
#CephFilesystemSubVolumeGroup: {
	metav1.#TypeMeta
	metadata: metav1.#ObjectMeta @go(ObjectMeta)

	// Spec represents the specification of a Ceph Filesystem SubVolumeGroup
	spec: #CephFilesystemSubVolumeGroupSpec @go(Spec)

	// Status represents the status of a CephFilesystem SubvolumeGroup
	// +kubebuilder:pruning:PreserveUnknownFields
	// +optional
	status?: null | #CephFilesystemSubVolumeGroupStatus @go(Status,*CephFilesystemSubVolumeGroupStatus)
}

// CephFilesystemSubVolumeGroup represents a list of Ceph Clients
#CephFilesystemSubVolumeGroupList: {
	metav1.#TypeMeta
	metadata: metav1.#ListMeta @go(ListMeta)
	items: [...#CephFilesystemSubVolumeGroup] @go(Items,[]CephFilesystemSubVolumeGroup)
}

// CephFilesystemSubVolumeGroupSpec represents the specification of a Ceph Filesystem SubVolumeGroup
#CephFilesystemSubVolumeGroupSpec: {
	// The name of the subvolume group. If not set, the default is the name of the subvolumeGroup CR.
	// +kubebuilder:validation:XValidation:message="name is immutable",rule="self == oldSelf"
	// +optional
	name?: string @go(Name)

	// FilesystemName is the name of Ceph Filesystem SubVolumeGroup volume name. Typically it's the name of
	// the CephFilesystem CR. If not coming from the CephFilesystem CR, it can be retrieved from the
	// list of Ceph Filesystem volumes with `ceph fs volume ls`. To learn more about Ceph Filesystem
	// abstractions see https://docs.ceph.com/en/latest/cephfs/fs-volumes/#fs-volumes-and-subvolumes
	// +kubebuilder:validation:XValidation:message="filesystemName is immutable",rule="self == oldSelf"
	filesystemName: string @go(FilesystemName)

	// Pinning configuration of CephFilesystemSubVolumeGroup,
	// reference https://docs.ceph.com/en/latest/cephfs/fs-volumes/#pinning-subvolumes-and-subvolume-groups
	// only one out of (export, distributed, random) can be set at a time
	// +optional
	pinning?: #CephFilesystemSubVolumeGroupSpecPinning @go(Pinning)

	// Quota size of the Ceph Filesystem subvolume group.
	// +optional
	quota?: null | resource.#Quantity @go(Quota,*resource.Quantity)

	// The data pool name for the Ceph Filesystem subvolume group layout, if the default CephFS pool is not desired.
	// +optional
	dataPoolName?: string @go(DataPoolName)

	// ClusterID to be used for this subvolume group in the CSI configuration.
	// It must be unique among all Ceph clusters managed by Rook.
	// If not specified, the clusterID will be generated and can be found in the CR status.
	// +optional
	// +kubebuilder:validation:XValidation:message="ClusterID is immutable",rule="self == oldSelf"
	// +kubebuilder:validation:MinLength=1
	// +kubebuilder:validation:MaxLength=36
	// +kubebuilder:validation:Pattern=`^[a-zA-Z0-9_-]+$`
	clusterID?: string @go(ClusterID)
}

// CephFilesystemSubVolumeGroupSpecPinning represents the pinning configuration of SubVolumeGroup
// +kubebuilder:validation:XValidation:message="only one pinning type should be set",rule="(has(self.export) && !has(self.distributed) && !has(self.random)) || (!has(self.export) && has(self.distributed) && !has(self.random)) || (!has(self.export) && !has(self.distributed) && has(self.random)) || (!has(self.export) && !has(self.distributed) && !has(self.random))"
#CephFilesystemSubVolumeGroupSpecPinning: {
	// +kubebuilder:validation:Minimum=-1
	// +kubebuilder:validation:Maximum=256
	// +optional
	// +nullable
	export?: null | int @go(Export,*int)

	// +kubebuilder:validation:Minimum=0
	// +kubebuilder:validation:Maximum=1
	// +optional
	// +nullable
	distributed?: null | int @go(Distributed,*int)

	// +kubebuilder:validation:Minimum=0.0
	// +kubebuilder:validation:Maximum=1.0
	// +optional
	// +nullable
	random?: null | float64 @go(Random,*float64)
}

// CephFilesystemSubVolumeGroupStatus represents the Status of Ceph Filesystem SubVolumeGroup
#CephFilesystemSubVolumeGroupStatus: {
	// +optional
	phase?: #ConditionType @go(Phase)

	// +optional
	// +nullable
	info?: {[string]: string} @go(Info,map[string]string)

	// ObservedGeneration is the latest generation observed by the controller.
	// +optional
	observedGeneration?: int64 @go(ObservedGeneration)
}

// +genclient
// +genclient:noStatus
// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object
// CephBlockPoolRadosNamespace represents a Ceph BlockPool Rados Namespace
// +kubebuilder:printcolumn:name="Phase",type=string,JSONPath=`.status.phase`
// +kubebuilder:printcolumn:name="BlockPool",type=string,JSONPath=`.spec.blockPoolName`,description="Name of the Ceph BlockPool"
// +kubebuilder:printcolumn:name="Age",type=date,JSONPath=`.metadata.creationTimestamp`
// +kubebuilder:subresource:status
// +kubebuilder:resource:shortName=cephbprns;cephrns
#CephBlockPoolRadosNamespace: {
	metav1.#TypeMeta
	metadata: metav1.#ObjectMeta @go(ObjectMeta)

	// Spec represents the specification of a Ceph BlockPool Rados Namespace
	spec: #CephBlockPoolRadosNamespaceSpec @go(Spec)

	// Status represents the status of a CephBlockPool Rados Namespace
	// +kubebuilder:pruning:PreserveUnknownFields
	// +optional
	status?: null | #CephBlockPoolRadosNamespaceStatus @go(Status,*CephBlockPoolRadosNamespaceStatus)
}

// CephBlockPoolRadosNamespaceList represents a list of Ceph BlockPool Rados Namespace
#CephBlockPoolRadosNamespaceList: {
	metav1.#TypeMeta
	metadata: metav1.#ListMeta @go(ListMeta)
	items: [...#CephBlockPoolRadosNamespace] @go(Items,[]CephBlockPoolRadosNamespace)
}

// RadosNamespaceMirroring represents the mirroring configuration of CephBlockPoolRadosNamespace
#RadosNamespaceMirroring: {
	// RemoteNamespace is the name of the CephBlockPoolRadosNamespace on the secondary cluster CephBlockPool
	// +optional
	remoteNamespace?: null | string @go(RemoteNamespace,*string)

	// Mode is the mirroring mode; either pool or image.
	// +kubebuilder:validation:Enum="";pool;image
	mode: #RadosNamespaceMirroringMode @go(Mode)

	// SnapshotSchedules is the scheduling of snapshot for mirrored images
	// +optional
	snapshotSchedules?: [...#SnapshotScheduleSpec] @go(SnapshotSchedules,[]SnapshotScheduleSpec)
}

// RadosNamespaceMirroringMode represents the mode of the RadosNamespace
#RadosNamespaceMirroringMode: string // #enumRadosNamespaceMirroringMode

#enumRadosNamespaceMirroringMode:
	#RadosNamespaceMirroringModePool |
	#RadosNamespaceMirroringModeImage

// RadosNamespaceMirroringModePool represents the pool mode
#RadosNamespaceMirroringModePool: #RadosNamespaceMirroringMode & "pool"

// RadosNamespaceMirroringModeImage represents the image mode
#RadosNamespaceMirroringModeImage: #RadosNamespaceMirroringMode & "image"

// CephBlockPoolRadosNamespaceSpec represents the specification of a CephBlockPool Rados Namespace
#CephBlockPoolRadosNamespaceSpec: {
	// The name of the CephBlockPoolRadosNamespaceSpec namespace. If not set, the default is the name of the CR.
	// +kubebuilder:validation:XValidation:message="name is immutable",rule="self == oldSelf"
	// +optional
	name?: string @go(Name)

	// BlockPoolName is the name of Ceph BlockPool. Typically it's the name of
	// the CephBlockPool CR.
	// +kubebuilder:validation:XValidation:message="blockPoolName is immutable",rule="self == oldSelf"
	blockPoolName: string @go(BlockPoolName)

	// Mirroring configuration of CephBlockPoolRadosNamespace
	// +optional
	mirroring?: null | #RadosNamespaceMirroring @go(Mirroring,*RadosNamespaceMirroring)

	// ClusterID to be used for this RadosNamespace in the CSI configuration.
	// It must be unique among all Ceph clusters managed by Rook.
	// If not specified, the clusterID will be generated and can be found in the CR status.
	// +optional
	// +kubebuilder:validation:XValidation:message="ClusterID is immutable",rule="self == oldSelf"
	// +kubebuilder:validation:MinLength=1
	// +kubebuilder:validation:MaxLength=36
	// +kubebuilder:validation:Pattern=`^[a-zA-Z0-9_-]+$`
	clusterID?: string @go(ClusterID)
}

// CephBlockPoolRadosNamespaceStatus represents the Status of Ceph BlockPool
// Rados Namespace
#CephBlockPoolRadosNamespaceStatus: {
	// +optional
	phase?: #ConditionType @go(Phase)

	// +optional
	// +nullable
	info?: {[string]: string} @go(Info,map[string]string)

	// +optional
	mirroringStatus?: null | #MirroringStatusSpec @go(MirroringStatus,*MirroringStatusSpec)

	// +optional
	mirroringInfo?: null | #MirroringInfoSpec @go(MirroringInfo,*MirroringInfoSpec)

	// +optional
	snapshotScheduleStatus?: null | #SnapshotScheduleStatusSpec @go(SnapshotScheduleStatus,*SnapshotScheduleStatusSpec)
	conditions?: [...#Condition] @go(Conditions,[]Condition)
}

// Represents the source of a volume to mount.
// Only one of its members may be specified.
// This is a subset of the full Kubernetes API's VolumeSource that is reduced to what is most likely
// to be useful for mounting config files/dirs into Rook pods.
#ConfigFileVolumeSource: {
	// hostPath represents a pre-existing file or directory on the host
	// machine that is directly exposed to the container. This is generally
	// used for system agents or other privileged things that are allowed
	// to see the host machine. Most containers will NOT need this.
	// More info: https://kubernetes.io/docs/concepts/storage/volumes#hostpath
	// ---
	// +optional
	hostPath?: null | v1.#HostPathVolumeSource @go(HostPath,*v1.HostPathVolumeSource) @protobuf(1,bytes,opt)

	// emptyDir represents a temporary directory that shares a pod's lifetime.
	// More info: https://kubernetes.io/docs/concepts/storage/volumes#emptydir
	// +optional
	emptyDir?: null | v1.#EmptyDirVolumeSource @go(EmptyDir,*v1.EmptyDirVolumeSource) @protobuf(2,bytes,opt)

	// secret represents a secret that should populate this volume.
	// More info: https://kubernetes.io/docs/concepts/storage/volumes#secret
	// +optional
	secret?: null | v1.#SecretVolumeSource @go(Secret,*v1.SecretVolumeSource) @protobuf(6,bytes,opt)

	// persistentVolumeClaimVolumeSource represents a reference to a
	// PersistentVolumeClaim in the same namespace.
	// More info: https://kubernetes.io/docs/concepts/storage/persistent-volumes#persistentvolumeclaims
	// +optional
	persistentVolumeClaim?: null | v1.#PersistentVolumeClaimVolumeSource @go(PersistentVolumeClaim,*v1.PersistentVolumeClaimVolumeSource) @protobuf(10,bytes,opt)

	// configMap represents a configMap that should populate this volume
	// +optional
	configMap?: null | v1.#ConfigMapVolumeSource @go(ConfigMap,*v1.ConfigMapVolumeSource) @protobuf(19,bytes,opt)

	// projected items for all in one resources secrets, configmaps, and downward API
	projected?: null | v1.#ProjectedVolumeSource @go(Projected,*v1.ProjectedVolumeSource) @protobuf(26,bytes,opt)
}

// CephCOSIDriver represents the CRD for the Ceph COSI Driver Deployment
// +kubebuilder:resource:shortName=cephcosi
#CephCOSIDriver: {
	metav1.#TypeMeta
	metadata: metav1.#ObjectMeta @go(ObjectMeta)

	// Spec represents the specification of a Ceph COSI Driver
	spec: #CephCOSIDriverSpec @go(Spec)
}

// CephCOSIDriverList represents a list of Ceph COSI Driver
// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object
#CephCOSIDriverList: {
	metav1.#TypeMeta
	metadata: metav1.#ListMeta @go(ListMeta)
	items: [...#CephCOSIDriver] @go(Items,[]CephCOSIDriver)
}

// CephCOSIDriverSpec represents the specification of a Ceph COSI Driver
#CephCOSIDriverSpec: {
	// Image is the container image to run the Ceph COSI driver
	// +optional
	image?: string @go(Image)

	// ObjectProvisionerImage is the container image to run the COSI driver sidecar
	// +optional
	objectProvisionerImage?: string @go(ObjectProvisionerImage)

	// DeploymentStrategy is the strategy to use to deploy the COSI driver.
	// +optional
	// +kubebuilder:validation:Enum=Never;Auto;Always
	deploymentStrategy?: #COSIDeploymentStrategy @go(DeploymentStrategy)

	// Placement is the placement strategy to use for the COSI driver
	// +optional
	placement?: #Placement @go(Placement)

	// Resources is the resource requirements for the COSI driver
	// +optional
	resources?: v1.#ResourceRequirements @go(Resources)
}

// COSIDeploymentStrategy represents the strategy to use to deploy the Ceph COSI driver
#COSIDeploymentStrategy: string // #enumCOSIDeploymentStrategy

#enumCOSIDeploymentStrategy:
	#COSIDeploymentStrategyNever |
	#COSIDeploymentStrategyAuto |
	#COSIDeploymentStrategyAlways

// Never means the Ceph COSI driver will never deployed
#COSIDeploymentStrategyNever: #COSIDeploymentStrategy & "Never"

// Auto means the Ceph COSI driver will be deployed automatically if object store is present
#COSIDeploymentStrategyAuto: #COSIDeploymentStrategy & "Auto"

// Always means the Ceph COSI driver will be deployed even if the object store is not present
#COSIDeploymentStrategyAlways: #COSIDeploymentStrategy & "Always"
